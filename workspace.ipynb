{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2dc78d7-a5f5-4ad8-a01c-68912ad5aefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil, glob, os\n",
    "\n",
    "for file in glob.glob('a/*'):\n",
    "    shutil.copy(file, f'b/{file.split(os.sep)[-1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "644015d9-2d27-4354-9668-5ebb2dce93e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.0610, 0.0610, 0.0610, 0.0610, 0.0610, 0.0610, 0.0610, 0.4509, 0.0610,\n",
       "         0.0610]),\n",
       " tensor(2.1658))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "support = torch.linspace(-10, 10, 10)\n",
    "\n",
    "q = torch.zeros(10)\n",
    "q[7]=2\n",
    "\n",
    "q = F.softmax(q,-1)\n",
    "\n",
    "q, (q*support).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2bb0a18b-aad0-4c5b-9f25-5b0927ef2377",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ True,  True, False])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=torch.tensor([1,0,-1])\n",
    "a!=-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9be866db-c464-4266-ad03-8ce0524d6594",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sneep\\anaconda3\\envs\\python_\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\sneep\\anaconda3\\envs\\python_\\Lib\\site-packages\\diffusers\\utils\\outputs.py:63: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMPALA ResNet Parameters: 1.56M\n",
      "DQN Parameters: 35.21M\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import time\n",
    "\n",
    "import math\n",
    "\n",
    "from nosaveddata import *\n",
    "\n",
    "\n",
    "def renormalize(tensor, has_batch=False):\n",
    "    shape = tensor.shape\n",
    "    tensor = tensor.view(tensor.shape[0], -1)\n",
    "    max_value,_ = torch.max(tensor, -1, keepdim=True)\n",
    "    min_value,_ = torch.min(tensor, -1, keepdim=True)\n",
    "    return ((tensor - min_value) / (max_value - min_value + 1e-5)).view(shape)\n",
    "\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, n_actions, hiddens=2048, mlp_layers=1, scale_width=4,\n",
    "                 n_atoms=51, Vmin=-10, Vmax=10):\n",
    "        super().__init__()\n",
    "        self.support = torch.linspace(Vmin, Vmax, n_atoms).cuda()\n",
    "        \n",
    "        self.hiddens=hiddens\n",
    "        self.scale_width=scale_width\n",
    "        self.act = nn.ReLU()\n",
    "        \n",
    "        \n",
    "        self.encoder_cnn = IMPALA_Resnet(scale_width=scale_width, norm=False, init=init_xavier, act=self.act)\n",
    "        \n",
    "        \n",
    "\n",
    "        # Single layer dense that maps the flattened encoded representation into hiddens.\n",
    "        self.projection = MLP(13824, med_hiddens=hiddens, out_hiddens=hiddens,\n",
    "                              last_init=init_xavier, layers=1)\n",
    "        self.prediction = MLP(hiddens, out_hiddens=hiddens, layers=1, last_init=init_xavier)\n",
    "                                              \n",
    "        self.transition = nn.Sequential(DQN_Conv(32*scale_width+n_actions, 32*scale_width, 3, 1, 1, norm=False, init=init_xavier, act=self.act),\n",
    "                                        DQN_Conv(32*scale_width, 32*scale_width, 3, 1, 1, norm=False, init=init_xavier, act=self.act))\n",
    "\n",
    "        self.reward_mlp = MLP(hiddens, out_hiddens=1, layers=1, last_init=init_xavier)\n",
    "\n",
    "        # Single layer dense that maps hiddens into the output dim according to:\n",
    "        # 1. https://arxiv.org/pdf/1707.06887.pdf -- Distributional Reinforcement Learning\n",
    "        # 2. https://arxiv.org/pdf/1511.06581.pdf -- Dueling DQN\n",
    "        self.a = MLP(hiddens, out_hiddens=n_actions*num_buckets, layers=1, in_act=self.act, last_init=init_xavier)\n",
    "        self.v = MLP(hiddens, out_hiddens=num_buckets, layers=1, in_act=self.act, last_init=init_xavier)\n",
    "    \n",
    "        params_count(self, 'DQN')\n",
    "    \n",
    "    def forward(self, X, y_action):\n",
    "        X, z = self.encode(X)\n",
    "        \n",
    "        \n",
    "        q, action = self.q_head(X)\n",
    "        z_pred = self.get_transition(z, y_action)\n",
    "\n",
    "        reward_pred = self.reward_mlp(z_pred)\n",
    "\n",
    "        return q, action, X[:,1:].clone().detach(), z_pred, reward_pred\n",
    "    \n",
    "    def get_root(self, X):\n",
    "        X, z = self.encode(X)\n",
    "        q, action = self.q_head(X)\n",
    "\n",
    "        return z, q.squeeze(-3)\n",
    "\n",
    "    def get_Q_last_state(self, z):\n",
    "        z = self.projection(z.flatten(-3,-1)).view(self.batch,-1)\n",
    "        z = self.prediction(z)\n",
    "\n",
    "        \n",
    "        q, action = self.q_head(z)\n",
    "\n",
    "        action = action[:,:,None,None].expand_as(q)[:,:,0][:,:,None]\n",
    "        q = q.gather(-2,action)\n",
    "        \n",
    "        return (q.squeeze()*self.support).sum(-1)\n",
    "    \n",
    "    def env_step(self, X):\n",
    "        with torch.no_grad():\n",
    "            X, _ = self.encode(X)\n",
    "            _, action = self.q_head(X)\n",
    "            \n",
    "            return action.detach()\n",
    "    \n",
    "\n",
    "    def encode(self, X):\n",
    "        batch, seq = X.shape[:2]\n",
    "        self.batch = batch\n",
    "        self.seq = seq\n",
    "        X = self.encoder_cnn(X.contiguous().view(self.batch*self.seq, *(X.shape[2:])))\n",
    "        X = renormalize(X).contiguous().view(self.batch, self.seq, *X.shape[-3:])\n",
    "        X = X.contiguous().view(self.batch, self.seq, *X.shape[-3:])\n",
    "        z = X.clone()\n",
    "        X = X.flatten(-3,-1)\n",
    "        X = self.projection(X)\n",
    "        return X, z\n",
    "\n",
    "    def transition_one_step(self, z, action):\n",
    "        self.batch = z.shape[0]\n",
    "        z = z.contiguous().view(-1, *z.shape[-3:])\n",
    "        \n",
    "        action = F.one_hot(action.clone(), n_actions).view(-1, n_actions)\n",
    "        action = action.view(-1, n_actions, 1, 1).expand(-1, n_actions, *z.shape[-2:])\n",
    "\n",
    "        #print(f\"transition_one_step {z.shape, action.shape}\")\n",
    "        z_pred = torch.cat( (z, action), 1)\n",
    "        z_pred = self.transition(z_pred)\n",
    "        z_transition = renormalize(z_pred)\n",
    "        \n",
    "        z_pred = self.projection(z_transition.flatten(-3,-1)).view(self.batch,-1)\n",
    "        z_pred = self.prediction(z_pred)\n",
    "\n",
    "        \n",
    "        q = self.dueling_dqn(z_pred).squeeze(-3)\n",
    "        action = (q*self.support).sum(-1).argmax(-1)\n",
    "        \n",
    "        reward_pred = self.reward_mlp(z_pred)\n",
    "        \n",
    "        return z_transition, q, reward_pred.squeeze(-1)\n",
    "        \n",
    "    def get_transition(self, z, action):\n",
    "        z = z.contiguous().view(-1, *z.shape[-3:])\n",
    "        \n",
    "        action = F.one_hot(action.clone(), n_actions).view(-1, n_actions)\n",
    "        action = action.view(-1, 5, n_actions, 1, 1).expand(-1, 5, n_actions, *z.shape[-2:])\n",
    "\n",
    "        z_pred = torch.cat( (z, action[:,0]), 1)\n",
    "        z_pred = self.transition(z_pred)\n",
    "        z_pred = renormalize(z_pred)\n",
    "        \n",
    "        z_preds=[z_pred.clone()]\n",
    "        \n",
    "\n",
    "        for k in range(4):\n",
    "            z_pred = torch.cat( (z_pred, action[:,k+1]), 1)\n",
    "            z_pred = self.transition(z_pred)\n",
    "            z_pred = renormalize(z_pred)\n",
    "            \n",
    "            z_preds.append(z_pred)\n",
    "        \n",
    "        \n",
    "        z_pred = torch.stack(z_preds,1)\n",
    "\n",
    "        z_pred = self.projection(z_pred.flatten(-3,-1)).view(self.batch,5,-1)\n",
    "        z_pred = self.prediction(z_pred)\n",
    "        \n",
    "        return z_pred\n",
    "\n",
    "    \n",
    "    def q_head(self, X):\n",
    "        q = self.dueling_dqn(X)\n",
    "        action = (q*self.support).sum(-1).argmax(-1)\n",
    "        \n",
    "        return q, action\n",
    "\n",
    "    def get_max_action(self, X):\n",
    "        with torch.no_grad():\n",
    "            X, _ = self.encode(X)\n",
    "            q = self.dueling_dqn(X)\n",
    "            \n",
    "            action = (q*self.support).sum(-1).argmax(-1)\n",
    "            return action\n",
    "\n",
    "    def evaluate(self, X, action):\n",
    "        with torch.no_grad():\n",
    "            X, _ = self.encode(X)\n",
    "            \n",
    "            q = self.dueling_dqn(X)\n",
    "            \n",
    "            action = action[:,:,None,None].expand_as(q)[:,:,0][:,:,None]\n",
    "            q = q.gather(-2,action)\n",
    "            \n",
    "            return q\n",
    "\n",
    "    def dueling_dqn(self, X):\n",
    "        X = F.relu(X)\n",
    "        \n",
    "        a = self.a(X).view(self.batch, -1, n_actions, num_buckets)\n",
    "        v = self.v(X).view(self.batch, -1, 1, num_buckets)\n",
    "        \n",
    "        q = v + a - a.mean(-2,keepdim=True)\n",
    "        q = F.softmax(q,-1)\n",
    "        \n",
    "        return q\n",
    "    \n",
    "    def network_ema(self, rand_network, target_network, alpha=0.5):\n",
    "        for param, param_target in zip(rand_network.parameters(), target_network.parameters()):\n",
    "            param_target.data = alpha * param_target.data + (1 - alpha) * param.data.clone()\n",
    "\n",
    "    def hard_reset(self, random_model, alpha=0.5):\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            self.network_ema(random_model.encoder_cnn, self.encoder_cnn, alpha)\n",
    "            self.network_ema(random_model.transition, self.transition, alpha)\n",
    "\n",
    "            self.network_ema(random_model.projection, self.projection, 0)\n",
    "            self.network_ema(random_model.prediction, self.prediction, 0)\n",
    "            self.network_ema(random_model.reward_mlp, self.reward_mlp, 0)\n",
    "\n",
    "            self.network_ema(random_model.a, self.a, 0)\n",
    "            self.network_ema(random_model.v, self.v, 0)\n",
    "\n",
    "\n",
    "num_buckets = 51\n",
    "n_actions = 7\n",
    "\n",
    "model = DQN(n_actions).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "255d604f-55b2-457f-84ce-d365d43b44a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GUMBEL & Q_sa tensor([[-0.2536, -0.4209, -0.0024, -0.7191, -0.0036, -0.0155, -0.3478],\n",
      "        [ 0.0752, -0.5040, -0.4156, -0.5600,  0.2159, -0.0634, -0.3638],\n",
      "        [-0.1573, -0.3986, -0.3678, -0.5914,  0.6631,  0.0274, -0.3696]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[-0.4036, -0.5291, -0.5031, -0.7707, -0.0362, -0.1193, -0.4009],\n",
      "        [-0.3582, -0.5717, -0.4956, -0.7085,  0.0107, -0.0842, -0.4084],\n",
      "        [-0.2964, -0.4439, -0.4410, -0.6243,  0.0063, -0.0127, -0.3822]],\n",
      "       device='cuda:0', grad_fn=<StackBackward0>)\n",
      "\n",
      "\n",
      "gumbel q (torch.Size([3, 7]), torch.Size([3, 7]), torch.Size([3, 7]))\n",
      "ACTION AMMOUNT IS HEREBYRAUBIESUYRBAESYUI 7\n",
      "TOPK K ACTIONS tensor([[2, 4, 5, 0, 6, 1, 3],\n",
      "        [4, 0, 5, 6, 2, 1, 3],\n",
      "        [4, 5, 0, 2, 6, 1, 3]], device='cuda:0')\n",
      "HALVE SIM (torch.Size([3, 7]), torch.Size([3, 7]))\n",
      "next_halve 2\n",
      "Q Mask (tensor([[-0.2536, -0.4209, -0.0024, -0.7191, -0.0036, -0.0155, -0.3478],\n",
      "        [ 0.0752, -0.5040, -0.4156, -0.5600,  0.2159, -0.0634, -0.3638],\n",
      "        [-0.1573, -0.3986, -0.3678, -0.5914,  0.6631,  0.0274, -0.3696]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>), tensor([[1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1]], device='cuda:0'))\n",
      "\n",
      "\n",
      "\n",
      "Q Mask (tensor([[-2.5356e-01, -4.2091e-01,  2.8862e+01, -7.1909e-01, -3.5511e-03,\n",
      "         -1.5487e-02, -3.4783e-01],\n",
      "        [ 7.5194e-02, -5.0397e-01, -4.1562e-01, -5.5995e-01,  2.9633e+01,\n",
      "         -6.3402e-02, -3.6382e-01],\n",
      "        [-1.5731e-01, -3.9865e-01, -3.6782e-01, -5.9139e-01,  2.5003e+01,\n",
      "          2.7404e-02, -3.6965e-01]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1]], device='cuda:0'))\n",
      "\n",
      "\n",
      "\n",
      "gumbel q (torch.Size([3, 7]), torch.Size([3, 7]), torch.Size([3, 7]))\n",
      "ACTION AMMOUNT IS HEREBYRAUBIESUYRBAESYUI 4\n",
      "TOPK K ACTIONS tensor([[2, 4, 5, 0],\n",
      "        [4, 0, 5, 6],\n",
      "        [4, 5, 0, 2]], device='cuda:0')\n",
      "HALVE SIM (torch.Size([3, 7]), torch.Size([3, 7]))\n",
      "next_halve 7\n",
      "Q Mask (tensor([[-2.5356e-01, -4.2091e-01,  2.9428e+01, -7.1909e-01, -3.5511e-03,\n",
      "         -1.5487e-02, -3.4783e-01],\n",
      "        [ 7.5194e-02, -5.0397e-01, -4.1562e-01, -5.5995e-01,  3.0210e+01,\n",
      "         -6.3402e-02, -3.6382e-01],\n",
      "        [-1.5731e-01, -3.9865e-01, -3.6782e-01, -5.9139e-01,  2.5480e+01,\n",
      "          2.7404e-02, -3.6965e-01]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[1, 0, 1, 0, 1, 1, 0],\n",
      "        [1, 0, 0, 0, 1, 1, 1],\n",
      "        [1, 0, 1, 0, 1, 1, 0]], device='cuda:0'))\n",
      "\n",
      "\n",
      "\n",
      "Q Mask (tensor([[-2.5356e-01, -4.2091e-01,  2.9994e+01, -7.1909e-01, -3.5511e-03,\n",
      "         -1.5487e-02, -3.4783e-01],\n",
      "        [ 7.5194e-02, -5.0397e-01, -4.1562e-01, -5.5995e-01,  3.0786e+01,\n",
      "         -6.3402e-02, -3.6382e-01],\n",
      "        [-1.5731e-01, -3.9865e-01, -3.6782e-01, -5.9139e-01,  2.5957e+01,\n",
      "          2.7404e-02, -3.6965e-01]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[1, 0, 1, 0, 1, 1, 0],\n",
      "        [1, 0, 0, 0, 1, 1, 1],\n",
      "        [1, 0, 1, 0, 1, 1, 0]], device='cuda:0'))\n",
      "\n",
      "\n",
      "\n",
      "Q Mask (tensor([[-2.5356e-01, -4.2091e-01,  3.0560e+01, -7.1909e-01, -3.5511e-03,\n",
      "         -1.5487e-02, -3.4783e-01],\n",
      "        [ 7.5194e-02, -5.0397e-01, -4.1562e-01, -5.5995e-01,  3.1363e+01,\n",
      "         -6.3402e-02, -3.6382e-01],\n",
      "        [-1.5731e-01, -3.9865e-01, -3.6782e-01, -5.9139e-01,  2.6434e+01,\n",
      "          2.7404e-02, -3.6965e-01]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[1, 0, 1, 0, 1, 1, 0],\n",
      "        [1, 0, 0, 0, 1, 1, 1],\n",
      "        [1, 0, 1, 0, 1, 1, 0]], device='cuda:0'))\n",
      "\n",
      "\n",
      "\n",
      "Q Mask (tensor([[-2.5356e-01, -4.2091e-01,  3.1126e+01, -7.1909e-01, -3.5511e-03,\n",
      "         -1.5487e-02, -3.4783e-01],\n",
      "        [ 7.5194e-02, -5.0397e-01, -4.1562e-01, -5.5995e-01,  3.1940e+01,\n",
      "         -6.3402e-02, -3.6382e-01],\n",
      "        [-1.5731e-01, -3.9865e-01, -3.6782e-01, -5.9139e-01,  2.6912e+01,\n",
      "          2.7404e-02, -3.6965e-01]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[1, 0, 1, 0, 1, 1, 0],\n",
      "        [1, 0, 0, 0, 1, 1, 1],\n",
      "        [1, 0, 1, 0, 1, 1, 0]], device='cuda:0'))\n",
      "\n",
      "\n",
      "\n",
      "Q Mask (tensor([[-2.5356e-01, -4.2091e-01,  3.1692e+01, -7.1909e-01, -3.5511e-03,\n",
      "         -1.5487e-02, -3.4783e-01],\n",
      "        [ 7.5194e-02, -5.0397e-01, -4.1562e-01, -5.5995e-01,  3.2517e+01,\n",
      "         -6.3402e-02, -3.6382e-01],\n",
      "        [-1.5731e-01, -3.9865e-01, -3.6782e-01, -5.9139e-01,  2.7389e+01,\n",
      "          2.7404e-02, -3.6965e-01]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[1, 0, 1, 0, 1, 1, 0],\n",
      "        [1, 0, 0, 0, 1, 1, 1],\n",
      "        [1, 0, 1, 0, 1, 1, 0]], device='cuda:0'))\n",
      "\n",
      "\n",
      "\n",
      "gumbel q (torch.Size([3, 7]), torch.Size([3, 7]), torch.Size([3, 7]))\n",
      "ACTION AMMOUNT IS HEREBYRAUBIESUYRBAESYUI 2\n",
      "TOPK K ACTIONS tensor([[2, 4],\n",
      "        [4, 0],\n",
      "        [4, 5]], device='cuda:0')\n",
      "HALVE SIM (torch.Size([3, 7]), torch.Size([3, 7]))\n",
      "next_halve 17\n",
      "Q Mask (tensor([[-2.5356e-01, -4.2091e-01,  3.2258e+01, -7.1909e-01, -3.5511e-03,\n",
      "         -1.5487e-02, -3.4783e-01],\n",
      "        [ 7.5194e-02, -5.0397e-01, -4.1562e-01, -5.5995e-01,  3.3094e+01,\n",
      "         -6.3402e-02, -3.6382e-01],\n",
      "        [-1.5731e-01, -3.9865e-01, -3.6782e-01, -5.9139e-01,  2.7866e+01,\n",
      "          2.7404e-02, -3.6965e-01]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[0, 0, 1, 0, 1, 0, 0],\n",
      "        [1, 0, 0, 0, 1, 0, 0],\n",
      "        [0, 0, 0, 0, 1, 1, 0]], device='cuda:0'))\n",
      "\n",
      "\n",
      "\n",
      "Q Mask (tensor([[-2.5356e-01, -4.2091e-01,  3.2824e+01, -7.1909e-01, -3.5511e-03,\n",
      "         -1.5487e-02, -3.4783e-01],\n",
      "        [ 7.5194e-02, -5.0397e-01, -4.1562e-01, -5.5995e-01,  3.3670e+01,\n",
      "         -6.3402e-02, -3.6382e-01],\n",
      "        [-1.5731e-01, -3.9865e-01, -3.6782e-01, -5.9139e-01,  2.8343e+01,\n",
      "          2.7404e-02, -3.6965e-01]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[0, 0, 1, 0, 1, 0, 0],\n",
      "        [1, 0, 0, 0, 1, 0, 0],\n",
      "        [0, 0, 0, 0, 1, 1, 0]], device='cuda:0'))\n",
      "\n",
      "\n",
      "\n",
      "Q Mask (tensor([[-2.5356e-01, -4.2091e-01,  3.3390e+01, -7.1909e-01, -3.5511e-03,\n",
      "         -1.5487e-02, -3.4783e-01],\n",
      "        [ 7.5194e-02, -5.0397e-01, -4.1562e-01, -5.5995e-01,  3.4247e+01,\n",
      "         -6.3402e-02, -3.6382e-01],\n",
      "        [-1.5731e-01, -3.9865e-01, -3.6782e-01, -5.9139e-01,  2.8821e+01,\n",
      "          2.7404e-02, -3.6965e-01]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[0, 0, 1, 0, 1, 0, 0],\n",
      "        [1, 0, 0, 0, 1, 0, 0],\n",
      "        [0, 0, 0, 0, 1, 1, 0]], device='cuda:0'))\n",
      "\n",
      "\n",
      "\n",
      "Q Mask (tensor([[-2.5356e-01, -4.2091e-01,  3.3956e+01, -7.1909e-01, -3.5511e-03,\n",
      "         -1.5487e-02, -3.4783e-01],\n",
      "        [ 7.5194e-02, -5.0397e-01, -4.1562e-01, -5.5995e-01,  3.4824e+01,\n",
      "         -6.3402e-02, -3.6382e-01],\n",
      "        [-1.5731e-01, -3.9865e-01, -3.6782e-01, -5.9139e-01,  2.9298e+01,\n",
      "          2.7404e-02, -3.6965e-01]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[0, 0, 1, 0, 1, 0, 0],\n",
      "        [1, 0, 0, 0, 1, 0, 0],\n",
      "        [0, 0, 0, 0, 1, 1, 0]], device='cuda:0'))\n",
      "\n",
      "\n",
      "\n",
      "Q Mask (tensor([[-2.5356e-01, -4.2091e-01,  3.4522e+01, -7.1909e-01, -3.5511e-03,\n",
      "         -1.5487e-02, -3.4783e-01],\n",
      "        [ 7.5194e-02, -5.0397e-01, -4.1562e-01, -5.5995e-01,  3.5401e+01,\n",
      "         -6.3402e-02, -3.6382e-01],\n",
      "        [-1.5731e-01, -3.9865e-01, -3.6782e-01, -5.9139e-01,  2.9775e+01,\n",
      "          2.7404e-02, -3.6965e-01]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[0, 0, 1, 0, 1, 0, 0],\n",
      "        [1, 0, 0, 0, 1, 0, 0],\n",
      "        [0, 0, 0, 0, 1, 1, 0]], device='cuda:0'))\n",
      "\n",
      "\n",
      "\n",
      "Q Mask (tensor([[-2.5356e-01, -4.2091e-01,  3.5088e+01, -7.1909e-01, -3.5511e-03,\n",
      "         -1.5487e-02, -3.4783e-01],\n",
      "        [ 7.5194e-02, -5.0397e-01, -4.1562e-01, -5.5995e-01,  3.5978e+01,\n",
      "         -6.3402e-02, -3.6382e-01],\n",
      "        [-1.5731e-01, -3.9865e-01, -3.6782e-01, -5.9139e-01,  3.0252e+01,\n",
      "          2.7404e-02, -3.6965e-01]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[0, 0, 1, 0, 1, 0, 0],\n",
      "        [1, 0, 0, 0, 1, 0, 0],\n",
      "        [0, 0, 0, 0, 1, 1, 0]], device='cuda:0'))\n",
      "\n",
      "\n",
      "\n",
      "Q Mask (tensor([[-2.5356e-01, -4.2091e-01,  3.5654e+01, -7.1909e-01, -3.5511e-03,\n",
      "         -1.5487e-02, -3.4783e-01],\n",
      "        [ 7.5194e-02, -5.0397e-01, -4.1562e-01, -5.5995e-01,  3.6554e+01,\n",
      "         -6.3402e-02, -3.6382e-01],\n",
      "        [-1.5731e-01, -3.9865e-01, -3.6782e-01, -5.9139e-01,  3.0730e+01,\n",
      "          2.7404e-02, -3.6965e-01]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[0, 0, 1, 0, 1, 0, 0],\n",
      "        [1, 0, 0, 0, 1, 0, 0],\n",
      "        [0, 0, 0, 0, 1, 1, 0]], device='cuda:0'))\n",
      "\n",
      "\n",
      "\n",
      "Q Mask (tensor([[-2.5356e-01, -4.2091e-01,  3.6219e+01, -7.1909e-01, -3.5511e-03,\n",
      "         -1.5487e-02, -3.4783e-01],\n",
      "        [ 7.5194e-02, -5.0397e-01, -4.1562e-01, -5.5995e-01,  3.7131e+01,\n",
      "         -6.3402e-02, -3.6382e-01],\n",
      "        [-1.5731e-01, -3.9865e-01, -3.6782e-01, -5.9139e-01,  3.1207e+01,\n",
      "          2.7404e-02, -3.6965e-01]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[0, 0, 1, 0, 1, 0, 0],\n",
      "        [1, 0, 0, 0, 1, 0, 0],\n",
      "        [0, 0, 0, 0, 1, 1, 0]], device='cuda:0'))\n",
      "\n",
      "\n",
      "\n",
      "Q Mask (tensor([[-2.5356e-01, -4.2091e-01,  3.6785e+01, -7.1909e-01, -3.5511e-03,\n",
      "         -1.5487e-02, -3.4783e-01],\n",
      "        [ 7.5194e-02, -5.0397e-01, -4.1562e-01, -5.5995e-01,  3.7708e+01,\n",
      "         -6.3402e-02, -3.6382e-01],\n",
      "        [-1.5731e-01, -3.9865e-01, -3.6782e-01, -5.9139e-01,  3.1684e+01,\n",
      "          2.7404e-02, -3.6965e-01]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[0, 0, 1, 0, 1, 0, 0],\n",
      "        [1, 0, 0, 0, 1, 0, 0],\n",
      "        [0, 0, 0, 0, 1, 1, 0]], device='cuda:0'))\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0000, 0.3537, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.3605, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.2983, 0.0000, 0.0000]],\n",
       "       device='cuda:0', grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "class MCTS_Node(nsd_Module):\n",
    "    def __init__(self, z, Q, reward, prev_state, n_actions, hiddens=2048, batch_size=3):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.transitions = [None]*self.n_actions\n",
    "        \n",
    "        self.n = torch.zeros(n_actions, device='cuda')\n",
    "        \n",
    "        self.p = F.softmax(Q,-1)\n",
    "        self.Q_sa = Q\n",
    "        self.Q = torch.zeros_like(Q)\n",
    "\n",
    "        self.choosen_action = torch.tensor(-1, device='cuda', dtype=torch.long)\n",
    "        \n",
    "        \n",
    "    def reset_n(self):\n",
    "        self.n = torch.zeros(self.n_actions, device='cuda')\n",
    "    \n",
    "    def get_stats(self):\n",
    "\n",
    "        return self.Q, self.Q_sa, self.z, self.p, self.n, self.reward, self.choosen_action\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class MCTS(nsd_Module):\n",
    "    def __init__(self, n_actions, k=5, batch_size=3, n_sim=16, topk_actions=8, c_visit=50, c_scale=0.1):\n",
    "        # Good c_scale values are 0.1 and 1\n",
    "        super().__init__()\n",
    "        self.topk_actions = torch.tensor(topk_actions)\n",
    "    \n",
    "\n",
    "    def get_root(self, model, x):\n",
    "        z, Q = model.get_root(x)\n",
    "        Q = (Q*model.support).sum(-1)\n",
    "        #print(f\"{z.shape, Q.shape}\")\n",
    "        nodes = []\n",
    "        for i in range(Q.shape[0]):\n",
    "            root = MCTS_Node(z[i], Q[i], torch.tensor([0]*self.batch_size).cuda()[i], prev_state=None, n_actions=self.n_actions, batch_size=self.batch_size)\n",
    "            nodes.append(root)\n",
    "        self.root = nodes\n",
    "        return self.root\n",
    "\n",
    "    def collate_nodes(self):\n",
    "        Q, Q_sa, Z, P, N, R, A = [], [], [], [], [], [], []\n",
    "        for node in self.cur_state:\n",
    "            q, q_sa, z, p, n, r, a = node.get_stats()\n",
    "            Q.append(q)\n",
    "            Q_sa.append(q_sa)\n",
    "            Z.append(z)\n",
    "            P.append(p)\n",
    "            N.append(n)\n",
    "            R.append(r)\n",
    "            A.append(a)\n",
    "        return torch.stack(Q,0), torch.stack(Q_sa,0), torch.stack(Z,0), torch.stack(P,0), torch.stack(N,0), torch.stack(R,0), torch.stack(A,0)\n",
    "    \n",
    "    def transition(self, model, x, action):\n",
    "        \n",
    "        z, Q, reward_pred = model.transition_one_step(x, action)\n",
    "        Q = (Q*model.support).sum(-1)\n",
    "        #print(f\"{Q.shape, reward_pred.shape}\")\n",
    "    \n",
    "        nodes = []\n",
    "        \n",
    "        for i in range(Q.shape[0]):\n",
    "            if self.cur_state[i].transitions[action[i]] == None:\n",
    "                node = MCTS_Node(z[i], Q[i], reward_pred[i], prev_state=self.cur_state[i], n_actions=self.n_actions, batch_size=self.batch_size)\n",
    "                nodes.append(node)\n",
    "                self.cur_state[i].transitions[action[i]] = node\n",
    "            else:\n",
    "                nodes.append(self.cur_state[i].transitions[action[i]])\n",
    "                \n",
    "\n",
    "        return nodes\n",
    "\n",
    "    \n",
    "    def backup(self, model):\n",
    "        \n",
    "        Q, Q_sa, z, p, n, r_t, choosen_action = self.collate_nodes()\n",
    "        \n",
    "        next_values = model.get_Q_last_state(z)#[:,None]\n",
    "\n",
    "        rewards = [r_t]\n",
    "        gammas = torch.ones(self.batch_size, self.k, device='cuda')*0.997\n",
    "        \n",
    "\n",
    "        for i in range(len(self.cur_state)):\n",
    "            self.cur_state[i] = self.cur_state[i].prev_state\n",
    "            \n",
    "\n",
    "        for l in range(self.k):\n",
    "            Q, Q_sa, z, p, n, r_t, choosen_action = self.collate_nodes()\n",
    "            \n",
    "            r = torch.stack(list(reversed(rewards)), -1)\n",
    "\n",
    "            returns = (r*gammas[:,:l+1].cumprod(-1)).sum(-1) + next_values*(gammas[:,:l+1].prod(-1))\n",
    "            #print(f\"backup returns {returns.shape}\")\n",
    "            #print(f\"n choosen action {n}\\n{choosen_action}\\n\")\n",
    "            \n",
    "            n_action = n[torch.arange(self.batch_size), choosen_action]\n",
    "            \n",
    "\n",
    "            Q[torch.arange(self.batch_size), choosen_action] = (n_action*Q[torch.arange(self.batch_size),choosen_action] + returns) / (n_action+1)\n",
    "            \n",
    "            n[torch.arange(self.batch_size), choosen_action] += 1\n",
    "\n",
    "            \n",
    "            \n",
    "            rewards.append(r_t)\n",
    "            \n",
    "            for i in range(len(self.cur_state)):\n",
    "                self.cur_state[i].Q = Q[i]\n",
    "                self.cur_state[i].n = n[i]\n",
    "                \n",
    "                self.cur_state[i] = self.cur_state[i].prev_state\n",
    "                \n",
    "        \n",
    "    def forward(self, model, x):\n",
    "\n",
    "        self.cur_state = self.get_root(model, x)\n",
    "\n",
    "        q_sa = self.collate_nodes()[1]\n",
    "        gumbel = F.gumbel_softmax(torch.zeros_like(q_sa))\n",
    "        gumbel_sa = gumbel+q_sa\n",
    "        print(f\"GUMBEL & Q_sa {gumbel_sa}\\n{q_sa}\\n\\n\")\n",
    "\n",
    "        k = min(self.topk_actions, self.n_actions)\n",
    "        \n",
    "        Q_mask = F.one_hot(gumbel_sa.topk(k)[1], self.n_actions).sum(-2)\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "        halve_sims = self.n_sim//2\n",
    "        halves = 2\n",
    "\n",
    "        next_halve = math.floor(self.n_sim/(torch.log2(self.topk_actions)*(self.topk_actions)))\n",
    "        \n",
    "        for sim in range(self.n_sim):\n",
    "            actions_to_step = []\n",
    "            for l in range(self.k):\n",
    "                q, q_sa, z, p, n, _, _ = self.collate_nodes()\n",
    "                \n",
    "                if l==0 and sim==next_halve:\n",
    "                    print(f\"gumbel q {gumbel_sa.shape, q.shape, n.shape}\")\n",
    "                    gumbel_sa = gumbel + q_sa\n",
    "                    k = min(self.topk_actions//halves*2, self.n_actions)\n",
    "                    print(f\"ACTION AMMOUNT IS HEREBYRAUBIESUYRBAESYUI {k}\")\n",
    "                    \n",
    "                    Q_mask = F.one_hot(gumbel_sa.topk(k)[1], self.n_actions).sum(-2)\n",
    "                    print(f\"TOPK K ACTIONS {gumbel_sa.topk(k)[1]}\")\n",
    "                    \n",
    "                    #Q_mask = gumbel_sa + (self.c_visit + n.max(-1)[0])*self.c_scale*q\n",
    "                    #Q_mask = F.one_hot(gumbel_sa.topk(self.topk_actions//halves)[1], self.n_actions).sum(-2)\n",
    "                    \n",
    "                    print(f\"HALVE SIM {Q_mask.shape, n.shape}\")\n",
    "\n",
    "                    next_halve += math.floor(self.n_sim/(torch.log2(self.topk_actions)*(self.topk_actions/(halves*2))))\n",
    "                    print(f\"next_halve {next_halve}\")\n",
    "                    \n",
    "                    halves*=2\n",
    "\n",
    "                if l==0:\n",
    "                    \n",
    "                    #print(f\"gumel n q {gumbel_sa.shape, n.max(-1)[0].shape, q.shape}\")\n",
    "                    Q = gumbel_sa + (self.c_visit + n.max(-1)[0][:,None])*self.c_scale*q\n",
    "                    print(f\"Q Mask {Q, Q_mask}\")\n",
    "                    #print(f\"{n}\")\n",
    "\n",
    "                    action = (Q*Q_mask).argmax(-1) # Gumbel top-k\n",
    "                    \n",
    "                else:\n",
    "                    completed_Q = (n==0)*q_sa + (n>0)*q\n",
    "                    \n",
    "                    completed_Q = (self.c_visit + n.max(-1)[0][:,None])*self.c_scale*completed_Q\n",
    "                    \n",
    "                    \n",
    "                    improved_policy = F.softmax((q_sa+completed_Q),-1)\n",
    "\n",
    "                    \n",
    "                    #print(f\"{improved_policy.shape, (n/(1+n.sum(-1)[:,None])).shape}\")\n",
    "                    Q = improved_policy - n/(1+n.sum(-1)[:,None])\n",
    "                    \n",
    "                    action = Q.argmax(-1)\n",
    "                    #print(f\"l and action: {l} {action}\")\n",
    "\n",
    "                \n",
    "                \n",
    "                \n",
    "                actions_to_step.append(Q.argmax(-1))\n",
    "                \n",
    "                \n",
    "                for i in range(len(self.cur_state)):\n",
    "                    self.cur_state[i].choosen_action = action[i]\n",
    "                    \n",
    "                self.cur_state = self.transition(model, z, action)\n",
    "                \n",
    "            self.backup(model)\n",
    "            self.cur_state = self.root\n",
    "            print(f\"\\n\\n\")\n",
    "        \n",
    "        Q = self.collate_nodes()[0]\n",
    "        return Q/self.n_sim, Q.argmax(-1), actions_to_step\n",
    "\n",
    "\n",
    "mcts = MCTS(n_actions=7, n_sim=16, topk_actions=8, batch_size=3)\n",
    "\n",
    "\n",
    "\n",
    "#x = torch.randn(3,128,12,9).cuda()\n",
    "#actions = torch.randint(0, n_actions, (x.shape[0],)).cuda()\n",
    "\n",
    "#q, next_state = model.transition_one_step(x, actions)\n",
    "\n",
    "x = torch.randn(3,1,12,96,72).cuda()\n",
    "\n",
    "#print(f\"{q.shape, next_state.shape}\")\n",
    "Q, act, _ = mcts(model, x)\n",
    "Q#.shape, act.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "989e6d59-c449-4852-8886-3de5133113f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(5), tensor([6, 5, 4]))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([-1,0,1,2,3,4,5]).max(-1)[0], torch.tensor([-1,0,1,2,3,4,5]).topk(3)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dea3f2e3-d421-4d91-b56e-101a62ac25dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "math.ceil(7/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "151f6e40-fb4b-4871-9a5c-6f565e250893",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([-1,2]).max(-1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6559b1e5-39c4-4336-baaf-4b69b2f240b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.980149500625"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.995**(4/1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d5bd6adc-aa12-4b33-8d4b-999bf31cfea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sneep\\AppData\\Local\\Temp\\ipykernel_8868\\3376276355.py:2: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  a, F.gumbel_softmax(torch.zeros_like(a))+a, F.softmax(a)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.2252, -1.0917,  1.4278]),\n",
       " tensor([ 0.2898, -0.1902,  1.4617]),\n",
       " tensor([0.2175, 0.0583, 0.7242]))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(3)\n",
    "a, F.gumbel_softmax(torch.zeros_like(a))+a, F.softmax(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e919cefd-c3a2-46e3-b352-22dc28641d04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, [2, 3])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def a(p):\n",
    "    return p.pop(0)\n",
    "\n",
    "c = [1,2,3]\n",
    "\n",
    "a(c), c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "265cb70a-c286-4b88-87f3-ac14d386603e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(12, device='cuda:0')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_envs=1\n",
    "model.env_step(x).view(num_envs).squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b11ed3-3333-43c6-b618-163cc3b5a516",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "51ec618b-d86b-4b22-a100-71a8cde87338",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-9.1040, -6.3314, -3.5589, -0.7863,  1.9863,  4.7589,  7.5314, 10.3040],\n",
       "        device='cuda:0'),\n",
       " tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0285, 1.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0285, 1.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0285, 1.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0285, 1.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0285, 1.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0285, 1.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0285, 1.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0285, 1.0000]],\n",
       "        device='cuda:0'))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def project_distribution(supports, weights, target_support):\n",
    "    with torch.no_grad():\n",
    "        v_min, v_max = target_support[0], target_support[-1]\n",
    "        # `N` in Eq7.\n",
    "        num_dims = target_support.shape[-1]\n",
    "        # delta_z = `\\Delta z` in Eq7.\n",
    "        delta_z = (v_max - v_min) / (num_buckets-1)\n",
    "        # clipped_support = `[\\hat{T}_{z_j}]^{V_max}_{V_min}` in Eq7.\n",
    "        clipped_support = supports.clip(v_min, v_max)\n",
    "        # numerator = `|clipped_support - z_i|` in Eq7.\n",
    "        numerator = (clipped_support[:,None] - target_support[None,:,None].repeat_interleave(clipped_support.shape[0],0)).abs()\n",
    "        quotient = 1 - (numerator / delta_z)\n",
    "        # clipped_quotient = `[1 - numerator / (\\Delta z)]_0^1` in Eq7.\n",
    "        clipped_quotient = quotient.clip(0, 1)\n",
    "        # inner_prod = `\\sum_{j=0}^{N-1} clipped_quotient * p_j(x', \\pi(x'))` in Eq7.\n",
    "        inner_prod = (clipped_quotient * weights[:,None]).sum(-1)\n",
    "        #inner_prod = (clipped_quotient).sum(-1) * weights\n",
    "        return inner_prod.squeeze()\n",
    "\n",
    "\n",
    "support = torch.linspace(-10, 10, 8).cuda()\n",
    "w = torch.ones(1,8).cuda()\n",
    "#w[0,6]=1\n",
    "weights = F.softmax(w,-1)\n",
    "\n",
    "tgt_sup = 0.6+(0.997**10)*support\n",
    "\n",
    "tgt_sup, project_distribution(tgt_sup, weights, support)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333f0dc7-59f4-428a-91ac-d62ed9e60df5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e888185e-e629-40b3-936e-b47cf99455f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150e3f2c-372a-40e5-8803-e88d11306508",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60080665-e9e7-4b05-8948-27911c47795b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2994648a710>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "initial_n = 4\n",
    "final_n = 1\n",
    "schedule_max_step = 10000\n",
    "\n",
    "v=[]\n",
    "for grad_step in range(40000):\n",
    "    n = initial_n * (final_n/initial_n)**(min(torch.tensor(grad_step-10000).clip(0),schedule_max_step) / schedule_max_step)\n",
    "    v.append(n)\n",
    "\n",
    "plt.plot(v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "828e5304-fe03-4de4-bda4-bb803c582ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(torch.Size([32, 1]), tensor([[147],\n",
      "        [189],\n",
      "        [233],\n",
      "        [ 13],\n",
      "        [215],\n",
      "        [226],\n",
      "        [ 11],\n",
      "        [141],\n",
      "        [ 11],\n",
      "        [243],\n",
      "        [180],\n",
      "        [ 56],\n",
      "        [  6],\n",
      "        [215],\n",
      "        [161],\n",
      "        [ 13],\n",
      "        [ 88],\n",
      "        [154],\n",
      "        [ 11],\n",
      "        [ 97],\n",
      "        [ 60],\n",
      "        [189],\n",
      "        [ 39],\n",
      "        [141],\n",
      "        [150],\n",
      "        [189],\n",
      "        [115],\n",
      "        [212],\n",
      "        [189],\n",
      "        [141],\n",
      "        [ 11],\n",
      "        [ 11]]))\n",
      "torch.Size([32, 1, 512])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([32])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from nosaveddata import *\n",
    "\n",
    "class Quantizer1d(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_embeddings=256,\n",
    "                 dim=512\n",
    "                 ):\n",
    "        super(Quantizer1d, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings, dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        \n",
    "        dist = torch.cdist(x, self.embedding.weight[None, :].repeat((x.size(0), 1, 1)))\n",
    "        min_encoding_indices = torch.argmin(dist, dim=-1)\n",
    "        \n",
    "        quant_out = torch.index_select(self.embedding.weight, 0, min_encoding_indices.view(-1)).view(B,T,C)\n",
    "        print(f\"{min_encoding_indices.shape, min_encoding_indices}\")\n",
    "        \n",
    "        commmitment_loss = ((quant_out.detach() - x) ** 2).mean((1,2))\n",
    "        codebook_loss = ((quant_out - x.detach()) ** 2).mean((1,2))\n",
    "        quantize_losses = {\n",
    "            'codebook_loss' : codebook_loss,\n",
    "            'commitment_loss' : commmitment_loss\n",
    "        }\n",
    "        quant_out = x + (quant_out - x).detach()\n",
    "        min_encoding_indices = min_encoding_indices.contiguous().view((B,-1))\n",
    "        return quant_out, quantize_losses, min_encoding_indices\n",
    "\n",
    "    def forward_idx(self, x, idx):\n",
    "        B, T, C = x.shape\n",
    "        \n",
    "        \n",
    "        quant_out = torch.index_select(self.embedding.weight, 0, idx.view(-1)).view(B,T,C)\n",
    "        \n",
    "        print(f\"{x.shape}\")\n",
    "        commmitment_loss = ((quant_out.detach() - x) ** 2).mean((1,2))\n",
    "        codebook_loss = ((quant_out - x.detach()) ** 2).mean((1,2))\n",
    "        quantize_losses = {\n",
    "            'codebook_loss' : codebook_loss,\n",
    "            'commitment_loss' : commmitment_loss\n",
    "        }\n",
    "        quant_out = x + (quant_out - x).detach()\n",
    "        return quant_out, quantize_losses\n",
    "\n",
    "quant = Quantizer1d(256,512)\n",
    "\n",
    "x = torch.randn(32,1,512)\n",
    "idx = torch.randint(0,256,(32,))[:,None]\n",
    "\n",
    "a, b, c  = quant(x)\n",
    "\n",
    "x, loss = quant.forward_idx(x, idx)\n",
    "\n",
    "loss['codebook_loss'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "720d21a2-abe3-4f2d-a17d-6614bff47a5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.9774,  0.0935, -0.3474,  ...,  0.1858,  0.9352,  0.8609],\n",
       "         [ 0.4313, -0.9067,  0.3586,  ...,  2.5234,  1.3809,  0.4820],\n",
       "         [ 0.9409, -0.5029, -0.8184,  ..., -0.7990, -1.3363,  1.2311],\n",
       "         [-0.7292,  0.3482, -0.3422,  ...,  1.2561,  0.0179, -2.1140]],\n",
       "        grad_fn=<EmbeddingBackward0>),\n",
       " tensor([26,  9, 37, 35]),\n",
       " tensor([-0.9774,  0.0935, -0.3474,  ...,  0.1858,  0.9352,  0.8609]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from nosaveddata import *\n",
    "\n",
    "seed_np_torch(42)\n",
    "\n",
    "a = torch.randn(38,2048)\n",
    "\n",
    "b = nn.Embedding(38,2048)\n",
    "\n",
    "state_dict = {'weight': a}\n",
    "\n",
    "b.load_state_dict(state_dict)\n",
    "\n",
    "c = torch.randint(0,38,(4,))\n",
    "\n",
    "b(c), c, a[26]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a38d8d4b-dcd4-4a47-8cbe-2757621a804e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([[2]]).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "deb33f37-c373-48e0-8871-ebb1cbfb5ac1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1000 in range(999,1001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ac55d1b8-7a6f-4d01-a4bb-1a66ee33b4d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([9, 8, 7]),\n",
       "indices=tensor([9, 8, 7]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a=torch.arange(10)\n",
    "\n",
    "a.topk(3,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6767425-f06c-4aed-8ddf-f744b36658fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bf9c5bf5-a80a-4229-a7a5-5f5e70b65fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([300, 2048])\n",
      "Sampled Vectors:\n",
      "[299, 106, 229, 18, 244, 17, 48, 173, 194, 133, 224, 222, 142, 163, 206, 51, 201, 216, 180, 268, 245, 97, 22, 160, 243, 121, 237, 66, 171, 172, 123, 170, 77, 120, 4, 149, 254, 57, 6, 74, 131, 85, 141, 205, 47, 252, 112, 127, 193, 132, 187, 87, 255, 39, 235, 108, 276, 214, 125, 43, 99, 79, 72, 19, 10, 162, 115, 81, 140, 179, 269, 83, 210, 75, 297, 52, 110, 55, 295, 174, 135, 288, 105, 178, 155, 184, 287, 271, 259, 136, 67, 24, 147, 5, 182, 257, 291, 62, 208, 42, 238, 60, 227, 148, 262, 294, 196, 27, 26, 146, 176, 200, 53, 138, 212, 128, 8, 247, 242, 199, 230, 107, 0, 63, 215, 263, 258, 68, 185, 246, 168, 251, 165, 188, 1, 65, 3, 228, 102, 253, 78, 167, 175, 145, 69, 261, 264, 30, 23, 154, 104, 159, 31, 211, 248, 191, 50, 226, 41, 93, 256, 21, 282, 113, 122, 25, 28, 234, 114, 213, 281, 296, 284, 103, 272, 198, 20, 240, 82, 220, 189, 290, 143, 278, 169, 9, 2, 118, 13, 58, 90, 7, 209, 111, 37, 280, 283, 249, 86, 46, 195, 275, 223, 34, 95, 130, 207, 225, 59, 190, 151, 45, 177, 126, 156, 116, 100, 274, 192, 236, 164, 11, 260, 56, 298, 221, 88, 109, 124, 183, 117, 119, 96, 44, 186, 70, 158, 137, 279, 218, 181, 265, 80, 267, 202, 101, 64, 16, 289, 94, 12, 157, 250, 73, 91, 36, 273, 35, 161, 134, 166, 231, 14, 270, 15, 233, 98, 197, 285, 232, 293, 92, 153, 144, 54, 139, 29, 241, 32, 152, 33, 76, 84, 217, 219, 204, 150, 38, 61, 239, 203, 277, 89, 286, 266, 71, 49, 292, 129, 40]\n",
      "tensor([[-3.3842e-01,  1.6901e+00, -1.8987e+00,  ...,  5.0981e-02,\n",
      "          4.6265e-01,  6.6756e-01],\n",
      "        [ 2.1892e-03,  2.1569e-01, -4.7295e-02,  ...,  1.5882e-01,\n",
      "         -1.0036e+00, -1.6020e-01],\n",
      "        [-1.5362e+00, -1.4659e-01, -4.3534e-01,  ..., -1.9187e-01,\n",
      "         -5.1484e-01, -7.0315e-01],\n",
      "        ...,\n",
      "        [-2.4512e+00, -1.0245e+00, -1.5181e+00,  ..., -1.4532e+00,\n",
      "         -1.0773e+00, -4.6821e-01],\n",
      "        [-9.2478e-01, -3.6423e-01, -1.0964e+00,  ...,  3.3037e-01,\n",
      "         -2.2902e-01,  3.0449e-01],\n",
      "        [ 7.8360e-01, -4.7116e-01,  6.5296e-01,  ..., -4.0646e-01,\n",
      "          4.6076e-01,  9.0516e-01]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Assuming you have 5 vectors stored in a tensor called 'vectors'\n",
    "#vectors = torch.randn(5, 10)  # Assuming each vector has size 10\n",
    "\n",
    "\n",
    "# Compute pairwise distances between all vectors\n",
    "\n",
    "def get_highest_l1_vectors(vectors, ammount=300):\n",
    "    distances = torch.cdist(vectors, vectors)\n",
    "    \n",
    "    # Initialize a list to store the sampled indices\n",
    "    sampled_indices = []\n",
    "    \n",
    "    # Start by randomly selecting the first index\n",
    "    sampled_indices.append(torch.randint(0, vectors.size(0), (1,)).item())\n",
    "    \n",
    "    # Repeat until you have sampled 3 vectors\n",
    "    while len(sampled_indices) < ammount:\n",
    "        # Compute the distances from the already sampled vectors to all others\n",
    "        sampled_distances = distances[sampled_indices, :].min(dim=0).values\n",
    "        \n",
    "        # Select the index with the maximum distance as the next sampled index\n",
    "        next_index = sampled_distances.argmax().item()\n",
    "        # Add the index to the list of sampled indices\n",
    "        sampled_indices.append(next_index)\n",
    "    \n",
    "    # Extract the sampled vectors\n",
    "    return vectors[sampled_indices], sampled_indices\n",
    "\n",
    "vectors = torch.randn(300, 2048)\n",
    "print(f\"{vectors.shape}\")\n",
    "\n",
    "sampled_vectors, sampled_indices = get_highest_l1_vectors(vectors)\n",
    "\n",
    "print(\"Sampled Vectors:\")\n",
    "print(f\"{sampled_indices}\")\n",
    "print(sampled_vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ce085b-8569-4b18-9385-e2318888306f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "45d80acf-1a3e-4b64-ab3b-549f0147967e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.9950), tensor(0.0705), tensor(0.0996))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from diffusers import DPMSolverMultistepScheduler \n",
    "noise_scheduler = DPMSolverMultistepScheduler(num_train_timesteps=1000, beta_start=0.00085, beta_end=0.012, use_karras_sigmas=False, solver_order=2)\n",
    "\n",
    "idx=10\n",
    "noise_scheduler.alpha_t[idx], (1-noise_scheduler.alpha_t[idx]).sqrt(), (1-noise_scheduler.alphas_cumprod[idx]).sqrt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b056f0f-3a26-4b68-a041-9ef4abef9bf1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7abd5d6d-7413-4d28-a5dd-ca79729fc05e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMPALA ResNet Parameters: 1.56M\n",
      "IMPALA ResNet Parameters: 1.59M\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from nosaveddata import *\n",
    "\n",
    "\n",
    "def get_patches(x, patch_shape):\n",
    "    c, (h, w) = x.shape[1], patch_shape\n",
    "    \n",
    "    return x.unfold(2,h,1).unfold(3,w,1).transpose(1,3).reshape(-1,c,h,w).float()\n",
    "\n",
    "def get_whitening_parameters(patches):\n",
    "    n,c,h,w = patches.shape\n",
    "    patches_flat = patches.view(n, -1)\n",
    "    est_patch_covariance = (patches_flat.T @ patches_flat) / n\n",
    "    \n",
    "    eigenvalues, eigenvectors = torch.linalg.eigh(est_patch_covariance, UPLO='U')\n",
    "    \n",
    "    return eigenvalues.flip(0).view(-1, 1, 1, 1), eigenvectors.T.reshape(c*h*w,c,h,w).flip(0)\n",
    "\n",
    "def init_whitening_conv(layer, train_set, eps=5e-4):\n",
    "    patches = get_patches(train_set, patch_shape=layer.weight.data.shape[2:])\n",
    "    \n",
    "    eigenvalues, eigenvectors = get_whitening_parameters(patches)\n",
    "    \n",
    "    eigenvectors_scaled = eigenvectors / torch.sqrt(eigenvalues + eps)\n",
    "    layer.weight.data[:] = torch.cat((eigenvectors_scaled, -eigenvectors_scaled))\n",
    "    layer.weight.requires_grad=False\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class IMPALA_Resnet_Whitened(nsd_Module):\n",
    "    def __init__(self, first_channels=12, scale_width=1, norm=True, init=init_partial_dirac, act=nn.SiLU()):\n",
    "        super().__init__()\n",
    "        # lhs 2 is because we use concatenate positive and negative eigenvectors, 3 is the kernel size\n",
    "        self.whitened_channels = 2 * first_channels * 3**2\n",
    "        \n",
    "        self.cnn = nn.Sequential(self.whitened_block(first_channels, 16*scale_width),\n",
    "                                 self.get_block(16*scale_width, 32*scale_width),\n",
    "                                 self.get_block(32*scale_width, 32*scale_width, last_relu=True))\n",
    "        \n",
    "        self.cnn[0][1].apply(init)\n",
    "        params_count(self, 'IMPALA ResNet')\n",
    "\n",
    "    def whitened_block(self, in_hiddens, out_hiddens, last_relu=False):\n",
    "        \n",
    "        blocks = nn.Sequential(DQN_Conv(in_hiddens, self.whitened_channels, 3, 1, 1, max_pool=True, act=self.act, norm=self.norm, init=self.init),\n",
    "                               nn.Conv2d(self.whitened_channels,out_hiddens, 1, padding=0, stride=1),\n",
    "                               Residual_Block(out_hiddens, out_hiddens, norm=self.norm, act=self.act, init=self.init),\n",
    "                               Residual_Block(out_hiddens, out_hiddens, norm=self.norm, act=self.act, init=self.init, out_act=self.act if last_relu else nn.Identity())\n",
    "                              )\n",
    "        \n",
    "        return blocks\n",
    "    \n",
    "    def get_block(self, in_hiddens, out_hiddens, last_relu=False):\n",
    "        \n",
    "        blocks = nn.Sequential(DQN_Conv(in_hiddens, out_hiddens, 3, 1, 1, max_pool=True, act=self.act, norm=self.norm, init=self.init),\n",
    "                               Residual_Block(out_hiddens, out_hiddens, norm=self.norm, act=self.act, init=self.init),\n",
    "                               Residual_Block(out_hiddens, out_hiddens, norm=self.norm, act=self.act, init=self.init, out_act=self.act if last_relu else nn.Identity())\n",
    "                              )\n",
    "        \n",
    "        return blocks\n",
    "        \n",
    "    def forward(self, X):\n",
    "        return self.cnn(X)\n",
    "\n",
    "\n",
    "x=torch.randn(500,12,96,72)\n",
    "IMPALA_Resnet(12,scale_width=4)\n",
    "network = IMPALA_Resnet_Whitened(12,scale_width=4)\n",
    "\n",
    "init_whitening_conv(network.cnn[0][0].conv[0], x)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "29c01737-7852-40ff-881c-48d83c11a73b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000e+00, 1.5113e-06, 1.2090e-05, 4.0805e-05, 9.6723e-05, 1.8891e-04,\n",
       "        3.2644e-04, 5.1837e-04, 7.7378e-04, 1.1017e-03, 1.5113e-03, 2.0115e-03,\n",
       "        2.6115e-03, 3.3203e-03, 4.1470e-03, 5.1006e-03, 6.1902e-03, 7.4250e-03,\n",
       "        8.8138e-03, 1.0366e-02, 1.2090e-02, 1.3996e-02, 1.6092e-02, 1.8388e-02,\n",
       "        2.0892e-02, 2.3614e-02, 2.6562e-02, 2.9747e-02, 3.3176e-02, 3.6859e-02,\n",
       "        4.0805e-02, 4.5023e-02, 4.9522e-02, 5.4311e-02, 5.9400e-02, 6.4797e-02,\n",
       "        7.0511e-02, 7.6551e-02, 8.2928e-02, 8.9648e-02, 9.6723e-02, 1.0416e-01,\n",
       "        1.1197e-01, 1.2016e-01, 1.2874e-01, 1.3772e-01, 1.4710e-01, 1.5691e-01,\n",
       "        1.6714e-01, 1.7780e-01, 1.8891e-01, 2.0047e-01, 2.1250e-01, 2.2500e-01,\n",
       "        2.3797e-01, 2.5144e-01, 2.6541e-01, 2.7988e-01, 2.9487e-01, 3.1039e-01,\n",
       "        3.2644e-01, 3.4303e-01, 3.6018e-01, 3.7789e-01, 3.9618e-01, 4.1504e-01,\n",
       "        4.3449e-01, 4.5454e-01, 4.7520e-01, 4.9647e-01, 5.1837e-01, 5.4091e-01,\n",
       "        5.6409e-01, 5.8792e-01, 6.1241e-01, 6.3758e-01, 6.6342e-01, 6.8995e-01,\n",
       "        7.1719e-01, 7.4513e-01, 7.7378e-01])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steps=40000\n",
    "k=5\n",
    "sched = 0.95**k * (torch.arange(steps+1) / steps)**3\n",
    "\n",
    "sched[::500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "babf7648-ba1b-4193-9968-bd7227f706c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00909090909090909\n",
      "0.001818181830458343\n",
      "0.00909090880304575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sneep\\anaconda3\\envs\\python_\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    }
   ],
   "source": [
    "class LookaheadState:\n",
    "    def __init__(self, net, steps, k=5):\n",
    "        self.k=k\n",
    "        self.net_ema = {k: v.clone() for k, v in net.state_dict().items()}\n",
    "        self.sched = 0.95**k * (torch.arange(steps+1) / steps)**3\n",
    "\n",
    "    def update(self, net, step):\n",
    "        decay = self.sched[step].item()\n",
    "        if step%self.k==0:\n",
    "            for ema_param, net_param in zip(self.net_ema.values(), net.state_dict().values()):\n",
    "                ema_param.lerp_(net_param, 1-decay)\n",
    "                net_param.copy_(ema_param)\n",
    "                \n",
    "    def update_fixed_decay(self, net, decay, step):\n",
    "        if step%self.k==0:\n",
    "            for ema_param, net_param in zip(self.net_ema.values(), net.state_dict().values()):\n",
    "                ema_param.lerp_(net_param, 1-decay)\n",
    "                net_param.copy_(ema_param)\n",
    "\n",
    "lookahead_state = LookaheadState(network, 40000)\n",
    "\n",
    "lookahead_state.update(network, 8)\n",
    "\n",
    "def Triangle_Scheduler(optimizer, steps, start=0.2, end=0.07, peak=0.23):\n",
    "    def triangle(steps, start, end, peak):\n",
    "        xp = torch.tensor([0, int(peak * steps), steps])\n",
    "        fp = torch.tensor([start, 1, end])\n",
    "        x = torch.arange(1+steps)\n",
    "        m = (fp[1:] - fp[:-1]) / (xp[1:] - xp[:-1])\n",
    "        b = fp[:-1] - (m * xp[:-1])\n",
    "        indices = torch.sum(torch.ge(x[:, None], xp[None, :]), 1) - 1\n",
    "        indices = torch.clamp(indices, 0, len(m) - 1)\n",
    "        return m[indices] * x + b[indices]\n",
    "    lr_schedule = triangle(steps, start, end, peak)\n",
    "    return torch.optim.lr_scheduler.LambdaLR(optimizer, lambda i: lr_schedule[i])\n",
    "\n",
    "momentum=0.9\n",
    "lr = 0.1 / (1+1/(1-momentum))\n",
    "print(f\"{lr}\")\n",
    "\n",
    "optim = torch.optim.SGD(network.parameters(), lr=lr, weight_decay=0.1, momentum=momentum, nesterov=True)\n",
    "\n",
    "sched = Triangle_Scheduler(optim, 40000)\n",
    "\n",
    "print(f\"{optim.param_groups[0]['lr']}\")\n",
    "for i in range(int(40000*0.23)):\n",
    "    sched.step()\n",
    "print(f\"{optim.param_groups[0]['lr']}\")\n",
    "\n",
    "total_train_steps=40000\n",
    "\n",
    "\n",
    "lookahead_state.update(network, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a0e6cdee-1248-4eb9-9181-36f58ec87c48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.01123046875, 2.2412109374999998e-05)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = 11.5/1024# * (1 + 1 / (1 - 0.85))\n",
    "\n",
    "lr, 0.0153/(1 + 1 / (1 - 0.85))*lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ed1821-d35e-48f5-b41f-a0d4c6eb7e8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ae9d0b46-19f3-4a8c-a404-bcedaf8e30b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv(nn.Conv2d):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, padding='same', bias=False):\n",
    "        super().__init__(in_channels, out_channels, kernel_size=kernel_size, padding=padding, bias=bias)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        super().reset_parameters()\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.zero_()\n",
    "        w = self.weight.data\n",
    "        torch.nn.init.dirac_(w[:w.size(1)])\n",
    "\n",
    "def init_partial_dirac(module):\n",
    "    if type(module) in (nn.Linear, nn.Conv2d, nn.Conv1d, nn.Conv3d):\n",
    "        w = module.weight.data\n",
    "        \n",
    "        nn.init.dirac_(module.weight[:w.shape[1]])\n",
    "        nn.init.xavier_uniform_(module.weight[w.shape[1]:], gain=1)\n",
    "\n",
    "        if module.bias is not None:\n",
    "            nn.init.zeros_(module.bias)\n",
    "            \n",
    "\n",
    "model = Conv(16,32)\n",
    "model.apply(init_partial_dirac)\n",
    "last_relu=False\n",
    "act=nn.ReLU()\n",
    "\n",
    "in_hiddens=16\n",
    "out_hiddens=32\n",
    "\n",
    "norm = False\n",
    "model = nn.Sequential(DQN_Conv(in_hiddens, out_hiddens, 3, 1, 1, max_pool=True, act=act, norm=norm, init=init_partial_dirac),\n",
    "                               Residual_Block(out_hiddens, out_hiddens, norm=norm, act=act, init=init_partial_dirac),\n",
    "                               Residual_Block(out_hiddens, out_hiddens, norm=norm, act=act, init=init_partial_dirac, out_act=act if last_relu else nn.Identity()),\n",
    "                               MLP(512,512)\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "793d1ae1-ad48-46d0-9240-477a3b3a9634",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "espeak not installed on your system",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mphonemizer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m phonemize\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mphonemizer\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EspeakBackend\n\u001b[1;32m----> 5\u001b[0m backend \u001b[38;5;241m=\u001b[39m EspeakBackend(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124men-us\u001b[39m\u001b[38;5;124m'\u001b[39m, preserve_punctuation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, with_stress\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21menglish_cleaners2\u001b[39m(text):\n\u001b[0;32m      8\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m'''Pipeline for English text, including abbreviation expansion. + punctuation + stress'''\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\python_\\Lib\\site-packages\\phonemizer\\backend\\espeak\\espeak.py:45\u001b[0m, in \u001b[0;36mEspeakBackend.__init__\u001b[1;34m(self, language, punctuation_marks, preserve_punctuation, with_stress, tie, language_switch, words_mismatch, logger)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, language: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m     38\u001b[0m              punctuation_marks: Optional[Union[\u001b[38;5;28mstr\u001b[39m, Pattern]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     39\u001b[0m              preserve_punctuation: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     43\u001b[0m              words_mismatch: WordMismatch \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     44\u001b[0m              logger: Optional[Logger] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m---> 45\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m     46\u001b[0m         language, punctuation_marks\u001b[38;5;241m=\u001b[39mpunctuation_marks,\n\u001b[0;32m     47\u001b[0m         preserve_punctuation\u001b[38;5;241m=\u001b[39mpreserve_punctuation, logger\u001b[38;5;241m=\u001b[39mlogger)\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_espeak\u001b[38;5;241m.\u001b[39mset_voice(language)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_with_stress \u001b[38;5;241m=\u001b[39m with_stress\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\python_\\Lib\\site-packages\\phonemizer\\backend\\espeak\\base.py:39\u001b[0m, in \u001b[0;36mBaseEspeakBackend.__init__\u001b[1;34m(self, language, punctuation_marks, preserve_punctuation, logger)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, language: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m     36\u001b[0m              punctuation_marks: Optional[Union[\u001b[38;5;28mstr\u001b[39m, Pattern]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     37\u001b[0m              preserve_punctuation: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     38\u001b[0m              logger: Optional[Logger] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m---> 39\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m     40\u001b[0m         language,\n\u001b[0;32m     41\u001b[0m         punctuation_marks\u001b[38;5;241m=\u001b[39mpunctuation_marks,\n\u001b[0;32m     42\u001b[0m         preserve_punctuation\u001b[38;5;241m=\u001b[39mpreserve_punctuation,\n\u001b[0;32m     43\u001b[0m         logger\u001b[38;5;241m=\u001b[39mlogger)\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_espeak \u001b[38;5;241m=\u001b[39m EspeakWrapper()\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloaded \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_espeak\u001b[38;5;241m.\u001b[39mlibrary_path)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\python_\\Lib\\site-packages\\phonemizer\\backend\\base.py:77\u001b[0m, in \u001b[0;36mBaseBackend.__init__\u001b[1;34m(self, language, punctuation_marks, preserve_punctuation, logger)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;66;03m# ensure the backend is installed on the system\u001b[39;00m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[1;32m---> 77\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(  \u001b[38;5;66;03m# pragma: nocover\u001b[39;00m\n\u001b[0;32m     78\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m not installed on your system\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname()))\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_logger \u001b[38;5;241m=\u001b[39m logger\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minitializing backend \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mstr\u001b[39m(v) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mversion()))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: espeak not installed on your system"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from unidecode import unidecode\n",
    "from phonemizer import phonemize\n",
    "from phonemizer.backend import EspeakBackend\n",
    "backend = EspeakBackend('en-us', preserve_punctuation=True, with_stress=True)\n",
    "\n",
    "def english_cleaners2(text):\n",
    "  '''Pipeline for English text, including abbreviation expansion. + punctuation + stress'''\n",
    "  text = unidecode(text)\n",
    "  print(f\"{text}\")\n",
    "  text = text.lower()\n",
    "    \n",
    "  print(f\"{text}\")\n",
    "  phonemes = backend.phonemize([text], strip=True)[0]\n",
    "  phonemes = collapse_whitespace(phonemes)\n",
    "  return phonemes\n",
    "\n",
    "english_cleaners2(\"Olá doutor\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2a41eb-227b-43f1-a3fe-4938beb7667b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(10,30)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "model = Model()\n",
    "model = torch.compile(model)\n",
    "\n",
    "x=torch.randn(1,10)\n",
    "model(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb20ac00-c5b6-4a8c-81a4-7bfaff184ec2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d80d672-4bfe-42fb-88be-5e3fd3c0d36f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc37ab2-106c-40fa-92ba-eb7fbc318e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import epitran\n",
    "\n",
    "# Carregar o modelo em português\n",
    "nlp = spacy.load('pt_core_news_sm')\n",
    "\n",
    "# Texto de exemplo\n",
    "texto = \"O cachorro correu pelo parque.\"\n",
    "\n",
    "#texto = \"Exemplo de texto para fonemização.\"\n",
    "\n",
    "doc = nlp(texto)\n",
    "for token in doc:\n",
    "    try:\n",
    "        fonemas = fonemizar_texto(token.text)\n",
    "        print(f'{token.text} -> {fonemas}')\n",
    "    except Exception as e:\n",
    "        pass\n",
    "\n",
    "\n",
    "fonemizar_texto(tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293c8a2d-db7d-48e3-ae9d-19cfc3391aa2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608559a3-677c-431e-8a9a-8fb8461a71a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2825037a-8e22-4865-a828-bd33bd2f6c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nosaveddata import *\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self, len_state, num_quant, num_actions):\n",
    "        nn.Module.__init__(self)\n",
    "       \n",
    "        self.num_quant = num_quant\n",
    "        self.num_actions = num_actions\n",
    "       \n",
    "        self.layer1 = nn.Linear(len_state, 256)\n",
    "        self.layer2 = nn.Linear(256, num_actions*num_quant)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = torch.tanh(x)\n",
    "        x = self.layer2(x)\n",
    "        return x.view(-1, self.num_actions, self.num_quant)\n",
    "   \n",
    "    def select_action(self, state, eps):\n",
    "        if not isinstance(state, torch.Tensor):\n",
    "            state = torch.Tensor([state])    \n",
    "        action = torch.randint(0, 2, (1,))\n",
    "        if random.random() > eps:\n",
    "            action = self.forward(state).mean(2).max(1)[1]\n",
    "        return int(action)\n",
    "   \n",
    "\n",
    "eps_start, eps_end, eps_dec = 0.9, 0.1, 500\n",
    "eps = lambda steps: eps_end + (eps_start - eps_end) * np.exp(-1. * steps / eps_dec)\n",
    "\n",
    "Z = Network(len_state=8, num_quant=2, num_actions=7)\n",
    "Ztgt = Network(len_state=8, num_quant=2, num_actions=7)\n",
    "Ztgt.load_state_dict(Z.state_dict())\n",
    "tau = torch.Tensor((2 * np.arange(Z.num_quant) + 1) / (2.0 * Z.num_quant)).view(1, -1)\n",
    "\n",
    "batch_size=3\n",
    "\n",
    "def huber(x, k=1.0):\n",
    "    return torch.where(x.abs() < k, 0.5 * x.pow(2), k * (x.abs() - 0.5 * k))\n",
    "\n",
    "next_states = torch.randn(batch_size,8)\n",
    "states = next_states + torch.randn(batch_size,8)*0.01\n",
    "\n",
    "gamma=0.997\n",
    "rewards=torch.ones(batch_size,1)\n",
    "\n",
    "\n",
    "theta = Z(states)\n",
    "print(f\"{theta.shape, theta.mean(-1).argmax(-1)}\")\n",
    "theta = theta[np.arange(batch_size), theta.mean(2).max(1)[1]]\n",
    "\n",
    "\n",
    "Znext = Ztgt(next_states).detach()\n",
    "Znext_max = Znext[np.arange(batch_size), Znext.mean(2).max(1)[1]]\n",
    "\n",
    "print(f\"{Znext.mean(2).max(1)[1]}\")\n",
    "\n",
    "Ttheta = rewards + gamma  * Znext_max\n",
    "\n",
    "print(f\"{Ttheta.t()[..., None].shape, theta.shape}\")\n",
    "\n",
    "diff = Ttheta.t()[..., None] - theta\n",
    "\n",
    "loss = huber(diff) * (tau - (diff.detach() < 0).float()).abs()\n",
    "\n",
    "\n",
    "loss, diff, Ttheta, theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f11d706-7d17-40e9-80f3-97f58142ea74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf15e4c3-6015-41d0-b314-1dca5148c5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nosaveddata import *\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "a=torch.arange(2,device='cuda').long()[:,None].repeat_interleave(15,0)\n",
    "\n",
    "a,torch.zeros(6,1,device='cuda').long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac57f6d-e45c-43c9-a29f-e94d81fc69b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "\n",
    "p1 = 0.6697\n",
    "p2 = 0.6649\n",
    "n = 10000\n",
    "\n",
    "def statistical_difference(p1, p2, n):\n",
    "    \n",
    "    d=torch.tensor(p1-p2).abs()\n",
    "\n",
    "    std = 1.65 * math.sqrt((p1*(1-p1) + p2*(1-p2))/n)\n",
    "    \n",
    "    difference = torch.tensor([d-std, d+std])\n",
    "    \n",
    "    return difference.sort()[0]\n",
    "\n",
    "print(statistical_difference(0.834, 0.831, 100000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8e20d0-e2d2-4ce7-9c42-12aea4125e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nosaveddata import *\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class IMPALA_Resnet(nn.Module):\n",
    "    def __init__(self, first_channels=12, scale_width=1, norm=True, init=init_relu, act=nn.SiLU()):\n",
    "        super().__init__()\n",
    "        self.norm=norm\n",
    "        self.init=init\n",
    "        self.act =act\n",
    "        \n",
    "        self.cnn = nn.Sequential(self.get_block(first_channels, 16*scale_width),\n",
    "                                 self.get_block(16*scale_width, 32*scale_width),\n",
    "                                 self.get_block(32*scale_width, 32*scale_width, last_relu=True))\n",
    "        params_count(self, 'IMPALA ResNet')\n",
    "    def get_block(self, in_hiddens, out_hiddens, last_relu=False):\n",
    "        \n",
    "        blocks = nn.Sequential(DQN_Conv(in_hiddens, out_hiddens, 3, 1, 1, max_pool=True, act=self.act, norm=self.norm, init=self.init),\n",
    "                               Residual_Block(out_hiddens, out_hiddens, norm=self.norm, act=self.act, init=self.init),\n",
    "                               Residual_Block(out_hiddens, out_hiddens, norm=self.norm, act=self.act, init=self.init, out_act=self.act if last_relu else nn.Identity())\n",
    "                              )\n",
    "        \n",
    "        return blocks\n",
    "        \n",
    "    def forward(self, X):\n",
    "        return self.cnn(X)\n",
    "\n",
    "\n",
    "class IMPALA_YY(nn.Module):\n",
    "    def __init__(self, first_channels=12, scale_width=1, norm=True, init=init_relu, act=nn.SiLU()):\n",
    "        super().__init__()\n",
    "        self.norm=norm\n",
    "        self.init=init\n",
    "        self.act =act\n",
    "\n",
    "        self.yin = self.get_yin(first_channels, 16*scale_width, 32*scale_width)\n",
    "        \n",
    "        self.yang = self.get_yang(first_channels, 16*scale_width)\n",
    "                                 \n",
    "        self.head = nn.Sequential(self.get_yang(16*scale_width, 32*scale_width),\n",
    "                                  self.get_yang(32*scale_width, 32*scale_width, last_relu=True))\n",
    "        \n",
    "        params_count(self, 'IMPALA ResNet')\n",
    "\n",
    "    def get_yin(self, in_hiddens, hiddens, out_hiddens):\n",
    "        blocks = nn.Sequential(DQN_Conv(1, hiddens, 3, 1, 1, max_pool=True, act=self.act, norm=self.norm, init=self.init),\n",
    "                               Residual_Block(hiddens, hiddens, norm=self.norm, act=self.act, init=self.init),\n",
    "                               #DQN_Conv(hiddens, out_hiddens, 3, 1, 1, max_pool=True, act=self.act, norm=self.norm, init=self.init),\n",
    "                               #Residual_Block(out_hiddens, out_hiddens, norm=self.norm, act=self.act, init=self.init),\n",
    "                               #Residual_Block(out_hiddens, out_hiddens, norm=self.norm, act=self.act, init=self.init)\n",
    "                              )\n",
    "        return blocks          \n",
    "        \n",
    "    def get_yang(self, in_hiddens, out_hiddens, last_relu=False):\n",
    "        \n",
    "        blocks = nn.Sequential(DQN_Conv(in_hiddens, out_hiddens, 3, 1, 1, max_pool=True, act=self.act, norm=self.norm, init=self.init),\n",
    "                               Residual_Block(out_hiddens, out_hiddens, norm=self.norm, act=self.act, init=self.init),\n",
    "                               Residual_Block(out_hiddens, out_hiddens, norm=self.norm, act=self.act, init=self.init, out_act=self.act if last_relu else nn.Identity())\n",
    "                              )\n",
    "        \n",
    "        return blocks\n",
    "    \n",
    "    def forward(self, X):\n",
    "\n",
    "        y = self.yin(X[:,-3:].mean(-3)[:,None])\n",
    "        x = self.yang(X)\n",
    "        \n",
    "        X = x*(1-y) + x + y\n",
    "        \n",
    "        return self.head(X)\n",
    "\n",
    "model = IMPALA_Resnet(scale_width=4)\n",
    "x=torch.randn(32,12,96,72)\n",
    "model2 = IMPALA_YY(scale_width=4)\n",
    "\n",
    "model(x).shape, model2(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dcc521d-7bac-44e8-9584-ce0bab652aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from nosaveddata import *\n",
    "\n",
    "seed_np_torch(42)\n",
    "\n",
    "def network_ema(target_network, new_network, alpha=0.5):\n",
    "    for (param_name, param_target), param_new  in zip(target_network.cuda().named_parameters(), new_network.parameters()):\n",
    "        if 'ln' in param_name: #layer norm\n",
    "            param_target.data = param_new.data.clone()\n",
    "        else:\n",
    "            param_target.data = alpha * param_target.data + (1 - alpha) * param_new.data.clone()\n",
    "\n",
    "\n",
    "class Modeld(nsd_Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear = nn.Linear(10,32)\n",
    "        self.ln = nn.LayerNorm(32)\n",
    "\n",
    "    def forward(self,X):\n",
    "        return self.ln(self.linear(X))\n",
    "\n",
    "m = Modeld().cuda()\n",
    "m_rand= Modeld().cuda()\n",
    "\n",
    "\n",
    "optim=torch.optim.AdamW(m.parameters(), lr=1e-4)\n",
    "\n",
    "for i in range(4000):\n",
    "    x=torch.randn(1,10).cuda()\n",
    "    \n",
    "    loss = m(x).sum()\n",
    "    loss.backward()\n",
    "    \n",
    "    optim.step()\n",
    "    optim.zero_grad()\n",
    "\n",
    "network_ema(m,m_rand)\n",
    "\n",
    "m.ln.weight, m.linear.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3912798-5ac5-43f3-b591-98c21b5f71e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nosaveddata import *\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "model = nn.Linear(10,2).cuda()\n",
    "model.apply(init_xavier)\n",
    "model2 = nn.Linear(10,2).cuda()\n",
    "network_ema(model, model2, 0)\n",
    "model.apply(init_xavier)\n",
    "\n",
    "model.weight.data==model2.weight.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe94662c-07ea-4abb-b390-a28cf08d5a81",
   "metadata": {},
   "source": [
    "<h1>Preprocessing</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3a89a6-7f64-4075-8138-50b0e8229dd8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import os, glob\n",
    "from nosaveddata import *\n",
    "\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "paths = glob.glob('C:/Users/Augusto/Python/PyTorch/RL/mc_data/4/2023_01_09_14_48_09_100636/*.jpg')\n",
    "path = 'C:/Users/Augusto/Python/PyTorch/RL/mc_data/4/2023_01_09_14_48_09_100636/7,0,0,0,0,0,0,0,0,0,0,0,0,3,0,.jpg'\n",
    "\n",
    "\n",
    "\n",
    "tfms = transforms.Compose([\n",
    "                           transforms.Resize((96, 72)),\n",
    "                           transforms.ToTensor()\n",
    "                        ])\n",
    "\n",
    "img = Image.open(path)\n",
    "imgs=[]\n",
    "for p in paths:\n",
    "    imgs.append(tfms(Image.open(p)))\n",
    "imgs=torch.stack(imgs)\n",
    "\n",
    "print(imgs.shape)\n",
    "\n",
    "\n",
    "\n",
    "imgs, augments_applied = preprocess_iwm_no_solarize(imgs)\n",
    "    \n",
    "\n",
    "\n",
    "#plt.imshow(img_tfms)\n",
    "plot_imgs(imgs.permute(0,2,3,1))\n",
    "augments_applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a6f04e-3fe2-4c94-8984-2e74348e007b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from nosaveddata import *\n",
    "\n",
    "\n",
    "\n",
    "def gray_scale_stacked(X, p=0.2, stacks=4):\n",
    "    # Input: Tensor T e (B,C,T,D)\n",
    "    \n",
    "    probs = get_img_preprocessing_prob(X.shape[0], p, X.device)\n",
    "    stacked_probs = probs.repeat_interleave(stacks,0)\n",
    "    X = X.view(-1,X.shape[1]//stacks,*X.shape[-2:])\n",
    "    \n",
    "    gray_img = X.mean(1,keepdim=True).expand(-1,3,-1,-1)\n",
    "    \n",
    "    X = (1-stacked_probs)*X + stacked_probs*gray_img\n",
    "    \n",
    "    return X.view(X.shape[0]//stacks, -1, *X.shape[-2:]), probs.squeeze()\n",
    "\n",
    "def gaussian_blur(X, p=0.2, stacks=4, sigma_min=0.1, sigma_max=2):\n",
    "    # Input: Tensor T e (B,C,T,D)\n",
    "    \n",
    "    probs = get_img_preprocessing_prob(X.shape[0], p, X.device)\n",
    "    tfms = transforms.GaussianBlur(3, (sigma_min, sigma_max))\n",
    "    \n",
    "    blurred = tfms(X)\n",
    "    X = (1-probs)*X + probs*blurred\n",
    "    \n",
    "    return X, probs.squeeze()\n",
    "\n",
    "def solarization_stacked(X, p=0.2, stacks=4):\n",
    "    # Input: Tensor T e (B,C,T,D)\n",
    "\n",
    "    probs = get_img_preprocessing_prob(X.shape[0], p, X.device)\n",
    "    stacked_probs = probs.repeat_interleave(stacks,0)\n",
    "    \n",
    "    X = X.view(-1,X.shape[1]//stacks,*X.shape[-2:])\n",
    "    \n",
    "    tfms = transforms.RandomSolarize(0,p=1) # This prob is applied over all the batch or no image at all\n",
    "    \n",
    "    solarized = tfms(X)\n",
    "    X = (1-stacked_probs)*X + stacked_probs*solarized\n",
    "    \n",
    "    return X.view(X.shape[0]//stacks, -1, *X.shape[-2:]), probs.squeeze()\n",
    "\n",
    "\n",
    "def preprocess_iwm_stacked(imgs, p=0.2, stacks=4):\n",
    "    # Applies the same preprocessing for all images in the sequence, but separated by each beach\n",
    "    augments_applied=[]\n",
    "    \n",
    "    imgs, augmented = gray_scale_stacked(imgs, p, stacks)\n",
    "    augments_applied.append(augmented)\n",
    "    \n",
    "    imgs, augmented = gaussian_blur_stacked(imgs, p, stacks)\n",
    "    augments_applied.append(augmented)\n",
    "    \n",
    "    imgs, augmented = solarization_stacked(imgs, p, stacks)\n",
    "    augments_applied.append(augmented)\n",
    "    \n",
    "    augments_applied = torch.stack(augments_applied,1)\n",
    "    return imgs, augments_applied\n",
    "\n",
    "preprocess_iwm_stacked(torch.randn(32,12,96,72, device='cuda'))[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880deeee-5d84-47ef-9775-583dffaba410",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_img(imgs[-1].permute(1,2,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c2cb75-c68e-4775-aa5f-0a5afcec9a9a",
   "metadata": {},
   "source": [
    "<h1>DiT</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892bf794-e949-4766-a710-0b7002597581",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from nosaveddata import *\n",
    "\n",
    "unet = UNet_DiT_S_4(in_channels=4).cuda()\n",
    "x=torch.randn(32,4,32,32).cuda()\n",
    "c=torch.randn(32,384).cuda()\n",
    "t=torch.randint(0,1000,(32,)).cuda()\n",
    "unet(x,t).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea6bc6f-523d-4dfa-8ae1-8412cd2d0b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from nosaveddata import *\n",
    "\n",
    "\n",
    "model = DiT_Transformer(128, 8, 8, 108).cuda()\n",
    "\n",
    "X = torch.randn(16,108,128).cuda()\n",
    "c = torch.randn(16,128).cuda()\n",
    "\n",
    "model(X, c).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36203835-4741-4e59-b1af-fdf05b13c46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "list(torch.tensor([1,2,3,4]).split(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86dcafb6-0a86-4b23-a858-ffd84d0d0aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from nosaveddata import *\n",
    "\n",
    "model = UNet_DiT_1D(2048, 256, 2, 2048//64, seq_len=5).cuda()\n",
    "\n",
    "X = torch.randn(16,5,2048).cuda()\n",
    "t=torch.randint(0,1000,(16,)).cuda()\n",
    "c = torch.randn(16,256).cuda()\n",
    "\n",
    "model(X, t, c).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6a7bc2-2f1d-4bd2-a116-59fe67d4e0f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c34211d-6121-49b9-8e8e-ac989892c917",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
