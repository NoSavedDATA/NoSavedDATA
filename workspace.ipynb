{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "7d60d1d3-e4c8-44f5-9eb1-aec42c84f229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT Transformer Parameters: 1.59M\n",
      "input tensor(0., device='cuda:0', grad_fn=<MeanBackward0>) tensor(1.0000, device='cuda:0', grad_fn=<StdBackward0>)\n",
      "attn tensor(0.0100, device='cuda:0', grad_fn=<MeanBackward0>) tensor(1.0098, device='cuda:0', grad_fn=<StdBackward0>)\n",
      "ffn tensor(0.0125, device='cuda:0', grad_fn=<MeanBackward0>) tensor(1.0106, device='cuda:0', grad_fn=<StdBackward0>)\n",
      "\n",
      "input tensor(0.0125, device='cuda:0', grad_fn=<MeanBackward0>) tensor(1.0106, device='cuda:0', grad_fn=<StdBackward0>)\n",
      "attn tensor(0.0062, device='cuda:0', grad_fn=<MeanBackward0>) tensor(1.0179, device='cuda:0', grad_fn=<StdBackward0>)\n",
      "ffn tensor(0.0049, device='cuda:0', grad_fn=<MeanBackward0>) tensor(1.0159, device='cuda:0', grad_fn=<StdBackward0>)\n",
      "\n",
      "input tensor(0.0049, device='cuda:0', grad_fn=<MeanBackward0>) tensor(1.0159, device='cuda:0', grad_fn=<StdBackward0>)\n",
      "attn tensor(0.0049, device='cuda:0', grad_fn=<MeanBackward0>) tensor(1.0479, device='cuda:0', grad_fn=<StdBackward0>)\n",
      "ffn tensor(0.0055, device='cuda:0', grad_fn=<MeanBackward0>) tensor(1.0575, device='cuda:0', grad_fn=<StdBackward0>)\n",
      "\n",
      "input tensor(0.0055, device='cuda:0', grad_fn=<MeanBackward0>) tensor(1.0575, device='cuda:0', grad_fn=<StdBackward0>)\n",
      "attn tensor(-0.0286, device='cuda:0', grad_fn=<MeanBackward0>) tensor(1.0652, device='cuda:0', grad_fn=<StdBackward0>)\n",
      "ffn tensor(-0.0272, device='cuda:0', grad_fn=<MeanBackward0>) tensor(1.0663, device='cuda:0', grad_fn=<StdBackward0>)\n",
      "\n",
      "input tensor(-0.0272, device='cuda:0', grad_fn=<MeanBackward0>) tensor(1.0663, device='cuda:0', grad_fn=<StdBackward0>)\n",
      "attn tensor(-0.0095, device='cuda:0', grad_fn=<MeanBackward0>) tensor(1.0558, device='cuda:0', grad_fn=<StdBackward0>)\n",
      "ffn tensor(-0.0143, device='cuda:0', grad_fn=<MeanBackward0>) tensor(1.0570, device='cuda:0', grad_fn=<StdBackward0>)\n",
      "\n",
      "input tensor(-0.0143, device='cuda:0', grad_fn=<MeanBackward0>) tensor(1.0570, device='cuda:0', grad_fn=<StdBackward0>)\n",
      "attn tensor(-0.0315, device='cuda:0', grad_fn=<MeanBackward0>) tensor(1.0556, device='cuda:0', grad_fn=<StdBackward0>)\n",
      "ffn tensor(-0.0359, device='cuda:0', grad_fn=<MeanBackward0>) tensor(1.0528, device='cuda:0', grad_fn=<StdBackward0>)\n",
      "\n",
      "input tensor(-0.0359, device='cuda:0', grad_fn=<MeanBackward0>) tensor(1.0528, device='cuda:0', grad_fn=<StdBackward0>)\n",
      "attn tensor(-0.0318, device='cuda:0', grad_fn=<MeanBackward0>) tensor(1.0536, device='cuda:0', grad_fn=<StdBackward0>)\n",
      "ffn tensor(-0.0319, device='cuda:0', grad_fn=<MeanBackward0>) tensor(1.0452, device='cuda:0', grad_fn=<StdBackward0>)\n",
      "\n",
      "input tensor(-0.0319, device='cuda:0', grad_fn=<MeanBackward0>) tensor(1.0452, device='cuda:0', grad_fn=<StdBackward0>)\n",
      "attn tensor(-0.0157, device='cuda:0', grad_fn=<MeanBackward0>) tensor(1.0580, device='cuda:0', grad_fn=<StdBackward0>)\n",
      "ffn tensor(-0.0222, device='cuda:0', grad_fn=<MeanBackward0>) tensor(1.0584, device='cuda:0', grad_fn=<StdBackward0>)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 128, 128]),\n",
       " tensor([-0.0494, -0.0419,  0.0476], device='cuda:0', grad_fn=<SliceBackward0>),\n",
       " tensor([ 0.0256,  0.0170, -0.0095], device='cuda:0', grad_fn=<SliceBackward0>),\n",
       " tensor([-0.0228, -0.0082,  0.0009], device='cuda:0', grad_fn=<SliceBackward0>))"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from nosaveddata import *\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#model = ViT(128, 8, 4, first_channel=3).cuda()\n",
    "\n",
    "#model(torch.randn(16,3,96,72).cuda()).shape\n",
    "\n",
    "class Transformer_Block_NoLN(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dropout=0.0, bias=False, ffn_mult=4):\n",
    "        super().__init__()\n",
    "        self.attn = Attention(d_model, num_heads, bias, dropout)\n",
    "        self.mlp = FFN(d_model, dropout, bias, ffn_mult)\n",
    "\n",
    "    def forward(self, x, is_causal=True):\n",
    "        #x = renormalize(x)\n",
    "        means, stds = 0, 0\n",
    "        print('input', x.mean(), x.std())\n",
    "        x = x + self.attn(x, x, x, is_causal=is_causal)\n",
    "        print('attn', x.mean(), x.std())\n",
    "        x = x + self.mlp(x)\n",
    "        print('ffn', x.mean(), x.std())\n",
    "        print()\n",
    "        return x, means/3, stds/3\n",
    "\n",
    "class Transformer_NoDATA(nn.Module):\n",
    "    def __init__(self, d_model, num_blks, nhead, seq_len,\n",
    "                 dropout = 0.1, bias=False, report_params_count=True,\n",
    "                 ffn_mult=4, scale_init=1):\n",
    "        super().__init__()\n",
    "        self.num_hiddens = d_model\n",
    "        if scale_init==1:\n",
    "            scale_init=num_blks\n",
    "        \n",
    "        self.pos_encoding = nn.Linear(seq_len, d_model, bias=False)\n",
    "        \n",
    "        self.final_ln = LayerNormNoBias(d_model)\n",
    "        self.start_dropout = nn.Dropout(dropout)\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        self.blks = nn.Sequential()\n",
    "        for i in range(num_blks):\n",
    "            self.blks.add_module(\"block\"+str(i), Transformer_Block_NoLN(\n",
    "                                d_model, nhead, dropout, bias=False, ffn_mult=ffn_mult))\n",
    "            \n",
    "        \n",
    "        \n",
    "        # https://proceedings.mlr.press/v119/huang20f/huang20f.pdf\n",
    "        \n",
    "        self.apply(init_xavier)\n",
    "        #self.apply(self._init_weights)\n",
    "        \n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('proj.weight') or pn.endswith('W_v.weight') or pn.endswith('fc.weight') or pn.endswith('pos_encoding.weight'):\n",
    "                torch.nn.init.xavier_uniform_(p, gain=(torch.tensor(9*scale_init,dtype=torch.float)).pow(-1/4))\n",
    "        #for pn, p in self.named_parameters():\n",
    "        #    if pn.endswith('proj.weight'):\n",
    "        #        torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * num_blks))\n",
    "        \n",
    "        if report_params_count:\n",
    "            params_to_count = [p for p in self.parameters() if p.requires_grad]\n",
    "            print(f'GPT Transformer Parameters: {sum(p.numel() for p in params_to_count)/1e6:.2f}M')\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Embedding):\n",
    "            #torch.nn.init.normal_(module.weight, mean=0.0, std=1/math.sqrt(self.num_hiddens))\n",
    "            torch.nn.init.xavier_uniform_(p, gain=(torch.tensor(9*scale_init,dtype=torch.float)).pow(-2))\n",
    "            \n",
    "    \n",
    "\n",
    "        \n",
    "    def forward(self, X, is_causal=True):\n",
    "\n",
    "        pos = torch.arange(0, self.seq_len, dtype=torch.float32, device='cuda')\n",
    "        pos_emb = self.pos_encoding(pos)\n",
    "        X = self.start_dropout(X+pos_emb)\n",
    "        X = self.final_ln(X)\n",
    "        \n",
    "        for i, blk in enumerate(self.blks):\n",
    "            X = blk(X, is_causal)\n",
    "            \n",
    "        return X\n",
    "\n",
    "model = Transformer_NoDATA(128, 8, 4, seq_len=128, dropout=0).cuda()\n",
    "model(torch.randn(32,128,128).cuda()).shape, model.blks[1].attn.W_v.weight[0,0:3], model.blks[1].mlp.fc.weight[0,0:3], model.blks[1].mlp.proj.weight[0,0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "93f8b604-d14c-4e06-b449-8acf9256a24a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0156)"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(torch.tensor(8,dtype=torch.float)).pow(-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5f0734bc-5ed9-4587-aa13-ec4b4f65c506",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch.nn.functional' has no attribute 'smooth_l2_loss'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 13\u001b[0m\n\u001b[0;32m      8\u001b[0m b\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m8\u001b[39m,\u001b[38;5;241m4\u001b[39m,\u001b[38;5;241m16\u001b[39m)\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m#a=F.normalize(a,dim=-1)\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m#b=F.normalize(b,dim=-1)\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msmooth_l2_loss\u001b[49m(a,b), ((a\u001b[38;5;241m-\u001b[39mb)\u001b[38;5;241m.\u001b[39mpow(\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mmean()\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'torch.nn.functional' has no attribute 'smooth_l2_loss'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from nosaveddata import *\n",
    "\n",
    "a=torch.randn(8,4,16).cuda()\n",
    "b=torch.randn(8,4,16).cuda()\n",
    "\n",
    "#a=F.normalize(a,dim=-1)\n",
    "#b=F.normalize(b,dim=-1)\n",
    "\n",
    "F.smooth_l1_loss(a,b), ((a-b).pow(2)/2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c9acff02-da1c-4c8f-b8c8-2fccf6b7ad05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[27,  9],\n",
       "         [ 2, 10],\n",
       "         [ 4, 24],\n",
       "         [30, 12],\n",
       "         [18, 13],\n",
       "         [30, 25],\n",
       "         [ 2, 21],\n",
       "         [ 5,  0],\n",
       "         [ 1, 11],\n",
       "         [ 9, 21],\n",
       "         [26, 14],\n",
       "         [22, 27],\n",
       "         [29, 19],\n",
       "         [ 0,  6],\n",
       "         [11, 19],\n",
       "         [ 3, 13]]),\n",
       " tensor([[ 7, 32],\n",
       "         [ 0, 14],\n",
       "         [15, 19],\n",
       "         [ 5, 12],\n",
       "         [ 9, 13],\n",
       "         [14, 28],\n",
       "         [ 3, 13],\n",
       "         [17, 19],\n",
       "         [31, 13],\n",
       "         [22, 14],\n",
       "         [32,  1],\n",
       "         [10, 29],\n",
       "         [ 9, 21],\n",
       "         [17,  1],\n",
       "         [32,  1],\n",
       "         [19, 25]]),\n",
       " tensor([[ 6, 12],\n",
       "         [27, 29],\n",
       "         [32, 20],\n",
       "         [ 1,  8],\n",
       "         [22,  5],\n",
       "         [ 9, 23],\n",
       "         [10, 16],\n",
       "         [14, 17],\n",
       "         [24,  7],\n",
       "         [23, 26],\n",
       "         [ 5, 32],\n",
       "         [31, 18],\n",
       "         [24,  0],\n",
       "         [ 8, 11],\n",
       "         [ 3, 32],\n",
       "         [16, 11]]),\n",
       " tensor([[17,  3],\n",
       "         [20, 13],\n",
       "         [31, 16],\n",
       "         [31,  5],\n",
       "         [17, 12],\n",
       "         [29, 29],\n",
       "         [16, 10],\n",
       "         [25,  7],\n",
       "         [14, 22],\n",
       "         [11,  0],\n",
       "         [ 3, 25],\n",
       "         [31, 15],\n",
       "         [16,  3],\n",
       "         [15,  6],\n",
       "         [18,  1],\n",
       "         [15, 32]]))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = torch.randint(0,33,(16,8))\n",
    "i.split(8//4,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "109175a4-b9bd-48ce-a964-ec39bbc09f92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 25])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 8]), torch.Size([16, 25]))"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = torch.randint(0, 33, (16, 8))\n",
    "\n",
    "i=torch.arange(33)[None,:].repeat_interleave(16,0).float()\n",
    "i=torch.multinomial(i, num_samples=8, replacement=False)\n",
    "\n",
    "# Generate tensor containing all numbers from 0 to 33\n",
    "full_range = torch.arange(33).unsqueeze(0).repeat_interleave(16,0)  # Add a dimension for broadcasting\n",
    "\n",
    "# Create a mask of shape (16, 34) where 1 indicates presence of i\n",
    "mask = torch.zeros_like(full_range, dtype=torch.bool)\n",
    "mask.scatter_(1, i, 1)\n",
    "\n",
    "# Compute complement using set difference\n",
    "complement = full_range[~mask].view(i.shape[0], -1)\n",
    "\n",
    "i.shape, complement.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c07c00cf-bce7-4459-ba75-1d67c48e2985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IWM Parameters: 7.27M\n",
      "torch.Size([128, 3, 192]) torch.Size([128, 3, 192])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from nosaveddata import *\n",
    "\n",
    "class Transformer_Block_NoLN(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dropout=0.0, bias=False, ffn_mult=4):\n",
    "        super().__init__()\n",
    "        self.attn = Attention(d_model, num_heads, bias, dropout)\n",
    "        self.mlp = FFN(d_model, dropout, bias, ffn_mult)\n",
    "\n",
    "    def forward(self, x, is_causal=True):\n",
    "        #x = renormalize(x)\n",
    "        means, stds = 0, 0\n",
    "        means += x.mean()\n",
    "        stds += x.std()\n",
    "        x = x + self.attn(x, x, x, is_causal=is_causal)\n",
    "        means += x.mean()\n",
    "        stds += x.std()\n",
    "        x = x + self.mlp(x)\n",
    "        means += x.mean()\n",
    "        stds += x.std()\n",
    "\n",
    "        return x, means/3, stds/3\n",
    "\n",
    "class Transformer_NoDATA(nn.Module):\n",
    "    def __init__(self, d_model, num_blks, nhead, seq_len,\n",
    "                 dropout = 0.1, bias=False, report_params_count=True,\n",
    "                 ffn_mult=4, scale_init=1):\n",
    "        super().__init__()\n",
    "        self.num_hiddens = d_model\n",
    "        self.scale_init=scale_init\n",
    "        if scale_init==1:\n",
    "            self.scale_init=num_blks\n",
    "\n",
    "\n",
    "        self.pos_encoding = nn.Linear(seq_len, d_model, bias=False)\n",
    "\n",
    "        self.final_ln = LayerNormNoBias(d_model)\n",
    "        self.start_dropout = nn.Dropout(dropout)\n",
    "        self.seq_len = seq_len\n",
    "        self.num_blks=num_blks\n",
    "\n",
    "        self.blks = nn.Sequential()\n",
    "        for i in range(num_blks):\n",
    "            self.blks.add_module(\"block\"+str(i), Transformer_Block_NoLN(\n",
    "                                d_model, nhead, dropout, bias=False, ffn_mult=ffn_mult))\n",
    "\n",
    "\n",
    "\n",
    "        # https://proceedings.mlr.press/v119/huang20f/huang20f.pdf\n",
    "\n",
    "        self.apply(init_xavier)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('proj.weight') or pn.endswith('W_v.weight') or pn.endswith('fc.weight') or pn.endswith('pos_encoding.weight'):\n",
    "                torch.nn.init.xavier_uniform_(p, gain=(torch.tensor(9*self.scale_init,dtype=torch.float)).pow(-1/4))\n",
    "        #for pn, p in self.named_parameters():\n",
    "        #    if pn.endswith('proj.weight'):\n",
    "        #        torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * num_blks))\n",
    "\n",
    "        if report_params_count:\n",
    "            params_to_count = [p for p in self.parameters() if p.requires_grad]\n",
    "            print(f'GPT Transformer Parameters: {sum(p.numel() for p in params_to_count)/1e6:.2f}M')\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Embedding):\n",
    "            #torch.nn.init.normal_(module.weight, mean=0.0, std=1/math.sqrt(self.num_hiddens))\n",
    "            torch.nn.init.xavier_uniform_(module.weight, gain=(torch.tensor(9*self.scale_init,dtype=torch.float)).pow(-1/4))\n",
    "\n",
    "    def forward(self, X, is_causal=True):\n",
    "\n",
    "        pos = torch.arange(0, self.seq_len, dtype=torch.float32, device='cuda')\n",
    "        pos_emb = self.pos_encoding(pos)\n",
    "        X = self.start_dropout(X+pos_emb)\n",
    "        X = self.final_ln(X)\n",
    "\n",
    "        means, stds = 0, 0\n",
    "        for i, blk in enumerate(self.blks):\n",
    "            X, mean, std = blk(X, is_causal)\n",
    "            means += mean\n",
    "            stds += std\n",
    "\n",
    "        return X, means/self.num_blks, stds/self.num_blks\n",
    "    \n",
    "    def no_pos(self, X, is_causal=True):\n",
    "        X = self.start_dropout(X)\n",
    "        X = self.final_ln(X)\n",
    "\n",
    "        \n",
    "        for i, blk in enumerate(self.blks):\n",
    "            X, _, _ = blk(X, is_causal)\n",
    "\n",
    "        return X\n",
    "    \n",
    "    def masked(self, X, mask, is_causal=True):\n",
    "\n",
    "        pos = torch.arange(0, self.seq_len, dtype=torch.float32, device='cuda')\n",
    "        pos_emb = self.pos_encoding(pos)\n",
    "        X = self.start_dropout(X+pos_emb)\n",
    "        X = X.gather(1, mask)\n",
    "        \n",
    "        X = self.final_ln(X)\n",
    "\n",
    "        \n",
    "        for i, blk in enumerate(self.blks):\n",
    "            X, _, _ = blk(X, is_causal)\n",
    "\n",
    "        return X\n",
    "    \n",
    "\n",
    "class ViT(nn.Module):\n",
    "    def __init__(self, d_model, num_blks, nhead, patches=(16,16), img_size=(96,72), first_channel=3,\n",
    "                 dropout = 0, bias=True, report_params_count=True,\n",
    "                 ffn_mult=4, lr=1e-4):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_hiddens = d_model\n",
    "        self.first_channel=first_channel\n",
    "\n",
    "        self.patches = np.prod(patches)\n",
    "        self.N = int(np.prod(img_size)/self.patches)\n",
    "\n",
    "        self.in_proj = MLP(first_channel*self.patches, out_hiddens=d_model, last_init=init_xavier)\n",
    "\n",
    "        self.cls = nn.Embedding(1,d_model)\n",
    "        self.transformer = Transformer_NoDATA(d_model, num_blks, nhead, seq_len=self.N,\n",
    "                 dropout = dropout, bias=bias, report_params_count=False,\n",
    "                 ffn_mult=ffn_mult)\n",
    "\n",
    "        self.cls.apply(init_emb)\n",
    "\n",
    "\n",
    "    def patchify(self, X):\n",
    "        X = X.view(-1, self.patches*self.first_channel, self.N).transpose(-2,-1)\n",
    "        return X\n",
    "\n",
    "    def proj(self, X):\n",
    "        X = self.patchify(X)\n",
    "        return self.in_proj(X)\n",
    "\n",
    "    def transformers(self, X):\n",
    "        cls = self.cls(torch.zeros(X.shape[0], device='cuda', dtype=torch.long))\n",
    "\n",
    "        X = torch.cat((cls[:,None],X), 1)\n",
    "        X, means, std = self.transformer(X, is_causal=False)\n",
    "\n",
    "        return X, means, std\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.patchify(X)\n",
    "        X = self.in_proj(X)\n",
    "\n",
    "        cls = self.cls(torch.zeros(X.shape[0], device='cuda', dtype=torch.long))\n",
    "\n",
    "        X = torch.cat((cls[:,None],X), 1)\n",
    "        X, means, std = self.transformer(X, is_causal=False)\n",
    "\n",
    "        return X, means, std\n",
    "\n",
    "    def masked(self, X, mask):\n",
    "        \n",
    "        X = self.transformer.masked(X, mask, is_causal=False)\n",
    "\n",
    "        return X\n",
    "\n",
    "\n",
    "class ViT_IWM(nn.Module):\n",
    "    def __init__(self, encoder, d_encoder,\n",
    "                 d_predictor, num_blks_predictor, nhead_predictor,\n",
    "                 stacked_frames=4,\n",
    "                 mask_samples=4,\n",
    "                 masked_tokens=4,\n",
    "                 num_augmentations=3,\n",
    "                 dropout = 0, bias=True, report_params_count=True,\n",
    "                 ffn_mult=4, lr=1e-4):\n",
    "        super().__init__()\n",
    "        self.d_encoder = d_encoder\n",
    "        self.d_predictor = d_predictor\n",
    "        self.mask_samples=mask_samples\n",
    "        self.stacked_frames=stacked_frames\n",
    "        \n",
    "        self.patches = encoder.patches\n",
    "        self.N = encoder.N\n",
    "        self.masked_tokens=self.N//masked_tokens\n",
    "\n",
    "        self.encoder = encoder\n",
    "\n",
    "        self.predictor_proj = MLP(d_encoder, out_hiddens=d_predictor, last_init=init_xavier) \\\n",
    "                              if d_predictor!=d_encoder else nn.Identity()\n",
    "\n",
    "        self.predictor = Transformer_NoDATA(d_predictor, num_blks_predictor, nhead_predictor, seq_len=self.N,\n",
    "                 dropout = dropout, bias=bias, report_params_count=False,\n",
    "                 ffn_mult=ffn_mult, scale_init=num_blks_predictor)\n",
    "\n",
    "        self.mask = MLP(1, out_hiddens=d_encoder, last_init=init_xavier)\n",
    "        self.mask_pos_encoding = nn.Embedding(self.N, d_encoder)\n",
    "        self.mask_mlp = MLP(d_encoder+num_augmentations, d_encoder, d_encoder, layers=4, in_act=nn.ReLU(), out_act=nn.ReLU(),\n",
    "                            init=init_relu, last_init=init_relu)\n",
    "\n",
    "\n",
    "        self.mask_pos_encoding.apply(init_xavier)\n",
    "\n",
    "        self.head = MLP(d_predictor, out_hiddens=10, last_init=init_xavier)\n",
    "\n",
    "        if report_params_count:\n",
    "            params_count(self, 'IWM')\n",
    "\n",
    "    def hard_reset(self, new_network, alpha):\n",
    "        network_ema(self.encoder, new_network.encoder, alpha)\n",
    "\n",
    "        network_ema(self.predictor_proj, new_network.predictor_proj, 0.3)\n",
    "        network_ema(self.predictor, new_network.predictor, 0.3)\n",
    "\n",
    "        network_ema(self.mask, new_network.mask, 0.3)\n",
    "        network_ema(self.mask_pos_encoding, new_network.mask_pos_encoding, 0.3)\n",
    "        network_ema(self.mask_mlp, new_network.mask_mlp, 0.3)\n",
    "\n",
    "    def get_mask(self, X, augmentations):\n",
    "        B, T, D = X.shape\n",
    "        B = B//self.stacked_frames\n",
    "        m_rand = self.mask_samples*random.randint(0,int(self.masked_tokens*2//self.mask_samples)-1)\n",
    "        \n",
    "        \n",
    "        # Get non-overlapping mask\n",
    "        mask_pos = torch.arange(T, device='cuda')[None,:].repeat_interleave(B,0).float()\n",
    "        mask_pos = torch.multinomial(mask_pos, num_samples=self.masked_tokens+m_rand, replacement=False)\n",
    "        \n",
    "        mask_pos_repeat = mask_pos.repeat_interleave(self.stacked_frames,0)\n",
    "\n",
    "        # Get the mask complement\n",
    "        full_range = torch.arange(T,device='cuda')[None,:].repeat_interleave(B,0)\n",
    "\n",
    "        complement = torch.zeros_like(full_range, dtype=torch.bool)\n",
    "        complement.scatter_(1, mask_pos, 1)\n",
    "\n",
    "        complement = full_range[~complement].view(mask_pos.shape[0], -1)\n",
    "        \n",
    "\n",
    "        # Mask mlp for geometric + augmentation informations\n",
    "        mask = self.mask(torch.ones(B*self.stacked_frames,self.masked_tokens+m_rand,1, device='cuda'))\n",
    "\n",
    "        mask = mask + self.mask_pos_encoding(mask_pos_repeat)\n",
    "\n",
    "        augmentations = augmentations.repeat_interleave(self.stacked_frames,0)[:,None].expand(-1,mask.shape[1],-1)\n",
    "\n",
    "        mask = self.mask_mlp(torch.cat((mask,augmentations),-1))\n",
    "\n",
    "        # Expand to allow gather\n",
    "        mask_pos = mask_pos[:,:,None].expand(-1,-1,X.shape[-1])\n",
    "        complement = complement[:,:,None].expand(-1,-1,X.shape[-1])\n",
    "\n",
    "        return X, mask_pos, complement, mask\n",
    "\n",
    "    def encode(self, X):\n",
    "        return self.encoder(X)\n",
    "    def classify(self, X):\n",
    "        return self.head(X[:,0])\n",
    "\n",
    "    def forward(self, X, y, augmentations):\n",
    "        X = self.encoder.proj(X)\n",
    "        X_masked, mask_pos, complement, mask = self.get_mask(X, augmentations)\n",
    "        \n",
    "        X = self.encoder.masked(X, complement)\n",
    "        X = self.predictor_proj(X)\n",
    "        \n",
    "        mask = torch.cat(mask.split(mask.shape[1]//4,1), 0)\n",
    "        mask_pos = torch.cat(mask_pos.split(mask_pos.shape[1]//4,1), 0)\n",
    "        \n",
    "        \n",
    "        X = torch.cat((X.repeat_interleave(4,0),mask),1)\n",
    "        X = self.predictor.no_pos(X)[:,-mask.shape[1]:]\n",
    "        #mask_pos = mask_pos.contiguous().view(X.shape[0], -1, X.shape[-1])\n",
    "\n",
    "        return X, y.repeat_interleave(4,0).gather(1,mask_pos)\n",
    "\n",
    "\n",
    "encoder = ViT(192,8,12, (8,4), (32,32), dropout=0).cuda()\n",
    "model = ViT_IWM(encoder, 192, 192, 8, 12, 1, dropout=0).cuda()\n",
    "y = encoder(torch.randn(32,3,32,32).cuda())[0]\n",
    "augmentations = F.one_hot(torch.randint(0,1, (32,), device='cuda').long(),3).float()\n",
    "\n",
    "\n",
    "x, y = model(torch.randn(32,3,32,32).cuda(), y, augmentations)\n",
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab6be5f8-cb08-4a95-a8d0-959ccbb97435",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2,\n",
       " Parameter containing:\n",
       " tensor([[ 0.2791, -0.3060, -0.1291, -0.1775,  0.0281,  0.0852, -0.1571,  0.1810,\n",
       "          -0.3115, -0.2924],\n",
       "         [ 0.1707,  0.1237, -0.2965,  0.0481, -0.1261, -0.3033, -0.1083, -0.2032,\n",
       "          -0.2058,  0.2471],\n",
       "         [-0.2028, -0.0752, -0.2589, -0.0594, -0.0695,  0.1375, -0.1582, -0.0620,\n",
       "          -0.1634, -0.2093],\n",
       "         [-0.2052, -0.3068, -0.2306, -0.2475, -0.0786, -0.2804, -0.2356,  0.2132,\n",
       "          -0.0460, -0.2365],\n",
       "         [ 0.2985,  0.2796,  0.2979,  0.1542,  0.0803, -0.2214, -0.3028, -0.1565,\n",
       "           0.2806, -0.1342],\n",
       "         [-0.0816,  0.2452, -0.0195,  0.1074,  0.1561,  0.0796, -0.2248, -0.0610,\n",
       "          -0.2810, -0.1126],\n",
       "         [-0.0917,  0.2616, -0.2639,  0.1111, -0.1007, -0.1432, -0.2466, -0.1097,\n",
       "           0.3041,  0.0454],\n",
       "         [-0.1897, -0.3110, -0.2963,  0.1454,  0.3121, -0.1109,  0.0009, -0.0850,\n",
       "           0.2667,  0.1894],\n",
       "         [ 0.2256,  0.1407, -0.0943,  0.0882,  0.0930,  0.1346, -0.2222, -0.2553,\n",
       "          -0.0440, -0.1945],\n",
       "         [-0.0508, -0.1783, -0.2008,  0.2370,  0.2568, -0.0780, -0.1158,  0.3083,\n",
       "          -0.1569, -0.1271],\n",
       "         [-0.3109,  0.0077,  0.3064,  0.0214,  0.1315, -0.2155,  0.0659, -0.1426,\n",
       "           0.1550,  0.2609],\n",
       "         [-0.0157, -0.0590,  0.2592, -0.2801,  0.0824, -0.0103, -0.2292,  0.1984,\n",
       "           0.2357,  0.3075],\n",
       "         [-0.1531,  0.0536, -0.0905,  0.2943, -0.3094,  0.1138, -0.2555,  0.0710,\n",
       "          -0.2288, -0.1526],\n",
       "         [-0.2068, -0.2731, -0.2652,  0.0538,  0.2771, -0.0922, -0.2054,  0.2075,\n",
       "           0.1297,  0.2804],\n",
       "         [-0.0725,  0.0843,  0.0025, -0.0240,  0.1310, -0.1860, -0.0457,  0.0757,\n",
       "           0.1571,  0.1735],\n",
       "         [-0.1107,  0.0212, -0.2328,  0.0533,  0.0053,  0.2108,  0.3124,  0.3113,\n",
       "           0.0315, -0.1412],\n",
       "         [-0.2382, -0.2983, -0.2704,  0.2179, -0.1957,  0.2181,  0.0825, -0.2539,\n",
       "           0.1426, -0.0930],\n",
       "         [ 0.0751,  0.0992,  0.1163,  0.2125, -0.0222, -0.1314,  0.2016, -0.1426,\n",
       "          -0.1184,  0.1181],\n",
       "         [ 0.2390,  0.2405,  0.0640, -0.2925,  0.0591, -0.0705, -0.2239, -0.0793,\n",
       "          -0.2931,  0.1747],\n",
       "         [-0.2726,  0.1085,  0.0335, -0.2107, -0.0996, -0.2052,  0.2331,  0.0027,\n",
       "           0.1776,  0.1996],\n",
       "         [-0.2680, -0.0728, -0.2688,  0.1818, -0.2278,  0.0749,  0.0589, -0.0108,\n",
       "           0.0497,  0.2249],\n",
       "         [ 0.2045, -0.0338,  0.1357,  0.0604, -0.1161, -0.0053, -0.1966,  0.2853,\n",
       "           0.0329,  0.0900],\n",
       "         [ 0.1313, -0.0676, -0.1424,  0.1955, -0.0303, -0.2204,  0.0206, -0.0613,\n",
       "          -0.1565, -0.0938],\n",
       "         [ 0.1609,  0.2753,  0.0389,  0.0871,  0.1176,  0.2360, -0.2808,  0.2991,\n",
       "           0.0030,  0.0186],\n",
       "         [ 0.2411, -0.2117, -0.1160,  0.1668,  0.1428,  0.2950,  0.1332,  0.2129,\n",
       "           0.2442, -0.0107],\n",
       "         [ 0.0837,  0.1899, -0.1647,  0.0332,  0.2208,  0.1474,  0.2172, -0.0494,\n",
       "          -0.2036, -0.2312],\n",
       "         [-0.2525,  0.2282, -0.2122, -0.2430, -0.1317,  0.2493, -0.2528, -0.1773,\n",
       "          -0.2969, -0.1172],\n",
       "         [ 0.0052, -0.2966, -0.0530,  0.1706, -0.1960, -0.0821,  0.0299, -0.2594,\n",
       "           0.2974, -0.3128],\n",
       "         [ 0.1655, -0.1850,  0.2726,  0.2601,  0.2022, -0.2573,  0.0722, -0.0923,\n",
       "          -0.0067,  0.2321],\n",
       "         [ 0.1297,  0.1646, -0.0315, -0.2911, -0.0547, -0.1462,  0.2590, -0.0822,\n",
       "          -0.1006, -0.0450],\n",
       "         [-0.1595,  0.0629,  0.2635, -0.0350, -0.1268,  0.1266, -0.2516, -0.0548,\n",
       "           0.2870,  0.2322],\n",
       "         [-0.1537, -0.1995,  0.0445, -0.1574,  0.2861,  0.0008,  0.1495,  0.0894,\n",
       "          -0.1097,  0.1022]], requires_grad=True))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import inspect\n",
    "from nosaveddata import *\n",
    "\n",
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "\n",
    "class Hypers(object):\n",
    "    def __init__(self, depth=2):\n",
    "        super().__init__()\n",
    "        self.save_hypers(depth)\n",
    "    \n",
    "    def save_hypers(self, depth=1, ignore=[]):\n",
    "      \"\"\"Save function arguments into class attributes.\"\"\"\n",
    "\n",
    "      #f_back: frame caller\n",
    "      #frame: table of local variablies to the frame's function\n",
    "      frame = inspect.currentframe()\n",
    "      for d in range(depth):\n",
    "        frame = frame.f_back\n",
    "      _, _, _, local_vars = inspect.getargvalues(frame)\n",
    "      #takes the arguments of the function which called this save_hypers function\n",
    "      #it can backtrack functions according to the depth argument\n",
    "\n",
    "      self.hparams = {k:v for k, v in local_vars.items()\n",
    "          if k not in set(ignore+['self']) and not k.startswith('_')}\n",
    "      for k, v in self.hparams.items():\n",
    "          setattr(self, k, v)\n",
    "\n",
    "class nsd_Module(Hypers, nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__(depth=3)\n",
    "\n",
    "class A(nsd_Module):\n",
    "    def __init__(self, b=2, encoder=nn.Linear(10,32)):\n",
    "        super().__init__()\n",
    "\n",
    "a=A()\n",
    "\n",
    "a.b, a.encoder.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d87ecce5-58a0-4152-8d53-74712ac0ab72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first\n",
      "third\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.Third at 0x20eb7ce9120>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class First(object):\n",
    "    def __init__(self):\n",
    "        super(First, self).__init__()\n",
    "        print(\"first\")\n",
    "\n",
    "class Second(object):\n",
    "    def __init__(self):\n",
    "        #super(Second, self).__init__()\n",
    "        print(\"second\")\n",
    "\n",
    "class Third(First, Second):\n",
    "    def __init__(self):\n",
    "        super(Third, self).__init__()\n",
    "        print(\"third\")\n",
    "        \n",
    "Third()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "abad25ef-b505-4c56-832d-f1cf2ac16c1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[17, 30, 24],\n",
       "        [25,  9, 24],\n",
       "        [27, 23, 13],\n",
       "        [27, 21, 28],\n",
       "        [ 7, 26, 15],\n",
       "        [21, 30, 22],\n",
       "        [ 4, 30, 25],\n",
       "        [19, 25,  8]])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=torch.arange(32)[None,:].repeat_interleave(8,0).float()\n",
    "\n",
    "torch.multinomial(a, num_samples=3, replacement=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f479ba36-c926-4e17-ae35-5d8f599c7465",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2dc78d7-a5f5-4ad8-a01c-68912ad5aefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil, glob, os\n",
    "\n",
    "for file in glob.glob('a/*'):\n",
    "    shutil.copy(file, f'b/{file.split(os.sep)[-1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ac57f6d-e45c-43c9-a29f-e94d81fc69b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.006194621197431126, 0.015794621197430955)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "p1 = 0.6697\n",
    "p2 = 0.6649\n",
    "n = 10000\n",
    "\n",
    "d=p1-p2\n",
    "\n",
    "std = 1.65 * math.sqrt((p1*(1-p1) + p2*(1-p2))/n)\n",
    "d-std, d+std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "91924ae8-3392-4ae2-b446-69d92c62c07c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT Transformer Parameters: 1.59M\n",
      "attn tensor(0.0001, device='cuda:0', grad_fn=<MeanBackward0>) tensor(0.9988, device='cuda:0', grad_fn=<StdBackward0>)\n",
      "ffn tensor(-0.0026, device='cuda:0', grad_fn=<MeanBackward0>) tensor(0.9991, device='cuda:0', grad_fn=<StdBackward0>)\n",
      "attn tensor(0.0010, device='cuda:0', grad_fn=<MeanBackward0>) tensor(0.9987, device='cuda:0', grad_fn=<StdBackward0>)\n",
      "ffn tensor(0.0007, device='cuda:0', grad_fn=<MeanBackward0>) tensor(1.0002, device='cuda:0', grad_fn=<StdBackward0>)\n",
      "attn tensor(0.0018, device='cuda:0', grad_fn=<MeanBackward0>) tensor(1.0008, device='cuda:0', grad_fn=<StdBackward0>)\n",
      "ffn tensor(0.0020, device='cuda:0', grad_fn=<MeanBackward0>) tensor(0.9993, device='cuda:0', grad_fn=<StdBackward0>)\n",
      "attn tensor(-0.0025, device='cuda:0', grad_fn=<MeanBackward0>) tensor(1.0002, device='cuda:0', grad_fn=<StdBackward0>)\n",
      "ffn tensor(-0.0014, device='cuda:0', grad_fn=<MeanBackward0>) tensor(0.9998, device='cuda:0', grad_fn=<StdBackward0>)\n",
      "attn tensor(0.0009, device='cuda:0', grad_fn=<MeanBackward0>) tensor(1.0008, device='cuda:0', grad_fn=<StdBackward0>)\n",
      "ffn tensor(-0.0001, device='cuda:0', grad_fn=<MeanBackward0>) tensor(1.0008, device='cuda:0', grad_fn=<StdBackward0>)\n",
      "attn tensor(-0.0009, device='cuda:0', grad_fn=<MeanBackward0>) tensor(1.0020, device='cuda:0', grad_fn=<StdBackward0>)\n",
      "ffn tensor(0.0002, device='cuda:0', grad_fn=<MeanBackward0>) tensor(1.0026, device='cuda:0', grad_fn=<StdBackward0>)\n",
      "attn tensor(0.0002, device='cuda:0', grad_fn=<MeanBackward0>) tensor(0.9999, device='cuda:0', grad_fn=<StdBackward0>)\n",
      "ffn tensor(0.0006, device='cuda:0', grad_fn=<MeanBackward0>) tensor(1.0001, device='cuda:0', grad_fn=<StdBackward0>)\n",
      "attn tensor(0.0002, device='cuda:0', grad_fn=<MeanBackward0>) tensor(0.9996, device='cuda:0', grad_fn=<StdBackward0>)\n",
      "ffn tensor(0.0005, device='cuda:0', grad_fn=<MeanBackward0>) tensor(1.0013, device='cuda:0', grad_fn=<StdBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 128, 128])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GPT_Transformer(128, 8, 4, seq_len=128).cuda()\n",
    "model.eval()\n",
    "model(torch.randn(32,128,128).cuda()).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97f86914-537d-429b-adb4-49259296d173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT Transformer Parameters: 1.58M\n",
      "GPT Transformer Parameters: 0.21M\n",
      "ViT Temporal Parameters: 1.89M\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 27, 128])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model = ViT_Temporal(128, 8, temporal_aggr_num_blks=1, nhead=4, first_channel=3).cuda()\n",
    "\n",
    "model(torch.randn(16,12,96,72).cuda()).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4dcd9b8d-63eb-41c9-bc68-21d104f36874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT Transformer Parameters: 1.59M\n",
      "GPT Transformer Parameters: 0.25M\n",
      "ViT Temporal Parameters: 1.87M\n",
      "GPT Transformer Parameters: 1.59M\n",
      "IWM Parameters: 3.54M\n",
      "post temporal torch.Size([16, 108, 128])\n",
      "torch.Size([16, 108, 128])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 27, 128]),\n",
       " torch.Size([16, 27, 128]),\n",
       " tensor(6.9166, device='cuda:0', grad_fn=<MeanBackward0>))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "class ViT_IWM(nn.Module):\n",
    "    def __init__(self, encoder, d_encoder,\n",
    "                 d_predictor, num_blks_predictor, nhead_predictor,\n",
    "                 out_dim=2048,\n",
    "                 stacked_frames=4,\n",
    "                 masked_tokens=4,\n",
    "                 num_augmentations=3,\n",
    "                 dropout = 0.1, bias=False, report_params_count=True,\n",
    "                 ffn_mult=4):\n",
    "        super().__init__()\n",
    "        self.d_encoder = d_encoder\n",
    "        self.stacked_frames=stacked_frames\n",
    "        \n",
    "        self.patches = encoder.patches\n",
    "        self.N = encoder.N\n",
    "        self.masked_tokens=self.N//masked_tokens\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        \n",
    "        self.predictor_proj = MLP(d_encoder, out_hiddens=d_predictor, last_init=init_xavier) \\\n",
    "                              if d_predictor!=d_encoder else nn.Identity()\n",
    "        \n",
    "        self.predictor = GPT_Transformer(d_predictor, num_blks_predictor, nhead_predictor, seq_len=self.N,\n",
    "                 dropout = dropout, bias=bias, report_params_count=report_params_count,\n",
    "                 ffn_mult=ffn_mult)\n",
    "        \n",
    "        self.mask = MLP(1, out_hiddens=d_encoder, last_init=init_xavier)\n",
    "        self.mask_pos_encoding = nn.Embedding(self.N, d_encoder)\n",
    "        self.mask_mlp = MLP(d_encoder+num_augmentations, d_encoder, d_encoder, layers=4, in_act=nn.ReLU(), out_act=nn.ReLU(),\n",
    "                            init=init_relu, last_init=init_relu)\n",
    "        \n",
    "        \n",
    "        params_count(self, 'IWM')\n",
    "\n",
    "    def hard_reset(self, new_network, alpha):\n",
    "        network_ema(self.encoder, new_network.encoder, alpha)\n",
    "        \n",
    "        network_ema(self.predictor_proj, new_network.predictor_proj, 0)\n",
    "        network_ema(self.predictor, new_network.predictor, 0)\n",
    "\n",
    "        network_ema(self.mask, new_network.mask, 0)\n",
    "        network_ema(self.mask_pos_encoding, new_network.mask_pos_encoding, 0)\n",
    "        network_ema(self.mask_mlp, new_network.mask_mlp, 0)\n",
    "    \n",
    "    def get_mask(self, X, augmentations):\n",
    "        B, T, D = X.shape\n",
    "        B = B//self.stacked_frames\n",
    "        m_rand = random.randint(0,self.masked_tokens*2)\n",
    "        \n",
    "        mask_pos = torch.randint(0, T, (B,self.masked_tokens+m_rand), device='cuda')\n",
    "        mask_pos_repeat = mask_pos.repeat_interleave(self.stacked_frames,0)\n",
    "        \n",
    "        X_mask_pos = (mask_pos_repeat + torch.arange(B, device='cuda').repeat_interleave(self.stacked_frames,0)[:,None]*B).view(-1)\n",
    "        \n",
    "        \n",
    "        mask = self.mask(torch.ones(B*self.stacked_frames,self.masked_tokens+m_rand,1, device='cuda'))\n",
    "        \n",
    "        mask = mask + self.mask_pos_encoding(mask_pos_repeat)\n",
    "        augmentations = augmentations.repeat_interleave(self.stacked_frames,0)[:,None].expand(-1,mask.shape[1],-1)\n",
    "        \n",
    "        mask = self.mask_mlp(torch.cat((mask,augmentations),-1))\n",
    "        \n",
    "        \n",
    "        X.view(-1,D)[X_mask_pos]=X.view(-1,D)[X_mask_pos]*0+mask.view(-1,D)\n",
    "        \n",
    "        mask_pos = mask_pos[:,:self.masked_tokens,None].expand(-1,-1,X.shape[-1])\n",
    "        \n",
    "        \n",
    "        return X, mask_pos\n",
    "    \n",
    "    def encode(self, X):\n",
    "        return self.encoder(X)\n",
    "\n",
    "    \n",
    "    def forward(self, X, y, augmentations):\n",
    "        X = self.encoder.proj(X)\n",
    "        X_masked, mask_pos = self.get_mask(X, augmentations)\n",
    "        X = self.encoder.transformers(X_masked)\n",
    "        \n",
    "        X = self.predictor_proj(X)\n",
    "        \n",
    "        X = self.predictor(X)\n",
    "        mask_pos = mask_pos.contiguous().view(X.shape[0], -1, X.shape[-1])\n",
    "        \n",
    "        return X.gather(1,mask_pos), y.gather(1,mask_pos)\n",
    "\n",
    "encoder = ViT_Temporal(128, 8, patches=(8,8), temporal_aggr_num_blks=1, nhead=4, first_channel=3).cuda()\n",
    "\n",
    "model = ViT_IWM(encoder, 128,\n",
    "                128, 8, 4).cuda()\n",
    "\n",
    "x = torch.randn(16,12,96,72).cuda()\n",
    "\n",
    "with torch.no_grad():\n",
    "    y = model.encode(torch.randn(16,12,96,72).cuda())\n",
    "\n",
    "print(f\"\\npost temporal {y.shape}\\n\")\n",
    "augmentations = torch.bernoulli(torch.ones(x.shape[0], 3)*0.2).cuda()\n",
    "\n",
    "x, y_tgt = model(x, model.predictor_proj(y), augmentations)\n",
    "\n",
    "loss = nn.MSELoss(reduction='none')\n",
    "\n",
    "x=F.normalize(x)\n",
    "y_tgt=F.normalize(y_tgt)\n",
    "\n",
    "print(y.shape)\n",
    "\n",
    "\n",
    "x.shape, y_tgt.shape, loss(x,y_tgt).sum(-1).mean()\n",
    "\n",
    "\n",
    "\n",
    "#model.encode(torch.randn(32,12,96,72).cuda()).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "be15b16f-89ed-44ac-946a-1fa22711f76a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08193082362413406\n",
      "0.025160208344459534\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "def init_saving_variance(module, num_blks):\n",
    "    \n",
    "    torch.nn.init.xavier_uniform_(module.weight, gain=torch.tensor(9*num_blks).pow(-1/4))\n",
    "    if hasattr(module, 'bias'):\n",
    "        if module.bias is not None:\n",
    "            torch.nn.init.zeros_(module.bias)\n",
    "            \n",
    "model = nn.Embedding(32,512)\n",
    "print(f\"{model.weight[0,0]}\")\n",
    "init_saving_variance(model, 3)\n",
    "print(f\"{model.weight[0,0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c46fca2-fc56-4e11-87e5-ec9eb25e7446",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3799)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "\n",
    "a=torch.tensor(4*12)\n",
    "\n",
    "a.pow(-1/4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2752ee96-5262-4cdf-b4f2-2877929256b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 16, 8)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2%3, 192//12, 128//16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fab9435f-3676-4b98-8017-c2548314067d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.04419417382415922, tensor(0.3102), tensor(0.5939))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "\n",
    "a=torch.tensor(9*12)\n",
    "b=torch.tensor(0.67*12)\n",
    "\n",
    "\n",
    "math.sqrt(2/(512*2)), (a).pow(-1/4), (b).pow(-1/4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "187ab817-6e48-4151-89d5-222fa0af26a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8.0, 6.0, 6144)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "96/12, 72/12, 48*128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f15df7fa-d5c7-401f-ab76-146331e21c90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29.4, 39.2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.15*196, 0.2*196"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3cd5510-d6f1-41fa-a01a-2a45ce0cbeab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMPALA ResNet Parameters: 1.56M\n"
     ]
    }
   ],
   "source": [
    "model = IMPALA_Resnet(4,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8575bf-d684-49b3-b61c-eb10a53c4436",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6fa2ef3-3baa-47a6-8d2b-23807fe0432b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from nosaveddata import *\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1be3b499-7bc0-4f94-a872-d8843e9cc110",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet_DiT(nn.Module):\n",
    "    def __init__(self, in_channels, d_model, num_blks, nhead, patch=(2,2), img_size=(32,32),\n",
    "                             dropout = 0.1, bias=False, report_params_count=True,\n",
    "                             ffn_mult=4):\n",
    "        super().__init__()\n",
    "        self.first_channel=in_channels\n",
    "        self.patches = np.prod(patch)\n",
    "        self.img_size=img_size\n",
    "        self.N = int(np.prod(img_size)/self.patches)\n",
    "        \n",
    "        self.ts = TimestepEmbedder(d_model)\n",
    "        \n",
    "        self.in_proj = MLP(in_channels*self.patches, out_hiddens=d_model, last_init=init_xavier)\n",
    "        \n",
    "        self.dit =  DiT_Transformer(d_model, num_blks, nhead, self.patches,\n",
    "                             dropout = 0.1, bias=False, report_params_count=True,\n",
    "                             ffn_mult=4)\n",
    "        self.final_layer = DiT_FinalLayer(d_model, patch, in_channels)\n",
    "        \n",
    "        self.init_weights()\n",
    "    \n",
    "    def init_weights(self):\n",
    "        # Zero-out output layers:\n",
    "        self.final_layer.adaLN_modulation[-1].apply(init_zeros)\n",
    "        self.final_layer.linear.apply(init_zeros)\n",
    "    \n",
    "    def patchify(self, X):\n",
    "        X = X.view(-1, self.patches*self.first_channel, self.N).transpose(-2,-1)\n",
    "        return X\n",
    "    def depatchify(self, X):\n",
    "        X = X.transpose(-2,-1).contiguous().view(-1, self.first_channel,*self.img_size)\n",
    "        return X\n",
    "    \n",
    "    def forward(self, x, t):\n",
    "        c = self.ts(t)\n",
    "        \n",
    "        x = self.patchify(x)\n",
    "        x = self.in_proj(x)\n",
    "        \n",
    "        print(x.shape)\n",
    "        x = self.dit(x, c)\n",
    "        \n",
    "        x = self.final_layer(x, c)\n",
    "        x = self.depatchify(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class UNet_DiT_1D(nn.Module):\n",
    "    def __init__(self, in_channels, d_model, num_blks, nhead, seq_len,\n",
    "                             dropout = 0.1, bias=False, report_params_count=True,\n",
    "                             ffn_mult=4):\n",
    "        super().__init__()\n",
    "        self.first_channel=in_channels\n",
    "        \n",
    "        self.ts = TimestepEmbedder(d_model)\n",
    "        \n",
    "        self.in_proj = MLP(in_channels, out_hiddens=d_model, last_init=init_xavier) if in_channels!=d_model else nn.Identity()\n",
    "        \n",
    "        self.dit =  DiT_Transformer(d_model, num_blks, nhead, seq_len,\n",
    "                             dropout = 0.1, bias=False, report_params_count=True,\n",
    "                             ffn_mult=4)\n",
    "        \n",
    "        self.out_proj = MLP(d_model, out_hiddens=in_channels, last_init=init_xavier) if in_channels!=d_model else nn.Identity()\n",
    "        \n",
    "    \n",
    "    def forward(self, x, t):\n",
    "        c = self.ts(t)\n",
    "        x = self.in_proj(x)\n",
    "        x = self.dit(x, c)\n",
    "        x = self.out_proj(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54330876-b2af-4963-9053-6154632486f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DiT Transformer Parameters: 37.80M\n",
      "torch.Size([16, 33, 512])\n"
     ]
    }
   ],
   "source": [
    "#model = UNet_DiT(4, 512, 8, 8, patch=(4,4)).cuda()\n",
    "\n",
    "#x=torch.randn(16,4,32,32).cuda()\n",
    "x=torch.randn(16,33,512).cuda()\n",
    "c=torch.randint(0,1000,(x.shape[0],)).cuda()\n",
    "\n",
    "\n",
    "model = UNet_DiT_1D(512, 512, 8, 8, seq_len=33).cuda()\n",
    "#model = UNet_DiT_XL_2(in_channels=4, img_size=(32,32)).cuda()\n",
    "#model = UNet_DiT_XL_2(in_channels=4, patch=(2,2), img_size=(32,32)).cuda()\n",
    "\n",
    "with torch.no_grad():\n",
    "    print(model(x,c).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3912798-5ac5-43f3-b591-98c21b5f71e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True, True, True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True, True, True, True, True]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = nn.Linear(10,2).cuda()\n",
    "model.apply(init_xavier)\n",
    "model2 = nn.Linear(10,2).cuda()\n",
    "network_ema(model, model2, 0)\n",
    "#model.apply(init_xavier)\n",
    "\n",
    "model.weight.data==model2.weight.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe94662c-07ea-4abb-b390-a28cf08d5a81",
   "metadata": {},
   "source": [
    "<h1>Preprocessing</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b3a89a6-7f64-4075-8138-50b0e8229dd8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:/Users/Augusto/Python/PyTorch/RL/mc_data/4/2023_01_09_14_48_09_100636/7,0,0,0,0,0,0,0,0,0,0,0,0,3,0,.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 21\u001b[0m\n\u001b[0;32m     12\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:/Users/Augusto/Python/PyTorch/RL/mc_data/4/2023_01_09_14_48_09_100636/7,0,0,0,0,0,0,0,0,0,0,0,0,3,0,.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     16\u001b[0m tfms \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[0;32m     17\u001b[0m                            transforms\u001b[38;5;241m.\u001b[39mResize((\u001b[38;5;241m96\u001b[39m, \u001b[38;5;241m72\u001b[39m)),\n\u001b[0;32m     18\u001b[0m                            transforms\u001b[38;5;241m.\u001b[39mToTensor()\n\u001b[0;32m     19\u001b[0m                         ])\n\u001b[1;32m---> 21\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(path)\n\u001b[0;32m     22\u001b[0m imgs\u001b[38;5;241m=\u001b[39m[]\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m paths:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\python_\\Lib\\site-packages\\PIL\\Image.py:3243\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(fp, mode, formats)\u001b[0m\n\u001b[0;32m   3240\u001b[0m     filename \u001b[38;5;241m=\u001b[39m fp\n\u001b[0;32m   3242\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename:\n\u001b[1;32m-> 3243\u001b[0m     fp \u001b[38;5;241m=\u001b[39m builtins\u001b[38;5;241m.\u001b[39mopen(filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   3244\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   3246\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:/Users/Augusto/Python/PyTorch/RL/mc_data/4/2023_01_09_14_48_09_100636/7,0,0,0,0,0,0,0,0,0,0,0,0,3,0,.jpg'"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import os, glob\n",
    "from nosaveddata import *\n",
    "\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "paths = glob.glob('C:/Users/Augusto/Python/PyTorch/RL/mc_data/4/2023_01_09_14_48_09_100636/*.jpg')\n",
    "path = 'C:/Users/Augusto/Python/PyTorch/RL/mc_data/4/2023_01_09_14_48_09_100636/7,0,0,0,0,0,0,0,0,0,0,0,0,3,0,.jpg'\n",
    "\n",
    "\n",
    "\n",
    "tfms = transforms.Compose([\n",
    "                           transforms.Resize((96, 72)),\n",
    "                           transforms.ToTensor()\n",
    "                        ])\n",
    "\n",
    "img = Image.open(path)\n",
    "imgs=[]\n",
    "for p in paths:\n",
    "    imgs.append(tfms(Image.open(p)))\n",
    "imgs=torch.stack(imgs)\n",
    "\n",
    "print(imgs.shape)\n",
    "\n",
    "\n",
    "\n",
    "imgs, augments_applied = preprocess_iwm_no_solarize(imgs)\n",
    "    \n",
    "\n",
    "\n",
    "#plt.imshow(img_tfms)\n",
    "plot_imgs(imgs.permute(0,2,3,1))\n",
    "augments_applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3a6f04e-3fe2-4c94-8984-2e74348e007b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 12, 96, 72])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from nosaveddata import *\n",
    "\n",
    "\n",
    "\n",
    "def gray_scale_stacked(X, p=0.2, stacks=4):\n",
    "    # Input: Tensor T e (B,C,T,D)\n",
    "    \n",
    "    probs = get_img_preprocessing_prob(X.shape[0], p, X.device)\n",
    "    stacked_probs = probs.repeat_interleave(stacks,0)\n",
    "    X = X.view(-1,X.shape[1]//stacks,*X.shape[-2:])\n",
    "    \n",
    "    gray_img = X.mean(1,keepdim=True).expand(-1,3,-1,-1)\n",
    "    \n",
    "    X = (1-stacked_probs)*X + stacked_probs*gray_img\n",
    "    \n",
    "    return X.view(X.shape[0]//stacks, -1, *X.shape[-2:]), probs.squeeze()\n",
    "\n",
    "def gaussian_blur(X, p=0.2, stacks=4, sigma_min=0.1, sigma_max=2):\n",
    "    # Input: Tensor T e (B,C,T,D)\n",
    "    \n",
    "    probs = get_img_preprocessing_prob(X.shape[0], p, X.device)\n",
    "    tfms = transforms.GaussianBlur(3, (sigma_min, sigma_max))\n",
    "    \n",
    "    blurred = tfms(X)\n",
    "    X = (1-probs)*X + probs*blurred\n",
    "    \n",
    "    return X, probs.squeeze()\n",
    "\n",
    "def solarization_stacked(X, p=0.2, stacks=4):\n",
    "    # Input: Tensor T e (B,C,T,D)\n",
    "\n",
    "    probs = get_img_preprocessing_prob(X.shape[0], p, X.device)\n",
    "    stacked_probs = probs.repeat_interleave(stacks,0)\n",
    "    \n",
    "    X = X.view(-1,X.shape[1]//stacks,*X.shape[-2:])\n",
    "    \n",
    "    tfms = transforms.RandomSolarize(0,p=1) # This prob is applied over all the batch or no image at all\n",
    "    \n",
    "    solarized = tfms(X)\n",
    "    X = (1-stacked_probs)*X + stacked_probs*solarized\n",
    "    \n",
    "    return X.view(X.shape[0]//stacks, -1, *X.shape[-2:]), probs.squeeze()\n",
    "\n",
    "\n",
    "def preprocess_iwm_stacked(imgs, p=0.2, stacks=4):\n",
    "    # Applies the same preprocessing for all images in the sequence, but separated by each beach\n",
    "    augments_applied=[]\n",
    "    \n",
    "    imgs, augmented = gray_scale_stacked(imgs, p, stacks)\n",
    "    augments_applied.append(augmented)\n",
    "    \n",
    "    imgs, augmented = gaussian_blur_stacked(imgs, p, stacks)\n",
    "    augments_applied.append(augmented)\n",
    "    \n",
    "    imgs, augmented = solarization_stacked(imgs, p, stacks)\n",
    "    augments_applied.append(augmented)\n",
    "    \n",
    "    augments_applied = torch.stack(augments_applied,1)\n",
    "    return imgs, augments_applied\n",
    "\n",
    "preprocess_iwm_stacked(torch.randn(32,12,96,72, device='cuda'))[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880deeee-5d84-47ef-9775-583dffaba410",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_img(imgs[-1].permute(1,2,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c2cb75-c68e-4775-aa5f-0a5afcec9a9a",
   "metadata": {},
   "source": [
    "<h1>DiT</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aea6bc6f-523d-4dfa-8ae1-8412cd2d0b3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DiT Transformer Parameters: 2.38M\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 108, 128])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from nosaveddata import *\n",
    "\n",
    "\n",
    "model = DiT_Transformer(128, 8, 8, 108).cuda()\n",
    "\n",
    "X = torch.randn(16,108,128).cuda()\n",
    "c = torch.randn(16,128).cuda()\n",
    "\n",
    "model(X,c).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514e243c-2125-4d02-baf1-0f20bcd780a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from nosaveddata import *\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def modulate(x, shift, scale):\n",
    "    return x * (1 + scale[:,None]) + shift[:,None]\n",
    "    \n",
    "class DiT_Block(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dropout=0.0, bias=False, ffn_mult=4):\n",
    "        super().__init__()\n",
    "        self.ln_1 = LayerNormNoBias(d_model, bias=bias)\n",
    "        self.attn = Attention(d_model, num_heads, bias, dropout)\n",
    "        self.ln_2 = LayerNormNoBias(d_model, bias=bias)\n",
    "        self.mlp = FFN(d_model, dropout, bias, ffn_mult)\n",
    "        \n",
    "        self.adaLN_modulation = nn.Sequential(\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(d_model, 6 * d_model, bias=True)\n",
    "        )\n",
    "        self.adaLN_modulation.apply(init_zeros)\n",
    "        \n",
    "    def forward(self, x, c):\n",
    "        shift_msa, scale_msa, gate_msa, shift_mlp, scale_mlp, gate_mlp = self.adaLN_modulation(c).chunk(6, dim=1)\n",
    "        x_ln = modulate(self.ln_1(x), shift_msa, scale_msa)\n",
    "        x = x + gate_msa[:,None] * self.attn(x_ln, x_ln, x_ln, is_causal=False)\n",
    "        x = x + gate_mlp[:,None] * self.mlp(modulate(self.ln_2(x), shift_mlp, scale_mlp))\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class DiT_Transformer(nn.Module):\n",
    "    def __init__(self, d_model, num_blks, nhead, seq_len,\n",
    "                 dropout = 0.1, bias=False, report_params_count=True,\n",
    "                 ffn_mult=4):\n",
    "        super().__init__()\n",
    "        self.num_hiddens = d_model\n",
    "\n",
    "        self.pos_encoding = nn.Sequential(nn.Linear(seq_len, d_model, bias=False),\n",
    "                                          LayerNormNoBias(d_model)) #Stable Embedding Layer\n",
    "        \n",
    "        self.final_ln = LayerNormNoBias(d_model)\n",
    "        self.start_dropout = nn.Dropout(dropout)\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        self.blks = nn.Sequential()\n",
    "        for i in range(num_blks):\n",
    "            self.blks.add_module(\"block\"+str(i), DiT_Block(\n",
    "                                d_model, nhead, dropout, bias=False, ffn_mult=ffn_mult))\n",
    "            \n",
    "        \n",
    "        #nn.init.xavier_uniform_(self.pos_encoding[0].weight)\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "        # apply special scaled init to the residual projections, per GPT-2 paper\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('proj.weight'):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * num_blks))\n",
    "        \n",
    "        if report_params_count:\n",
    "            params_to_count = [p for p in self.parameters() if p.requires_grad]\n",
    "            print(f'DiT Transformer Parameters: {sum(p.numel() for p in params_to_count)/1e6:.2f}M')\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            #torch.nn.init.xavier_normal_(module.weight)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            #torch.nn.init.xavier_normal_(module.weight)\n",
    "\n",
    "        \n",
    "    def forward(self, X, c):\n",
    "        # Input:\n",
    "        # X e (B, T, D)\n",
    "        # c e (B, D)\n",
    "        \n",
    "        pos = torch.arange(0, self.seq_len, dtype=torch.float32, device='cuda')\n",
    "        pos_emb = self.pos_encoding(pos)\n",
    "        X = self.start_dropout(X+pos_emb)\n",
    "\n",
    "        for i, blk in enumerate(self.blks):\n",
    "            X = blk(X, c)\n",
    "            \n",
    "        return self.final_ln(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86dcafb6-0a86-4b23-a858-ffd84d0d0aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DiT_Transformer(512, 8, 8, 128).cuda()\n",
    "\n",
    "X = torch.randn(16,128,512).cuda()\n",
    "c = torch.randn(16,512).cuda()\n",
    "\n",
    "model(X,c).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6a7bc2-2f1d-4bd2-a116-59fe67d4e0f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c34211d-6121-49b9-8e8e-ac989892c917",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
