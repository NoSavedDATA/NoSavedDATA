{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2dc78d7-a5f5-4ad8-a01c-68912ad5aefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil, glob, os\n",
    "\n",
    "for file in glob.glob('a/*'):\n",
    "    shutil.copy(file, f'b/{file.split(os.sep)[-1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "644015d9-2d27-4354-9668-5ebb2dce93e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.0610, 0.0610, 0.0610, 0.0610, 0.0610, 0.0610, 0.0610, 0.4509, 0.0610,\n",
       "         0.0610]),\n",
       " tensor(2.1658))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "support = torch.linspace(-10, 10, 10)\n",
    "\n",
    "q = torch.zeros(10)\n",
    "q[7]=2\n",
    "\n",
    "q = F.softmax(q,-1)\n",
    "\n",
    "q, (q*support).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60080665-e9e7-4b05-8948-27911c47795b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1fee583b950>]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "initial_n = 4\n",
    "final_n = 1\n",
    "schedule_max_step = 10000\n",
    "\n",
    "v=[]\n",
    "for grad_step in range(40000):\n",
    "    n = initial_n * (final_n/initial_n)**(min(torch.tensor(grad_step-10000).clip(0),schedule_max_step) / schedule_max_step)\n",
    "    v.append(n)\n",
    "\n",
    "plt.plot(v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "828e5304-fe03-4de4-bda4-bb803c582ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(torch.Size([32, 1]), tensor([[180],\n",
      "        [237],\n",
      "        [175],\n",
      "        [ 89],\n",
      "        [ 87],\n",
      "        [ 67],\n",
      "        [184],\n",
      "        [180],\n",
      "        [205],\n",
      "        [127],\n",
      "        [126],\n",
      "        [194],\n",
      "        [127],\n",
      "        [156],\n",
      "        [119],\n",
      "        [126],\n",
      "        [127],\n",
      "        [186],\n",
      "        [ 43],\n",
      "        [165],\n",
      "        [126],\n",
      "        [156],\n",
      "        [126],\n",
      "        [  5],\n",
      "        [180],\n",
      "        [105],\n",
      "        [127],\n",
      "        [ 79],\n",
      "        [126],\n",
      "        [173],\n",
      "        [186],\n",
      "        [180]]))\n",
      "torch.Size([32, 1, 512])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([32])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from nosaveddata import *\n",
    "\n",
    "class Quantizer1d(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_embeddings=256,\n",
    "                 dim=512\n",
    "                 ):\n",
    "        super(Quantizer1d, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings, dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        \n",
    "        dist = torch.cdist(x, self.embedding.weight[None, :].repeat((x.size(0), 1, 1)))\n",
    "        min_encoding_indices = torch.argmin(dist, dim=-1)\n",
    "        \n",
    "        quant_out = torch.index_select(self.embedding.weight, 0, min_encoding_indices.view(-1)).view(B,T,C)\n",
    "        print(f\"{min_encoding_indices.shape, min_encoding_indices}\")\n",
    "        \n",
    "        commmitment_loss = ((quant_out.detach() - x) ** 2).mean((1,2))\n",
    "        codebook_loss = ((quant_out - x.detach()) ** 2).mean((1,2))\n",
    "        quantize_losses = {\n",
    "            'codebook_loss' : codebook_loss,\n",
    "            'commitment_loss' : commmitment_loss\n",
    "        }\n",
    "        quant_out = x + (quant_out - x).detach()\n",
    "        min_encoding_indices = min_encoding_indices.contiguous().view((B,-1))\n",
    "        return quant_out, quantize_losses, min_encoding_indices\n",
    "\n",
    "    def forward_idx(self, x, idx):\n",
    "        B, T, C = x.shape\n",
    "        \n",
    "        \n",
    "        quant_out = torch.index_select(self.embedding.weight, 0, idx.view(-1)).view(B,T,C)\n",
    "        \n",
    "        print(f\"{x.shape}\")\n",
    "        commmitment_loss = ((quant_out.detach() - x) ** 2).mean((1,2))\n",
    "        codebook_loss = ((quant_out - x.detach()) ** 2).mean((1,2))\n",
    "        quantize_losses = {\n",
    "            'codebook_loss' : codebook_loss,\n",
    "            'commitment_loss' : commmitment_loss\n",
    "        }\n",
    "        quant_out = x + (quant_out - x).detach()\n",
    "        return quant_out, quantize_losses\n",
    "\n",
    "quant = Quantizer1d(256,512)\n",
    "\n",
    "x = torch.randn(32,1,512)\n",
    "idx = torch.randint(0,256,(32,))[:,None]\n",
    "\n",
    "a, b, c  = quant(x)\n",
    "\n",
    "x, loss = quant.forward_idx(x, idx)\n",
    "\n",
    "loss['codebook_loss'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "720d21a2-abe3-4f2d-a17d-6614bff47a5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.9774,  0.0935, -0.3474,  ...,  0.1858,  0.9352,  0.8609],\n",
       "         [ 0.4313, -0.9067,  0.3586,  ...,  2.5234,  1.3809,  0.4820],\n",
       "         [ 0.9409, -0.5029, -0.8184,  ..., -0.7990, -1.3363,  1.2311],\n",
       "         [-0.7292,  0.3482, -0.3422,  ...,  1.2561,  0.0179, -2.1140]],\n",
       "        grad_fn=<EmbeddingBackward0>),\n",
       " tensor([26,  9, 37, 35]),\n",
       " tensor([-0.9774,  0.0935, -0.3474,  ...,  0.1858,  0.9352,  0.8609]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from nosaveddata import *\n",
    "\n",
    "seed_np_torch(42)\n",
    "\n",
    "a = torch.randn(38,2048)\n",
    "\n",
    "b = nn.Embedding(38,2048)\n",
    "\n",
    "state_dict = {'weight': a}\n",
    "\n",
    "b.load_state_dict(state_dict)\n",
    "\n",
    "c = torch.randint(0,38,(4,))\n",
    "\n",
    "b(c), c, a[26]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a38d8d4b-dcd4-4a47-8cbe-2757621a804e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([[2]]).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "deb33f37-c373-48e0-8871-ebb1cbfb5ac1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1000 in range(999,1001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac55d1b8-7a6f-4d01-a4bb-1a66ee33b4d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([9, 8, 7]),\n",
       "indices=tensor([9, 8, 7]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a=torch.arange(10)\n",
    "\n",
    "a.topk(3,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6767425-f06c-4aed-8ddf-f744b36658fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf9c5bf5-a80a-4229-a7a5-5f5e70b65fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([300, 2048])\n",
      "Sampled Vectors:\n",
      "[213, 246, 91, 93, 184, 45, 292, 186, 278, 59, 252, 23, 221, 291, 260, 35, 58, 68, 137, 289, 140, 256, 238, 3, 222, 147, 6, 277, 198, 247, 71, 161, 203, 166, 261, 30, 178, 164, 78, 272, 61, 263, 40, 233, 155, 157, 189, 173, 50, 67, 248, 114, 160, 77, 170, 103, 94, 54, 108, 64, 132, 20, 72, 121, 175, 187, 255, 250, 268, 285, 237, 195, 191, 273, 37, 139, 225, 287, 264, 142, 204, 144, 159, 180, 192, 10, 99, 47, 21, 36, 118, 275, 25, 290, 258, 92, 126, 0, 8, 197, 7, 83, 113, 65, 200, 235, 253, 165, 27, 141, 162, 85, 215, 131, 12, 176, 148, 4, 120, 152, 138, 111, 110, 299, 60, 212, 104, 135, 211, 52, 97, 49, 182, 267, 41, 122, 13, 177, 95, 101, 55, 119, 38, 146, 199, 63, 283, 90, 105, 196, 154, 116, 172, 69, 81, 48, 117, 136, 53, 188, 218, 32, 2, 239, 226, 15, 207, 150, 279, 259, 245, 46, 57, 1, 14, 216, 130, 297, 145, 265, 17, 167, 51, 230, 257, 56, 181, 79, 234, 242, 98, 231, 87, 266, 127, 124, 249, 11, 153, 34, 171, 210, 29, 84, 294, 66, 194, 293, 143, 9, 133, 205, 244, 31, 169, 156, 262, 39, 80, 128, 209, 16, 5, 228, 163, 89, 298, 254, 271, 201, 227, 43, 115, 28, 22, 282, 243, 288, 134, 75, 179, 269, 151, 223, 240, 62, 73, 217, 44, 224, 168, 286, 86, 74, 236, 106, 109, 102, 232, 193, 18, 33, 296, 229, 82, 270, 185, 174, 202, 149, 280, 125, 123, 251, 19, 276, 112, 220, 241, 274, 190, 42, 214, 96, 206, 281, 295, 158, 107, 70, 219, 76, 129, 183, 88, 284, 100, 24, 208, 26]\n",
      "tensor([[ 1.4795,  0.3091,  0.7813,  ..., -0.2237,  0.0468, -1.0511],\n",
      "        [ 1.0126,  0.2016,  0.9576,  ...,  1.6146, -2.0180,  1.8738],\n",
      "        [ 1.6552, -2.4497, -0.1280,  ...,  0.5571, -0.8003,  1.5156],\n",
      "        ...,\n",
      "        [-0.3948,  1.1954, -0.1422,  ..., -2.1968,  0.0142,  1.1259],\n",
      "        [ 0.6327, -0.1744,  1.2227,  ...,  0.1129,  1.5544, -0.9843],\n",
      "        [-1.2301,  0.7252, -0.1949,  ..., -1.4241,  1.4676,  0.1440]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Assuming you have 5 vectors stored in a tensor called 'vectors'\n",
    "#vectors = torch.randn(5, 10)  # Assuming each vector has size 10\n",
    "\n",
    "\n",
    "# Compute pairwise distances between all vectors\n",
    "\n",
    "def get_highest_l1_vectors(vectors, ammount=300):\n",
    "    distances = torch.cdist(vectors, vectors)\n",
    "    \n",
    "    # Initialize a list to store the sampled indices\n",
    "    sampled_indices = []\n",
    "    \n",
    "    # Start by randomly selecting the first index\n",
    "    sampled_indices.append(torch.randint(0, vectors.size(0), (1,)).item())\n",
    "    \n",
    "    # Repeat until you have sampled 3 vectors\n",
    "    while len(sampled_indices) < ammount:\n",
    "        # Compute the distances from the already sampled vectors to all others\n",
    "        sampled_distances = distances[sampled_indices, :].min(dim=0).values\n",
    "        \n",
    "        # Select the index with the maximum distance as the next sampled index\n",
    "        next_index = sampled_distances.argmax().item()\n",
    "        # Add the index to the list of sampled indices\n",
    "        sampled_indices.append(next_index)\n",
    "    \n",
    "    # Extract the sampled vectors\n",
    "    return vectors[sampled_indices], sampled_indices\n",
    "\n",
    "vectors = torch.randn(300, 2048)\n",
    "print(f\"{vectors.shape}\")\n",
    "\n",
    "sampled_vectors, sampled_indices = get_highest_l1_vectors(vectors)\n",
    "\n",
    "print(\"Sampled Vectors:\")\n",
    "print(f\"{sampled_indices}\")\n",
    "print(sampled_vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ce085b-8569-4b18-9385-e2318888306f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45d80acf-1a3e-4b64-ab3b-549f0147967e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.9950), tensor(0.0705), tensor(0.0996))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from diffusers import DPMSolverMultistepScheduler \n",
    "noise_scheduler = DPMSolverMultistepScheduler(num_train_timesteps=1000, beta_start=0.00085, beta_end=0.012, use_karras_sigmas=False, solver_order=2)\n",
    "\n",
    "idx=10\n",
    "noise_scheduler.alpha_t[idx], (1-noise_scheduler.alpha_t[idx]).sqrt(), (1-noise_scheduler.alphas_cumprod[idx]).sqrt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b056f0f-3a26-4b68-a041-9ef4abef9bf1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7abd5d6d-7413-4d28-a5dd-ca79729fc05e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sneep\\anaconda3\\envs\\python_\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\sneep\\anaconda3\\envs\\python_\\Lib\\site-packages\\diffusers\\utils\\outputs.py:63: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMPALA ResNet Parameters: 1.56M\n",
      "IMPALA ResNet Parameters: 1.59M\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from nosaveddata import *\n",
    "\n",
    "\n",
    "def get_patches(x, patch_shape):\n",
    "    c, (h, w) = x.shape[1], patch_shape\n",
    "    \n",
    "    return x.unfold(2,h,1).unfold(3,w,1).transpose(1,3).reshape(-1,c,h,w).float()\n",
    "\n",
    "def get_whitening_parameters(patches):\n",
    "    n,c,h,w = patches.shape\n",
    "    patches_flat = patches.view(n, -1)\n",
    "    est_patch_covariance = (patches_flat.T @ patches_flat) / n\n",
    "    \n",
    "    eigenvalues, eigenvectors = torch.linalg.eigh(est_patch_covariance, UPLO='U')\n",
    "    \n",
    "    return eigenvalues.flip(0).view(-1, 1, 1, 1), eigenvectors.T.reshape(c*h*w,c,h,w).flip(0)\n",
    "\n",
    "def init_whitening_conv(layer, train_set, eps=5e-4):\n",
    "    patches = get_patches(train_set, patch_shape=layer.weight.data.shape[2:])\n",
    "    \n",
    "    eigenvalues, eigenvectors = get_whitening_parameters(patches)\n",
    "    \n",
    "    eigenvectors_scaled = eigenvectors / torch.sqrt(eigenvalues + eps)\n",
    "    layer.weight.data[:] = torch.cat((eigenvectors_scaled, -eigenvectors_scaled))\n",
    "    layer.weight.requires_grad=False\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class IMPALA_Resnet_Whitened(nsd_Module):\n",
    "    def __init__(self, first_channels=12, scale_width=1, norm=True, init=init_partial_dirac, act=nn.SiLU()):\n",
    "        super().__init__()\n",
    "        # lhs 2 is because we use concatenate positive and negative eigenvectors, 3 is the kernel size\n",
    "        self.whitened_channels = 2 * first_channels * 3**2\n",
    "        \n",
    "        self.cnn = nn.Sequential(self.whitened_block(first_channels, 16*scale_width),\n",
    "                                 self.get_block(16*scale_width, 32*scale_width),\n",
    "                                 self.get_block(32*scale_width, 32*scale_width, last_relu=True))\n",
    "        \n",
    "        self.cnn[0][1].apply(init)\n",
    "        params_count(self, 'IMPALA ResNet')\n",
    "\n",
    "    def whitened_block(self, in_hiddens, out_hiddens, last_relu=False):\n",
    "        \n",
    "        blocks = nn.Sequential(DQN_Conv(in_hiddens, self.whitened_channels, 3, 1, 1, max_pool=True, act=self.act, norm=self.norm, init=self.init),\n",
    "                               nn.Conv2d(self.whitened_channels,out_hiddens, 1, padding=0, stride=1),\n",
    "                               Residual_Block(out_hiddens, out_hiddens, norm=self.norm, act=self.act, init=self.init),\n",
    "                               Residual_Block(out_hiddens, out_hiddens, norm=self.norm, act=self.act, init=self.init, out_act=self.act if last_relu else nn.Identity())\n",
    "                              )\n",
    "        \n",
    "        return blocks\n",
    "    \n",
    "    def get_block(self, in_hiddens, out_hiddens, last_relu=False):\n",
    "        \n",
    "        blocks = nn.Sequential(DQN_Conv(in_hiddens, out_hiddens, 3, 1, 1, max_pool=True, act=self.act, norm=self.norm, init=self.init),\n",
    "                               Residual_Block(out_hiddens, out_hiddens, norm=self.norm, act=self.act, init=self.init),\n",
    "                               Residual_Block(out_hiddens, out_hiddens, norm=self.norm, act=self.act, init=self.init, out_act=self.act if last_relu else nn.Identity())\n",
    "                              )\n",
    "        \n",
    "        return blocks\n",
    "        \n",
    "    def forward(self, X):\n",
    "        return self.cnn(X)\n",
    "\n",
    "\n",
    "x=torch.randn(500,12,96,72)\n",
    "IMPALA_Resnet(12,scale_width=4)\n",
    "network = IMPALA_Resnet_Whitened(12,scale_width=4)\n",
    "\n",
    "init_whitening_conv(network.cnn[0][0].conv[0], x)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "29c01737-7852-40ff-881c-48d83c11a73b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000e+00, 1.5113e-06, 1.2090e-05, 4.0805e-05, 9.6723e-05, 1.8891e-04,\n",
       "        3.2644e-04, 5.1837e-04, 7.7378e-04, 1.1017e-03, 1.5113e-03, 2.0115e-03,\n",
       "        2.6115e-03, 3.3203e-03, 4.1470e-03, 5.1006e-03, 6.1902e-03, 7.4250e-03,\n",
       "        8.8138e-03, 1.0366e-02, 1.2090e-02, 1.3996e-02, 1.6092e-02, 1.8388e-02,\n",
       "        2.0892e-02, 2.3614e-02, 2.6562e-02, 2.9747e-02, 3.3176e-02, 3.6859e-02,\n",
       "        4.0805e-02, 4.5023e-02, 4.9522e-02, 5.4311e-02, 5.9400e-02, 6.4797e-02,\n",
       "        7.0511e-02, 7.6551e-02, 8.2928e-02, 8.9648e-02, 9.6723e-02, 1.0416e-01,\n",
       "        1.1197e-01, 1.2016e-01, 1.2874e-01, 1.3772e-01, 1.4710e-01, 1.5691e-01,\n",
       "        1.6714e-01, 1.7780e-01, 1.8891e-01, 2.0047e-01, 2.1250e-01, 2.2500e-01,\n",
       "        2.3797e-01, 2.5144e-01, 2.6541e-01, 2.7988e-01, 2.9487e-01, 3.1039e-01,\n",
       "        3.2644e-01, 3.4303e-01, 3.6018e-01, 3.7789e-01, 3.9618e-01, 4.1504e-01,\n",
       "        4.3449e-01, 4.5454e-01, 4.7520e-01, 4.9647e-01, 5.1837e-01, 5.4091e-01,\n",
       "        5.6409e-01, 5.8792e-01, 6.1241e-01, 6.3758e-01, 6.6342e-01, 6.8995e-01,\n",
       "        7.1719e-01, 7.4513e-01, 7.7378e-01])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steps=40000\n",
    "k=5\n",
    "sched = 0.95**k * (torch.arange(steps+1) / steps)**3\n",
    "\n",
    "sched[::500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "babf7648-ba1b-4193-9968-bd7227f706c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00909090909090909\n",
      "0.001818181830458343\n",
      "0.00909090880304575\n"
     ]
    }
   ],
   "source": [
    "class LookaheadState:\n",
    "    def __init__(self, net, steps, k=5):\n",
    "        self.k=k\n",
    "        self.net_ema = {k: v.clone() for k, v in net.state_dict().items()}\n",
    "        self.sched = 0.95**k * (torch.arange(steps+1) / steps)**3\n",
    "\n",
    "    def update(self, net, step):\n",
    "        decay = self.sched[step].item()\n",
    "        if step%self.k==0:\n",
    "            for ema_param, net_param in zip(self.net_ema.values(), net.state_dict().values()):\n",
    "                ema_param.lerp_(net_param, 1-decay)\n",
    "                net_param.copy_(ema_param)\n",
    "                \n",
    "    def update_fixed_decay(self, net, decay, step):\n",
    "        if step%self.k==0:\n",
    "            for ema_param, net_param in zip(self.net_ema.values(), net.state_dict().values()):\n",
    "                ema_param.lerp_(net_param, 1-decay)\n",
    "                net_param.copy_(ema_param)\n",
    "\n",
    "lookahead_state = LookaheadState(network, 40000)\n",
    "\n",
    "lookahead_state.update(network, 8)\n",
    "\n",
    "def Triangle_Scheduler(optimizer, steps, start=0.2, end=0.07, peak=0.23):\n",
    "    def triangle(steps, start, end, peak):\n",
    "        xp = torch.tensor([0, int(peak * steps), steps])\n",
    "        fp = torch.tensor([start, 1, end])\n",
    "        x = torch.arange(1+steps)\n",
    "        m = (fp[1:] - fp[:-1]) / (xp[1:] - xp[:-1])\n",
    "        b = fp[:-1] - (m * xp[:-1])\n",
    "        indices = torch.sum(torch.ge(x[:, None], xp[None, :]), 1) - 1\n",
    "        indices = torch.clamp(indices, 0, len(m) - 1)\n",
    "        return m[indices] * x + b[indices]\n",
    "    lr_schedule = triangle(steps, start, end, peak)\n",
    "    return torch.optim.lr_scheduler.LambdaLR(optimizer, lambda i: lr_schedule[i])\n",
    "\n",
    "momentum=0.9\n",
    "lr = 0.1 / (1+1/(1-momentum))\n",
    "print(f\"{lr}\")\n",
    "\n",
    "optim = torch.optim.SGD(network.parameters(), lr=lr, weight_decay=0.1, momentum=momentum, nesterov=True)\n",
    "\n",
    "sched = Triangle_Scheduler(optim, 40000)\n",
    "\n",
    "print(f\"{optim.param_groups[0]['lr']}\")\n",
    "for i in range(int(40000*0.23)):\n",
    "    sched.step()\n",
    "print(f\"{optim.param_groups[0]['lr']}\")\n",
    "\n",
    "total_train_steps=40000\n",
    "\n",
    "\n",
    "lookahead_state.update(network, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a0e6cdee-1248-4eb9-9181-36f58ec87c48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.01123046875, 2.2412109374999998e-05)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = 11.5/1024# * (1 + 1 / (1 - 0.85))\n",
    "\n",
    "lr, 0.0153/(1 + 1 / (1 - 0.85))*lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ed1821-d35e-48f5-b41f-a0d4c6eb7e8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ae9d0b46-19f3-4a8c-a404-bcedaf8e30b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv(nn.Conv2d):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, padding='same', bias=False):\n",
    "        super().__init__(in_channels, out_channels, kernel_size=kernel_size, padding=padding, bias=bias)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        super().reset_parameters()\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.zero_()\n",
    "        w = self.weight.data\n",
    "        torch.nn.init.dirac_(w[:w.size(1)])\n",
    "\n",
    "def init_partial_dirac(module):\n",
    "    if type(module) in (nn.Linear, nn.Conv2d, nn.Conv1d, nn.Conv3d):\n",
    "        w = module.weight.data\n",
    "        \n",
    "        nn.init.dirac_(module.weight[:w.shape[1]])\n",
    "        nn.init.xavier_uniform_(module.weight[w.shape[1]:], gain=1)\n",
    "\n",
    "        if module.bias is not None:\n",
    "            nn.init.zeros_(module.bias)\n",
    "            \n",
    "\n",
    "model = Conv(16,32)\n",
    "model.apply(init_partial_dirac)\n",
    "last_relu=False\n",
    "act=nn.ReLU()\n",
    "\n",
    "in_hiddens=16\n",
    "out_hiddens=32\n",
    "\n",
    "norm = False\n",
    "model = nn.Sequential(DQN_Conv(in_hiddens, out_hiddens, 3, 1, 1, max_pool=True, act=act, norm=norm, init=init_partial_dirac),\n",
    "                               Residual_Block(out_hiddens, out_hiddens, norm=norm, act=act, init=init_partial_dirac),\n",
    "                               Residual_Block(out_hiddens, out_hiddens, norm=norm, act=act, init=init_partial_dirac, out_act=act if last_relu else nn.Identity()),\n",
    "                               MLP(512,512)\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "793d1ae1-ad48-46d0-9240-477a3b3a9634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ola doutor\n",
      "ola doutor\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'backend' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m   phonemes \u001b[38;5;241m=\u001b[39m collapse_whitespace(phonemes)\n\u001b[0;32m     16\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m phonemes\n\u001b[1;32m---> 18\u001b[0m english_cleaners2(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOlá doutor\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[24], line 14\u001b[0m, in \u001b[0;36menglish_cleaners2\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     11\u001b[0m text \u001b[38;5;241m=\u001b[39m text\u001b[38;5;241m.\u001b[39mlower()\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 14\u001b[0m phonemes \u001b[38;5;241m=\u001b[39m backend\u001b[38;5;241m.\u001b[39mphonemize([text], strip\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     15\u001b[0m phonemes \u001b[38;5;241m=\u001b[39m collapse_whitespace(phonemes)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m phonemes\n",
      "\u001b[1;31mNameError\u001b[0m: name 'backend' is not defined"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from unidecode import unidecode\n",
    "from phonemizer import phonemize\n",
    "from phonemizer.backend import EspeakBackend\n",
    "backend = EspeakBackend('en-us', preserve_punctuation=True, with_stress=True)\n",
    "\n",
    "def english_cleaners2(text):\n",
    "  '''Pipeline for English text, including abbreviation expansion. + punctuation + stress'''\n",
    "  text = unidecode(text)\n",
    "  print(f\"{text}\")\n",
    "  text = text.lower()\n",
    "    \n",
    "  print(f\"{text}\")\n",
    "  phonemes = backend.phonemize([text], strip=True)[0]\n",
    "  phonemes = collapse_whitespace(phonemes)\n",
    "  return phonemes\n",
    "\n",
    "english_cleaners2(\"Olá doutor\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aa2a41eb-227b-43f1-a3fe-4938beb7667b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sneep\\anaconda3\\envs\\python_\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Windows not yet supported for torch.compile",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear(x)\n\u001b[0;32m     14\u001b[0m model \u001b[38;5;241m=\u001b[39m Model()\n\u001b[1;32m---> 15\u001b[0m model \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcompile(model)\n\u001b[0;32m     17\u001b[0m x\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m     18\u001b[0m model(x)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\python_\\Lib\\site-packages\\torch\\__init__.py:1820\u001b[0m, in \u001b[0;36mcompile\u001b[1;34m(model, fullgraph, dynamic, backend, mode, options, disable)\u001b[0m\n\u001b[0;32m   1817\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1818\u001b[0m     backend \u001b[38;5;241m=\u001b[39m _TorchCompileWrapper(backend, mode, options, dynamic)\n\u001b[1;32m-> 1820\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39moptimize(backend\u001b[38;5;241m=\u001b[39mbackend, nopython\u001b[38;5;241m=\u001b[39mfullgraph, dynamic\u001b[38;5;241m=\u001b[39mdynamic, disable\u001b[38;5;241m=\u001b[39mdisable)(model)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\python_\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:753\u001b[0m, in \u001b[0;36moptimize\u001b[1;34m(backend, nopython, guard_export_fn, guard_fail_fn, disable, dynamic, save_config)\u001b[0m\n\u001b[0;32m    715\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[0;32m    716\u001b[0m     backend\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minductor\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    717\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    723\u001b[0m     save_config\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    724\u001b[0m ):\n\u001b[0;32m    725\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    726\u001b[0m \u001b[38;5;124;03m    The main entrypoint of TorchDynamo.  Do graph capture and call\u001b[39;00m\n\u001b[0;32m    727\u001b[0m \u001b[38;5;124;03m    backend() to optimize extracted graphs.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    751\u001b[0m \u001b[38;5;124;03m            ...\u001b[39;00m\n\u001b[0;32m    752\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 753\u001b[0m     check_if_dynamo_supported()\n\u001b[0;32m    754\u001b[0m     \u001b[38;5;66;03m# Note: The hooks object could be global instead of passed around, *however* that would make\u001b[39;00m\n\u001b[0;32m    755\u001b[0m     \u001b[38;5;66;03m# for a confusing API usage and plumbing story wherein we nest multiple .optimize calls.\u001b[39;00m\n\u001b[0;32m    756\u001b[0m     \u001b[38;5;66;03m# There is some prior art around this, w/r/t nesting backend calls are enforced to be the same\u001b[39;00m\n\u001b[0;32m    757\u001b[0m     \u001b[38;5;66;03m# compiler, however, this feels onerous for callback and hooks, and it feels better to give our users an\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;66;03m# easier to understand UX at the cost of a little more plumbing on our end.\u001b[39;00m\n\u001b[0;32m    759\u001b[0m     hooks \u001b[38;5;241m=\u001b[39m Hooks(guard_export_fn\u001b[38;5;241m=\u001b[39mguard_export_fn, guard_fail_fn\u001b[38;5;241m=\u001b[39mguard_fail_fn)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\python_\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:702\u001b[0m, in \u001b[0;36mcheck_if_dynamo_supported\u001b[1;34m()\u001b[0m\n\u001b[0;32m    700\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_if_dynamo_supported\u001b[39m():\n\u001b[0;32m    701\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mplatform \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwin32\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 702\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWindows not yet supported for torch.compile\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    703\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mversion_info \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m12\u001b[39m):\n\u001b[0;32m    704\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPython 3.12+ not yet supported for torch.compile\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Windows not yet supported for torch.compile"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(10,30)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "model = Model()\n",
    "model = torch.compile(model)\n",
    "\n",
    "x=torch.randn(1,10)\n",
    "model(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb20ac00-c5b6-4a8c-81a4-7bfaff184ec2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d80d672-4bfe-42fb-88be-5e3fd3c0d36f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1bc37ab2-106c-40fa-92ba-eb7fbc318e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O -> o\n",
      "cachorro -> kɐkoʁo\n",
      "correu -> koʁew\n",
      "pelo -> pɛlo\n",
      "parque -> pɐɾkʷɛ\n",
      ". -> .\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 21\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     18\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m fonemizar_texto(tokens)\n",
      "Cell \u001b[1;32mIn[5], line 18\u001b[0m, in \u001b[0;36mfonemizar_texto\u001b[1;34m(texto)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfonemizar_texto\u001b[39m(texto):\n\u001b[0;32m     17\u001b[0m     ep \u001b[38;5;241m=\u001b[39m epitran\u001b[38;5;241m.\u001b[39mEpitran(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpor-Latn\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ep\u001b[38;5;241m.\u001b[39mtransliterate(texto)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\python_\\Lib\\site-packages\\epitran\\_epitran.py:52\u001b[0m, in \u001b[0;36mEpitran.transliterate\u001b[1;34m(self, word, normpunc, ligatures)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransliterate\u001b[39m(\u001b[38;5;28mself\u001b[39m, word: \u001b[38;5;28mstr\u001b[39m, normpunc: \u001b[38;5;28mbool\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, ligatures: \u001b[38;5;28mbool\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m     44\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Transliterates/transcribes a word into IPA\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \n\u001b[0;32m     46\u001b[0m \u001b[38;5;124;03m    :param word str: word to transcribe\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;124;03m    :rtype: str\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepi\u001b[38;5;241m.\u001b[39mtransliterate(word, normpunc, ligatures)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\python_\\Lib\\site-packages\\epitran\\simple.py:234\u001b[0m, in \u001b[0;36mSimpleEpitran.transliterate\u001b[1;34m(self, text, normpunc, ligatures)\u001b[0m\n\u001b[0;32m    231\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    232\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError during Korean transliteration: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 234\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeneral_trans(text, \u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    235\u001b[0m                           normpunc, ligatures)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\python_\\Lib\\site-packages\\epitran\\simple.py:167\u001b[0m, in \u001b[0;36mSimpleEpitran.general_trans\u001b[1;34m(self, text, filter_func, normpunc, ligatures)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgeneral_trans\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m, filter_func: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCallable[[tuple[str, bool]], bool]\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    156\u001b[0m                   normpunc: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, ligatures: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    157\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Transliaterates a word into IPA, filtering with filter_func\u001b[39;00m\n\u001b[0;32m    158\u001b[0m \n\u001b[0;32m    159\u001b[0m \u001b[38;5;124;03m    :param text str: word to transcribe; unicode string\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    165\u001b[0m \u001b[38;5;124;03m    :rtype: str\u001b[39;00m\n\u001b[0;32m    166\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 167\u001b[0m     text \u001b[38;5;241m=\u001b[39m unicodedata\u001b[38;5;241m.\u001b[39mnormalize(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNFD\u001b[39m\u001b[38;5;124m'\u001b[39m, text\u001b[38;5;241m.\u001b[39mlower())\n\u001b[0;32m    168\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m(after norm) text=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mrepr\u001b[39m(\u001b[38;5;28mlist\u001b[39m(text)))\n\u001b[0;32m    169\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrip_diacritics\u001b[38;5;241m.\u001b[39mprocess(text)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import epitran\n",
    "\n",
    "# Carregar o modelo em português\n",
    "nlp = spacy.load('pt_core_news_sm')\n",
    "\n",
    "# Texto de exemplo\n",
    "texto = \"O cachorro correu pelo parque.\"\n",
    "\n",
    "#texto = \"Exemplo de texto para fonemização.\"\n",
    "\n",
    "doc = nlp(texto)\n",
    "for token in doc:\n",
    "    try:\n",
    "        fonemas = fonemizar_texto(token.text)\n",
    "        print(f'{token.text} -> {fonemas}')\n",
    "    except Exception as e:\n",
    "        pass\n",
    "\n",
    "\n",
    "fonemizar_texto(tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293c8a2d-db7d-48e3-ae9d-19cfc3391aa2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608559a3-677c-431e-8a9a-8fb8461a71a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2825037a-8e22-4865-a828-bd33bd2f6c09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(torch.Size([3, 7, 2]), tensor([1, 1, 0]))\n",
      "tensor([1, 1, 0])\n",
      "(torch.Size([2, 3, 1]), torch.Size([3, 2]))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.1260, 0.1263],\n",
       "          [0.1239, 0.5740],\n",
       "          [0.1255, 0.0062]],\n",
       " \n",
       "         [[0.2315, 0.3767],\n",
       "          [0.0653, 0.3695],\n",
       "          [0.4300, 0.3719]]], grad_fn=<MulBackward0>),\n",
       " tensor([[[ 1.0039,  0.5802],\n",
       "          [ 0.9957,  1.2654],\n",
       "          [ 1.0019, -0.2223]],\n",
       " \n",
       "         [[ 1.4260,  1.0023],\n",
       "          [ 0.7229,  0.9927],\n",
       "          [ 2.2201,  0.9959]]], grad_fn=<SubBackward0>),\n",
       " tensor([[0.8626, 1.2847],\n",
       "         [1.2662, 0.9935],\n",
       "         [0.5365, 1.7547]]),\n",
       " tensor([[-0.1413,  0.2824],\n",
       "         [ 0.2705,  0.0008],\n",
       "         [-0.4653,  0.7588]], grad_fn=<IndexBackward0>))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nosaveddata import *\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self, len_state, num_quant, num_actions):\n",
    "        nn.Module.__init__(self)\n",
    "       \n",
    "        self.num_quant = num_quant\n",
    "        self.num_actions = num_actions\n",
    "       \n",
    "        self.layer1 = nn.Linear(len_state, 256)\n",
    "        self.layer2 = nn.Linear(256, num_actions*num_quant)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = torch.tanh(x)\n",
    "        x = self.layer2(x)\n",
    "        return x.view(-1, self.num_actions, self.num_quant)\n",
    "   \n",
    "    def select_action(self, state, eps):\n",
    "        if not isinstance(state, torch.Tensor):\n",
    "            state = torch.Tensor([state])    \n",
    "        action = torch.randint(0, 2, (1,))\n",
    "        if random.random() > eps:\n",
    "            action = self.forward(state).mean(2).max(1)[1]\n",
    "        return int(action)\n",
    "   \n",
    "\n",
    "eps_start, eps_end, eps_dec = 0.9, 0.1, 500\n",
    "eps = lambda steps: eps_end + (eps_start - eps_end) * np.exp(-1. * steps / eps_dec)\n",
    "\n",
    "Z = Network(len_state=8, num_quant=2, num_actions=7)\n",
    "Ztgt = Network(len_state=8, num_quant=2, num_actions=7)\n",
    "Ztgt.load_state_dict(Z.state_dict())\n",
    "tau = torch.Tensor((2 * np.arange(Z.num_quant) + 1) / (2.0 * Z.num_quant)).view(1, -1)\n",
    "\n",
    "batch_size=3\n",
    "\n",
    "def huber(x, k=1.0):\n",
    "    return torch.where(x.abs() < k, 0.5 * x.pow(2), k * (x.abs() - 0.5 * k))\n",
    "\n",
    "next_states = torch.randn(batch_size,8)\n",
    "states = next_states + torch.randn(batch_size,8)*0.01\n",
    "\n",
    "gamma=0.997\n",
    "rewards=torch.ones(batch_size,1)\n",
    "\n",
    "\n",
    "theta = Z(states)\n",
    "print(f\"{theta.shape, theta.mean(-1).argmax(-1)}\")\n",
    "theta = theta[np.arange(batch_size), theta.mean(2).max(1)[1]]\n",
    "\n",
    "\n",
    "Znext = Ztgt(next_states).detach()\n",
    "Znext_max = Znext[np.arange(batch_size), Znext.mean(2).max(1)[1]]\n",
    "\n",
    "print(f\"{Znext.mean(2).max(1)[1]}\")\n",
    "\n",
    "Ttheta = rewards + gamma  * Znext_max\n",
    "\n",
    "print(f\"{Ttheta.t()[..., None].shape, theta.shape}\")\n",
    "\n",
    "diff = Ttheta.t()[..., None] - theta\n",
    "\n",
    "loss = huber(diff) * (tau - (diff.detach() < 0).float()).abs()\n",
    "\n",
    "\n",
    "loss, diff, Ttheta, theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f11d706-7d17-40e9-80f3-97f58142ea74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf15e4c3-6015-41d0-b314-1dca5148c5e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1]], device='cuda:0'),\n",
       " tensor([[0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0]], device='cuda:0'))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nosaveddata import *\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "a=torch.arange(2,device='cuda').long()[:,None].repeat_interleave(15,0)\n",
    "\n",
    "a,torch.zeros(6,1,device='cuda').long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1ac57f6d-e45c-43c9-a29f-e94d81fc69b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0002, 0.0058])\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "\n",
    "p1 = 0.6697\n",
    "p2 = 0.6649\n",
    "n = 10000\n",
    "\n",
    "def statistical_difference(p1, p2, n):\n",
    "    \n",
    "    d=torch.tensor(p1-p2).abs()\n",
    "\n",
    "    std = 1.65 * math.sqrt((p1*(1-p1) + p2*(1-p2))/n)\n",
    "    \n",
    "    difference = torch.tensor([d-std, d+std])\n",
    "    \n",
    "    return difference.sort()[0]\n",
    "\n",
    "print(statistical_difference(0.834, 0.831, 100000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5a8e20d0-e2d2-4ce7-9c42-12aea4125e65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMPALA ResNet Parameters: 1.56M\n",
      "IMPALA ResNet Parameters: 1.63M\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 128, 12, 9]), torch.Size([32, 128, 12, 9]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nosaveddata import *\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class IMPALA_Resnet(nn.Module):\n",
    "    def __init__(self, first_channels=12, scale_width=1, norm=True, init=init_relu, act=nn.SiLU()):\n",
    "        super().__init__()\n",
    "        self.norm=norm\n",
    "        self.init=init\n",
    "        self.act =act\n",
    "        \n",
    "        self.cnn = nn.Sequential(self.get_block(first_channels, 16*scale_width),\n",
    "                                 self.get_block(16*scale_width, 32*scale_width),\n",
    "                                 self.get_block(32*scale_width, 32*scale_width, last_relu=True))\n",
    "        params_count(self, 'IMPALA ResNet')\n",
    "    def get_block(self, in_hiddens, out_hiddens, last_relu=False):\n",
    "        \n",
    "        blocks = nn.Sequential(DQN_Conv(in_hiddens, out_hiddens, 3, 1, 1, max_pool=True, act=self.act, norm=self.norm, init=self.init),\n",
    "                               Residual_Block(out_hiddens, out_hiddens, norm=self.norm, act=self.act, init=self.init),\n",
    "                               Residual_Block(out_hiddens, out_hiddens, norm=self.norm, act=self.act, init=self.init, out_act=self.act if last_relu else nn.Identity())\n",
    "                              )\n",
    "        \n",
    "        return blocks\n",
    "        \n",
    "    def forward(self, X):\n",
    "        return self.cnn(X)\n",
    "\n",
    "\n",
    "class IMPALA_YY(nn.Module):\n",
    "    def __init__(self, first_channels=12, scale_width=1, norm=True, init=init_relu, act=nn.SiLU()):\n",
    "        super().__init__()\n",
    "        self.norm=norm\n",
    "        self.init=init\n",
    "        self.act =act\n",
    "\n",
    "        self.yin = self.get_yin(first_channels, 16*scale_width, 32*scale_width)\n",
    "        \n",
    "        self.yang = self.get_yang(first_channels, 16*scale_width)\n",
    "                                 \n",
    "        self.head = nn.Sequential(self.get_yang(16*scale_width, 32*scale_width),\n",
    "                                  self.get_yang(32*scale_width, 32*scale_width, last_relu=True))\n",
    "        \n",
    "        params_count(self, 'IMPALA ResNet')\n",
    "\n",
    "    def get_yin(self, in_hiddens, hiddens, out_hiddens):\n",
    "        blocks = nn.Sequential(DQN_Conv(1, hiddens, 3, 1, 1, max_pool=True, act=self.act, norm=self.norm, init=self.init),\n",
    "                               Residual_Block(hiddens, hiddens, norm=self.norm, act=self.act, init=self.init),\n",
    "                               #DQN_Conv(hiddens, out_hiddens, 3, 1, 1, max_pool=True, act=self.act, norm=self.norm, init=self.init),\n",
    "                               #Residual_Block(out_hiddens, out_hiddens, norm=self.norm, act=self.act, init=self.init),\n",
    "                               #Residual_Block(out_hiddens, out_hiddens, norm=self.norm, act=self.act, init=self.init)\n",
    "                              )\n",
    "        return blocks          \n",
    "        \n",
    "    def get_yang(self, in_hiddens, out_hiddens, last_relu=False):\n",
    "        \n",
    "        blocks = nn.Sequential(DQN_Conv(in_hiddens, out_hiddens, 3, 1, 1, max_pool=True, act=self.act, norm=self.norm, init=self.init),\n",
    "                               Residual_Block(out_hiddens, out_hiddens, norm=self.norm, act=self.act, init=self.init),\n",
    "                               Residual_Block(out_hiddens, out_hiddens, norm=self.norm, act=self.act, init=self.init, out_act=self.act if last_relu else nn.Identity())\n",
    "                              )\n",
    "        \n",
    "        return blocks\n",
    "    \n",
    "    def forward(self, X):\n",
    "\n",
    "        y = self.yin(X[:,-3:].mean(-3)[:,None])\n",
    "        x = self.yang(X)\n",
    "        \n",
    "        X = x*(1-y) + x + y\n",
    "        \n",
    "        return self.head(X)\n",
    "\n",
    "model = IMPALA_Resnet(scale_width=4)\n",
    "x=torch.randn(32,12,96,72)\n",
    "model2 = IMPALA_YY(scale_width=4)\n",
    "\n",
    "model(x).shape, model2(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4dcc521d-7bac-44e8-9584-ce0bab652aab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Parameter containing:\n",
       " tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.1788,  0.1215, -0.1596,  0.0556,  0.0823, -0.0129, -0.1189,  0.1854,\n",
       "          -0.0022, -0.1650],\n",
       "         [-0.1079, -0.0155,  0.0935,  0.0209,  0.0326, -0.1374, -0.1405,  0.0014,\n",
       "           0.1486,  0.0473],\n",
       "         [-0.1218, -0.0415, -0.1404, -0.0332, -0.0325,  0.0417,  0.1003, -0.1978,\n",
       "           0.1183, -0.2110],\n",
       "         [ 0.1376,  0.0622,  0.0658,  0.1490, -0.1540, -0.0291,  0.1021,  0.0194,\n",
       "          -0.0155, -0.1166],\n",
       "         [ 0.1413,  0.0467,  0.0852, -0.0416, -0.0986, -0.0094,  0.0798, -0.0597,\n",
       "          -0.0080,  0.0361],\n",
       "         [-0.0403, -0.0299, -0.0763, -0.1011, -0.1358, -0.0595, -0.0660,  0.0495,\n",
       "           0.0058, -0.1400],\n",
       "         [ 0.1676, -0.0036,  0.1435, -0.1102, -0.0544,  0.0415, -0.0507, -0.1388,\n",
       "          -0.0399, -0.1282],\n",
       "         [-0.0417,  0.0976,  0.1985, -0.0430, -0.1056, -0.2105, -0.1068,  0.0640,\n",
       "           0.1031,  0.0113],\n",
       "         [-0.0810,  0.0300, -0.0838,  0.0648, -0.0344, -0.0191, -0.0008, -0.0291,\n",
       "          -0.0730,  0.0970],\n",
       "         [ 0.0181,  0.0973,  0.1791, -0.0737, -0.1388,  0.1066,  0.1464, -0.1301,\n",
       "          -0.0104, -0.2240],\n",
       "         [-0.1654,  0.0334,  0.0374, -0.0277, -0.0449,  0.0844, -0.0483, -0.1407,\n",
       "          -0.0683, -0.0313],\n",
       "         [ 0.1286, -0.1180,  0.0847, -0.1490, -0.1764, -0.1200,  0.1537,  0.0773,\n",
       "           0.1216,  0.0297],\n",
       "         [ 0.1321, -0.0624, -0.0828,  0.0215, -0.1387, -0.1543, -0.0796, -0.0187,\n",
       "           0.1392, -0.1064],\n",
       "         [-0.0327,  0.0235, -0.1350, -0.0757,  0.0594, -0.0579,  0.0890,  0.0967,\n",
       "           0.0695,  0.0077],\n",
       "         [ 0.0782, -0.0673,  0.0460, -0.0537,  0.0489,  0.1480, -0.0087, -0.0147,\n",
       "           0.0296, -0.1583],\n",
       "         [ 0.0593,  0.0761,  0.1255,  0.0146, -0.0622, -0.1359, -0.0037, -0.0036,\n",
       "           0.0385, -0.2118],\n",
       "         [ 0.1015, -0.1332, -0.0893, -0.1773,  0.1496, -0.1724,  0.0969,  0.0242,\n",
       "          -0.0006,  0.0432],\n",
       "         [-0.0394,  0.0802,  0.1564, -0.1618,  0.0214, -0.0073, -0.0271,  0.0378,\n",
       "           0.1012, -0.1675],\n",
       "         [ 0.0091,  0.1582,  0.1253, -0.0336, -0.1311,  0.0233, -0.0910,  0.1242,\n",
       "          -0.1045, -0.0490],\n",
       "         [-0.0296, -0.0042,  0.1874,  0.0485,  0.0873,  0.0968,  0.1189, -0.0648,\n",
       "          -0.0157,  0.0516],\n",
       "         [-0.0732,  0.0376,  0.0085, -0.1065,  0.0566, -0.0154,  0.0351,  0.1108,\n",
       "           0.1028,  0.1250],\n",
       "         [ 0.0430, -0.1425, -0.1306, -0.1279, -0.1485, -0.0603, -0.1484, -0.1523,\n",
       "          -0.0766, -0.2103],\n",
       "         [-0.1119,  0.0814,  0.1195, -0.0686, -0.1048,  0.1092, -0.0436, -0.0615,\n",
       "           0.0696, -0.2240],\n",
       "         [-0.0229,  0.1712,  0.0652,  0.0895, -0.0668,  0.0070,  0.0054, -0.0098,\n",
       "           0.0127,  0.0470],\n",
       "         [-0.0798, -0.0411, -0.0841, -0.1794, -0.0332, -0.1960,  0.0617,  0.0381,\n",
       "          -0.0976,  0.0966],\n",
       "         [-0.1505, -0.1166, -0.0273, -0.1724, -0.1253, -0.0856,  0.1394,  0.1173,\n",
       "           0.1279, -0.1099],\n",
       "         [-0.0391,  0.0680,  0.1183, -0.0349,  0.0495, -0.0640, -0.0647,  0.1476,\n",
       "          -0.0502, -0.0964],\n",
       "         [-0.0948, -0.1527,  0.0353, -0.0719,  0.0248,  0.0450, -0.1974,  0.0572,\n",
       "          -0.0696, -0.1601],\n",
       "         [ 0.0653,  0.0771, -0.0408, -0.1055, -0.1724, -0.1105, -0.2001,  0.0089,\n",
       "           0.0877, -0.1922],\n",
       "         [ 0.1398,  0.0609, -0.2065, -0.1483,  0.0707, -0.0817, -0.0667, -0.0828,\n",
       "           0.1140, -0.1365],\n",
       "         [ 0.1440, -0.0162,  0.0178,  0.0830, -0.0798, -0.0522, -0.0568,  0.1360,\n",
       "          -0.1322,  0.0088],\n",
       "         [-0.0752,  0.1041,  0.0040,  0.0281,  0.1395,  0.0087,  0.1325, -0.0534,\n",
       "          -0.0239,  0.0520]], device='cuda:0', requires_grad=True))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from nosaveddata import *\n",
    "\n",
    "seed_np_torch(42)\n",
    "\n",
    "def network_ema(target_network, new_network, alpha=0.5):\n",
    "    for (param_name, param_target), param_new  in zip(target_network.cuda().named_parameters(), new_network.parameters()):\n",
    "        if 'ln' in param_name: #layer norm\n",
    "            param_target.data = param_new.data.clone()\n",
    "        else:\n",
    "            param_target.data = alpha * param_target.data + (1 - alpha) * param_new.data.clone()\n",
    "\n",
    "\n",
    "class Modeld(nsd_Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear = nn.Linear(10,32)\n",
    "        self.ln = nn.LayerNorm(32)\n",
    "\n",
    "    def forward(self,X):\n",
    "        return self.ln(self.linear(X))\n",
    "\n",
    "m = Modeld().cuda()\n",
    "m_rand= Modeld().cuda()\n",
    "\n",
    "\n",
    "optim=torch.optim.AdamW(m.parameters(), lr=1e-4)\n",
    "\n",
    "for i in range(4000):\n",
    "    x=torch.randn(1,10).cuda()\n",
    "    \n",
    "    loss = m(x).sum()\n",
    "    loss.backward()\n",
    "    \n",
    "    optim.step()\n",
    "    optim.zero_grad()\n",
    "\n",
    "network_ema(m,m_rand)\n",
    "\n",
    "m.ln.weight, m.linear.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3912798-5ac5-43f3-b591-98c21b5f71e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sneep\\anaconda3\\envs\\python_\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\sneep\\anaconda3\\envs\\python_\\Lib\\site-packages\\diffusers\\utils\\outputs.py:63: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nosaveddata import *\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "model = nn.Linear(10,2).cuda()\n",
    "model.apply(init_xavier)\n",
    "model2 = nn.Linear(10,2).cuda()\n",
    "network_ema(model, model2, 0)\n",
    "model.apply(init_xavier)\n",
    "\n",
    "model.weight.data==model2.weight.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe94662c-07ea-4abb-b390-a28cf08d5a81",
   "metadata": {},
   "source": [
    "<h1>Preprocessing</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b3a89a6-7f64-4075-8138-50b0e8229dd8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:/Users/Augusto/Python/PyTorch/RL/mc_data/4/2023_01_09_14_48_09_100636/7,0,0,0,0,0,0,0,0,0,0,0,0,3,0,.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 21\u001b[0m\n\u001b[0;32m     12\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:/Users/Augusto/Python/PyTorch/RL/mc_data/4/2023_01_09_14_48_09_100636/7,0,0,0,0,0,0,0,0,0,0,0,0,3,0,.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     16\u001b[0m tfms \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[0;32m     17\u001b[0m                            transforms\u001b[38;5;241m.\u001b[39mResize((\u001b[38;5;241m96\u001b[39m, \u001b[38;5;241m72\u001b[39m)),\n\u001b[0;32m     18\u001b[0m                            transforms\u001b[38;5;241m.\u001b[39mToTensor()\n\u001b[0;32m     19\u001b[0m                         ])\n\u001b[1;32m---> 21\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(path)\n\u001b[0;32m     22\u001b[0m imgs\u001b[38;5;241m=\u001b[39m[]\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m paths:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\python_\\Lib\\site-packages\\PIL\\Image.py:3243\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(fp, mode, formats)\u001b[0m\n\u001b[0;32m   3240\u001b[0m     filename \u001b[38;5;241m=\u001b[39m fp\n\u001b[0;32m   3242\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename:\n\u001b[1;32m-> 3243\u001b[0m     fp \u001b[38;5;241m=\u001b[39m builtins\u001b[38;5;241m.\u001b[39mopen(filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   3244\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   3246\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:/Users/Augusto/Python/PyTorch/RL/mc_data/4/2023_01_09_14_48_09_100636/7,0,0,0,0,0,0,0,0,0,0,0,0,3,0,.jpg'"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import os, glob\n",
    "from nosaveddata import *\n",
    "\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "paths = glob.glob('C:/Users/Augusto/Python/PyTorch/RL/mc_data/4/2023_01_09_14_48_09_100636/*.jpg')\n",
    "path = 'C:/Users/Augusto/Python/PyTorch/RL/mc_data/4/2023_01_09_14_48_09_100636/7,0,0,0,0,0,0,0,0,0,0,0,0,3,0,.jpg'\n",
    "\n",
    "\n",
    "\n",
    "tfms = transforms.Compose([\n",
    "                           transforms.Resize((96, 72)),\n",
    "                           transforms.ToTensor()\n",
    "                        ])\n",
    "\n",
    "img = Image.open(path)\n",
    "imgs=[]\n",
    "for p in paths:\n",
    "    imgs.append(tfms(Image.open(p)))\n",
    "imgs=torch.stack(imgs)\n",
    "\n",
    "print(imgs.shape)\n",
    "\n",
    "\n",
    "\n",
    "imgs, augments_applied = preprocess_iwm_no_solarize(imgs)\n",
    "    \n",
    "\n",
    "\n",
    "#plt.imshow(img_tfms)\n",
    "plot_imgs(imgs.permute(0,2,3,1))\n",
    "augments_applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3a6f04e-3fe2-4c94-8984-2e74348e007b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 12, 96, 72])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from nosaveddata import *\n",
    "\n",
    "\n",
    "\n",
    "def gray_scale_stacked(X, p=0.2, stacks=4):\n",
    "    # Input: Tensor T e (B,C,T,D)\n",
    "    \n",
    "    probs = get_img_preprocessing_prob(X.shape[0], p, X.device)\n",
    "    stacked_probs = probs.repeat_interleave(stacks,0)\n",
    "    X = X.view(-1,X.shape[1]//stacks,*X.shape[-2:])\n",
    "    \n",
    "    gray_img = X.mean(1,keepdim=True).expand(-1,3,-1,-1)\n",
    "    \n",
    "    X = (1-stacked_probs)*X + stacked_probs*gray_img\n",
    "    \n",
    "    return X.view(X.shape[0]//stacks, -1, *X.shape[-2:]), probs.squeeze()\n",
    "\n",
    "def gaussian_blur(X, p=0.2, stacks=4, sigma_min=0.1, sigma_max=2):\n",
    "    # Input: Tensor T e (B,C,T,D)\n",
    "    \n",
    "    probs = get_img_preprocessing_prob(X.shape[0], p, X.device)\n",
    "    tfms = transforms.GaussianBlur(3, (sigma_min, sigma_max))\n",
    "    \n",
    "    blurred = tfms(X)\n",
    "    X = (1-probs)*X + probs*blurred\n",
    "    \n",
    "    return X, probs.squeeze()\n",
    "\n",
    "def solarization_stacked(X, p=0.2, stacks=4):\n",
    "    # Input: Tensor T e (B,C,T,D)\n",
    "\n",
    "    probs = get_img_preprocessing_prob(X.shape[0], p, X.device)\n",
    "    stacked_probs = probs.repeat_interleave(stacks,0)\n",
    "    \n",
    "    X = X.view(-1,X.shape[1]//stacks,*X.shape[-2:])\n",
    "    \n",
    "    tfms = transforms.RandomSolarize(0,p=1) # This prob is applied over all the batch or no image at all\n",
    "    \n",
    "    solarized = tfms(X)\n",
    "    X = (1-stacked_probs)*X + stacked_probs*solarized\n",
    "    \n",
    "    return X.view(X.shape[0]//stacks, -1, *X.shape[-2:]), probs.squeeze()\n",
    "\n",
    "\n",
    "def preprocess_iwm_stacked(imgs, p=0.2, stacks=4):\n",
    "    # Applies the same preprocessing for all images in the sequence, but separated by each beach\n",
    "    augments_applied=[]\n",
    "    \n",
    "    imgs, augmented = gray_scale_stacked(imgs, p, stacks)\n",
    "    augments_applied.append(augmented)\n",
    "    \n",
    "    imgs, augmented = gaussian_blur_stacked(imgs, p, stacks)\n",
    "    augments_applied.append(augmented)\n",
    "    \n",
    "    imgs, augmented = solarization_stacked(imgs, p, stacks)\n",
    "    augments_applied.append(augmented)\n",
    "    \n",
    "    augments_applied = torch.stack(augments_applied,1)\n",
    "    return imgs, augments_applied\n",
    "\n",
    "preprocess_iwm_stacked(torch.randn(32,12,96,72, device='cuda'))[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880deeee-5d84-47ef-9775-583dffaba410",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_img(imgs[-1].permute(1,2,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c2cb75-c68e-4775-aa5f-0a5afcec9a9a",
   "metadata": {},
   "source": [
    "<h1>DiT</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "892bf794-e949-4766-a710-0b7002597581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT Transformer Parameters: 31.91M\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 4, 32, 32])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from nosaveddata import *\n",
    "\n",
    "unet = UNet_DiT_S_4(in_channels=4).cuda()\n",
    "x=torch.randn(32,4,32,32).cuda()\n",
    "c=torch.randn(32,384).cuda()\n",
    "t=torch.randint(0,1000,(32,)).cuda()\n",
    "unet(x,t).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aea6bc6f-523d-4dfa-8ae1-8412cd2d0b3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DiT Transformer Parameters: 2.38M\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 108, 128])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from nosaveddata import *\n",
    "\n",
    "\n",
    "model = DiT_Transformer(128, 8, 8, 108).cuda()\n",
    "\n",
    "X = torch.randn(16,108,128).cuda()\n",
    "c = torch.randn(16,128).cuda()\n",
    "\n",
    "model(X, c).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36203835-4741-4e59-b1af-fdf05b13c46b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([1]), tensor([2]), tensor([3]), tensor([4])]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "list(torch.tensor([1,2,3,4]).split(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86dcafb6-0a86-4b23-a858-ffd84d0d0aeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sneep\\anaconda3\\envs\\python_\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\sneep\\anaconda3\\envs\\python_\\Lib\\site-packages\\diffusers\\utils\\outputs.py:63: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNet DIT Parameters: 3.61M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sneep\\Python\\nosaveddata\\nosaveddata\\builders\\transformer.py:66: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=is_causal)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 5, 2048])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from nosaveddata import *\n",
    "\n",
    "model = UNet_DiT_1D(2048, 256, 2, 2048//64, seq_len=5).cuda()\n",
    "\n",
    "X = torch.randn(16,5,2048).cuda()\n",
    "t=torch.randint(0,1000,(16,)).cuda()\n",
    "c = torch.randn(16,256).cuda()\n",
    "\n",
    "model(X, t, c).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6a7bc2-2f1d-4bd2-a116-59fe67d4e0f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c34211d-6121-49b9-8e8e-ac989892c917",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
