{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2dc78d7-a5f5-4ad8-a01c-68912ad5aefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil, glob, os\n",
    "\n",
    "for file in glob.glob('a/*'):\n",
    "    shutil.copy(file, f'b/{file.split(os.sep)[-1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "644015d9-2d27-4354-9668-5ebb2dce93e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.0610, 0.0610, 0.0610, 0.0610, 0.0610, 0.0610, 0.0610, 0.4509, 0.0610,\n",
       "         0.0610]),\n",
       " tensor(2.1658))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "support = torch.linspace(-10, 10, 10)\n",
    "\n",
    "q = torch.zeros(10)\n",
    "q[7]=2\n",
    "\n",
    "q = F.softmax(q,-1)\n",
    "\n",
    "q, (q*support).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55178105-cbc5-470d-b3d1-e3e98846ccdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([[0.3443],\n",
       "        [0.3067],\n",
       "        [0.3372],\n",
       "        [0.3206],\n",
       "        [0.3519],\n",
       "        [0.4528],\n",
       "        [0.4387],\n",
       "        [0.3539],\n",
       "        [0.3643],\n",
       "        [0.4521],\n",
       "        [0.3706],\n",
       "        [0.3587],\n",
       "        [0.4153],\n",
       "        [0.4074],\n",
       "        [0.4209],\n",
       "        [0.3815],\n",
       "        [0.3436],\n",
       "        [0.4790],\n",
       "        [0.3205],\n",
       "        [0.3848],\n",
       "        [0.3596],\n",
       "        [0.4136],\n",
       "        [0.3323],\n",
       "        [0.3561],\n",
       "        [0.2996],\n",
       "        [0.3548],\n",
       "        [0.4194],\n",
       "        [0.4063],\n",
       "        [0.4355],\n",
       "        [0.4406],\n",
       "        [0.4381],\n",
       "        [0.4130]], device='cuda:0'),\n",
       "indices=tensor([[0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0]], device='cuda:0'))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=torch.tensor([[0.3443, 0.1736, 0.0841, 0.1023, 0.0877, 0.0850, 0.1230],\n",
    "        [0.3067, 0.1803, 0.0900, 0.1094, 0.0949, 0.0906, 0.1282],\n",
    "        [0.3372, 0.1784, 0.0842, 0.1032, 0.0884, 0.0850, 0.1236],\n",
    "        [0.3206, 0.1784, 0.0879, 0.1051, 0.0930, 0.0898, 0.1252],\n",
    "        [0.3519, 0.1700, 0.0855, 0.1026, 0.0871, 0.0850, 0.1178],\n",
    "        [0.4528, 0.1640, 0.0673, 0.0857, 0.0675, 0.0675, 0.0953],\n",
    "        [0.4387, 0.1677, 0.0694, 0.0879, 0.0699, 0.0695, 0.0970],\n",
    "        [0.3539, 0.1787, 0.0813, 0.1006, 0.0843, 0.0812, 0.1201],\n",
    "        [0.3643, 0.1802, 0.0783, 0.0961, 0.0843, 0.0805, 0.1163],\n",
    "        [0.4521, 0.1688, 0.0651, 0.0847, 0.0668, 0.0666, 0.0958],\n",
    "        [0.3706, 0.1771, 0.0781, 0.0974, 0.0813, 0.0792, 0.1164],\n",
    "        [0.3587, 0.1821, 0.0791, 0.0997, 0.0819, 0.0801, 0.1184],\n",
    "        [0.4153, 0.1632, 0.0756, 0.0933, 0.0753, 0.0748, 0.1024],\n",
    "        [0.4074, 0.1587, 0.0850, 0.0984, 0.0753, 0.0759, 0.0994],\n",
    "        [0.4209, 0.1730, 0.0706, 0.0905, 0.0717, 0.0715, 0.1019],\n",
    "        [0.3815, 0.1694, 0.0786, 0.0970, 0.0813, 0.0794, 0.1128],\n",
    "        [0.3436, 0.1792, 0.0813, 0.1019, 0.0869, 0.0831, 0.1241],\n",
    "        [0.4790, 0.1713, 0.0588, 0.0783, 0.0607, 0.0608, 0.0911],\n",
    "        [0.3205, 0.1799, 0.0866, 0.1059, 0.0920, 0.0876, 0.1276],\n",
    "        [0.3848, 0.1867, 0.0711, 0.0919, 0.0763, 0.0735, 0.1156],\n",
    "        [0.3596, 0.1763, 0.0797, 0.0987, 0.0840, 0.0810, 0.1207],\n",
    "        [0.4136, 0.1614, 0.0767, 0.0933, 0.0768, 0.0757, 0.1024],\n",
    "        [0.3323, 0.1787, 0.0842, 0.1034, 0.0896, 0.0872, 0.1246],\n",
    "        [0.3561, 0.1662, 0.0888, 0.1047, 0.0870, 0.0840, 0.1132],\n",
    "        [0.2996, 0.1749, 0.0951, 0.1126, 0.0962, 0.0928, 0.1288],\n",
    "        [0.3548, 0.1763, 0.0826, 0.1001, 0.0858, 0.0832, 0.1172],\n",
    "        [0.4194, 0.1789, 0.0676, 0.0884, 0.0711, 0.0701, 0.1046],\n",
    "        [0.4063, 0.1623, 0.0812, 0.0954, 0.0769, 0.0759, 0.1019],\n",
    "        [0.4355, 0.1593, 0.0737, 0.0900, 0.0745, 0.0720, 0.0950],\n",
    "        [0.4406, 0.1632, 0.0730, 0.0891, 0.0696, 0.0688, 0.0957],\n",
    "        [0.4381, 0.1655, 0.0710, 0.0889, 0.0696, 0.0696, 0.0974],\n",
    "        [0.4130, 0.1698, 0.0731, 0.0927, 0.0726, 0.0726, 0.1062]],\n",
    "       device='cuda:0')\n",
    "\n",
    "torch.topk(a,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2354df30-f936-4a63-b725-aa57e3199f98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1553, 0.1541, 0.6906])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "F.gumbel_softmax(torch.zeros(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c657cd02-019e-4d8b-b42c-def84bdc28df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "adf4a277-0099-47ec-b8d3-29c8a76e5a4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-51.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value_support=300\n",
    "\n",
    "\n",
    "def value_phi(x):\n",
    "    return _phi(x, -value_support, value_support, 601)\n",
    "\n",
    "def reward_phi(self, x):\n",
    "    return self._phi(x, self.reward_support.min, self.reward_support.max, self.reward_support.size)\n",
    "\n",
    "def _phi(x, min, max, set_size: int):\n",
    "    #delta = self.value_support.delta\n",
    "    delta = 1\n",
    "\n",
    "    x.clamp_(min, max)\n",
    "    x_low = x.floor()\n",
    "    x_high = x.ceil()\n",
    "    p_high = x - x_low\n",
    "    p_low = 1 - p_high\n",
    "\n",
    "    target = torch.zeros(x.shape[0], set_size).to(x.device)\n",
    "    x_high_idx, x_low_idx = x_high - min / delta, x_low - min / delta\n",
    "    target.scatter_(1, x_high_idx.long().unsqueeze(-1), p_high.unsqueeze(-1))\n",
    "    target.scatter_(1, x_low_idx.long().unsqueeze(-1), p_low.unsqueeze(-1))\n",
    "    return target\n",
    "\n",
    "a = torch.ones(1)*-51\n",
    "value_th = value_phi(a)\n",
    "\n",
    "\n",
    "(value_th*torch.linspace(-300,300,601)).sum(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1746a162-f386-4ac3-b2ab-52329f907e22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x26117bf2ed0>]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiMAAAGdCAYAAADAAnMpAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8g+/7EAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA5f0lEQVR4nO3deXhU9d3//9csmUlCkkkgkBASCKuCQMAgGJFaNUpdqHb7cVNvsdxWf1pq1dztXWkVuty3WKvetpVKRa22vS1UW5e6UCkqagmLgSiyy5ZASEKAZLJPMnO+f0wyEEkgA0nOLM/HdZ0rwzmfM/MejpqX57Mci2EYhgAAAExiNbsAAAAQ3QgjAADAVIQRAABgKsIIAAAwFWEEAACYijACAABMRRgBAACmIowAAABT2c0uoDt8Pp/KysqUmJgoi8VidjkAAKAbDMNQbW2tMjIyZLV2ff8jLMJIWVmZsrKyzC4DAACchdLSUmVmZnZ5PCzCSGJioiT/l0lKSjK5GgAA0B1ut1tZWVmB3+NdCYsw0t41k5SURBgBACDMnGmIBQNYAQCAqQgjAADAVIQRAABgKsIIAAAwFWEEAACYijACAABMRRgBAACmIowAAABTEUYAAICpgg4j77//vmbNmqWMjAxZLBa98sorZzznvffe04UXXiin06lRo0bpueeeO4tSAQBAJAo6jNTX1ysnJ0dLlizpVvt9+/bpuuuu0+WXX67i4mLdc889+va3v61//OMfQRcLAAAiT9DPprnmmmt0zTXXdLv90qVLNXz4cD366KOSpLFjx+rDDz/U//7v/2rmzJnBfjwAAIgwvT5mpLCwUPn5+R32zZw5U4WFhV2e09zcLLfb3WHrDX9ad0AFK4p18HhDr7w/AAA4s14PI+Xl5UpLS+uwLy0tTW63W42NjZ2es3jxYrlcrsCWlZXVK7X95aNS/W3zIRWXVvfK+wMAgDMLydk0CxYsUE1NTWArLS3tlc+ZMMQlSdpysKZX3h8AAJxZ0GNGgpWenq6KiooO+yoqKpSUlKS4uLhOz3E6nXI6nb1dmiZmuvR/66VPCCMAAJim1++M5OXlafXq1R32rVq1Snl5eb390Wc0YUiyJOnTQzXy+QxziwEAIEoFHUbq6upUXFys4uJiSf6pu8XFxSopKZHk72KZO3duoP0dd9yhvXv36r/+67+0Y8cO/fa3v9Vf/vIX3XvvvT3zDc7B6LQEOe1W1Ta3av/RerPLAQAgKgUdRj766CNNnjxZkydPliQVFBRo8uTJWrhwoSTp8OHDgWAiScOHD9cbb7yhVatWKScnR48++qiefvrpkJjWG2Oz6oKMJEnSlkN01QAAYAaLYRgh3z/hdrvlcrlUU1OjpKSkHn3vn7y2Vc+t3a9bLx2uB64f16PvDQBANOvu7++QnE3Tl5hRAwCAuaI+jEzM9IeRT8tq5GUQKwAAfS7qw8iIgQmKd9jU4PFqz5E6s8sBACDqRH0YsVktGp/hvzvCeiMAAPS9qA8jkjQhs33cSLW5hQAAEIUIIzoxbuQTpvcCANDnCCOSJmYmS5K2lbnV4vWZWwwAAFGGMCJpWP94Jcba1dzq0+4KBrECANCXCCOSrFbLifVGDlWbWwwAAFGGMNKmfRArM2oAAOhbhJE2E9ue4MszagAA6FuEkTbtM2q2H3arudVrcjUAAEQPwkibzJQ4JcfHqMVraMfhWrPLAQAgahBG2lgslsAU309Y/AwAgD5DGDnJpKxkSdLm0mpT6wAAIJoQRk4yuS2MFBNGAADoM4SRk+S0hZG9R+pV09BibjEAAEQJwshJ+vdzKHtAvCSpmHEjAAD0CcLI5wTGjZQcN7cQAACiBGHkcyYxbgQAgD5FGPmcyUNTJPnDiGEYJlcDAEDkI4x8ztjBSXLYrapuaNH+ow1mlwMAQMQjjHyOw27V+IwkSVJxKeNGAADobYSRTkzK8nfVbC6pNrcQAACiAGGkE5OGJktiECsAAH2BMNKJ9pVYt5W51dTCE3wBAOhNhJFOZKbEKTXBoVafoa1lNWaXAwBARCOMdMJisZy0+Fm1qbUAABDpCCNdOHm9EQAA0HsII13gzggAAH2DMNKFiZkuWSzSoepGVdY2mV0OAAARizDShcTYGI0elCBJKubuCAAAvYYwchoXto0bKeIJvgAA9BrCyGnkDmsLI/sJIwAA9BbCyGm0h5FPDtWouZXFzwAA6A2EkdMYntpPA/o55Gn16dNDLH4GAEBvIIychsVi0YXtXTUH6KoBAKA3EEbOYEpbGPmIcSMAAPQKwsgZTMk+cWfEMAyTqwEAIPIQRs7gggyXHDarjtZ7tP9og9nlAAAQcQgjZxAbY9OETJckxo0AANAbCCPdMCUwiPWYyZUAABB5CCPdkMsgVgAAeg1hpBvaw8juyjpVN3hMrgYAgMhCGOmGAQlODU/tJ0naxHNqAADoUYSRbspl8TMAAHoFYaSbWPwMAIDeQRjppvbFzz4+WK0Wr8/kagAAiByEkW4akZqg5PgYNbX4tLXMbXY5AABEDMJIN1mtFuUO9d8d2biP9UYAAOgphJEgTB3eX5K0njACAECPIYwEoT2MbNx/TD4fD80DAKAnEEaCMH6IS/EOm2oaW7SzotbscgAAiAiEkSDE2KyB9UY20FUDAECPIIwEaVpg3MhRkysBACAyEEaCNHX4AEn+OyOGwbgRAADOFWEkSDlZLjnsVlXVebTnSL3Z5QAAEPbOKowsWbJE2dnZio2N1bRp07Rhw4bTtn/88cd13nnnKS4uTllZWbr33nvV1NR0VgWbzWm3aXJWsiTGjQAA0BOCDiMrVqxQQUGBFi1apE2bNiknJ0czZ85UZWVlp+1feOEF3XfffVq0aJG2b9+uZ555RitWrNCPfvSjcy7eLNNG+LtqGDcCAMC5CzqMPPbYY7rttts0b948jRs3TkuXLlV8fLyeffbZTtuvXbtW06dP1ze/+U1lZ2fr6quv1pw5c854NyWUBQax7mXcCAAA5yqoMOLxeFRUVKT8/PwTb2C1Kj8/X4WFhZ2ec8kll6ioqCgQPvbu3as333xT1157bZef09zcLLfb3WELJRcOTZHdalG5u0mlxxrNLgcAgLAWVBipqqqS1+tVWlpah/1paWkqLy/v9JxvfvOb+tnPfqZLL71UMTExGjlypL74xS+etptm8eLFcrlcgS0rKyuYMntdnMOmiZkuSXTVAABwrnp9Ns17772nBx98UL/97W+1adMm/e1vf9Mbb7yhn//8512es2DBAtXU1AS20tLS3i4zaO1TfHlODQAA58YeTOPU1FTZbDZVVFR02F9RUaH09PROz3nggQd0880369vf/rYkacKECaqvr9ftt9+uH//4x7JaT81DTqdTTqczmNL63LQR/bV0zR5m1AAAcI6CujPicDiUm5ur1atXB/b5fD6tXr1aeXl5nZ7T0NBwSuCw2WySFNaDP6cMS5HVIpUca9DhGsaNAABwtoLupikoKNCyZcv0/PPPa/v27brzzjtVX1+vefPmSZLmzp2rBQsWBNrPmjVLTz75pJYvX659+/Zp1apVeuCBBzRr1qxAKAlHibExuiCjbdzIXu6OAABwtoLqppGk2bNn68iRI1q4cKHKy8s1adIkrVy5MjCotaSkpMOdkPvvv18Wi0X333+/Dh06pIEDB2rWrFn6n//5n577Fia5eER/bTlUo7V7qnTj5CFmlwMAQFiyGGHQV+J2u+VyuVRTU6OkpCSzywl4d2el5v1+ozJT4vThD68wuxwAAEJKd39/82yaczA1u7/sVosOHm9U6bEGs8sBACAsEUbOQT+nXZPanlPzr8+qzC0GAIAwRRg5R5eM9K83snYPi58BAHA2CCPnKG9kqiR/GAmD4TcAAIQcwsg5unBYspx2q6rqmrW7ss7scgAACDuEkXPktNt0Ubb/Kb5rGTcCAEDQCCM94JJRjBsBAOBsEUZ6wCVt40bW7T0qr49xIwAABIMw0gPGZyQp0WmXu6lVW8tqzC4HAICwQhjpAXabVdNG0FUDAMDZIIz0kPb1Rlj8DACA4BBGesj0Uf5xIxv3H5On1WdyNQAAhA/CSA8Zk5ag1ASHmlp82lxy3OxyAAAIG4SRHmKxWAKrsdJVAwBA9xFGetCM0f4wsmY3YQQAgO4ijPSg9jDyycFqVTd4TK4GAIDwQBjpQYNdcRqTliDDkP71GVN8AQDoDsJID5sxeqAk6f1dR0yuBACA8EAY6WFfGOMPIx/sPiLDYGl4AADOhDDSw6Zm95fDblVZTZP2HKkzuxwAAEIeYaSHxTlsmja8vyTp/V3MqgEA4EwII72gfVbN+7sZNwIAwJkQRnpB+7iRdXuPqrnVa3I1AACENsJILzgvLVGDEp1qavHpo/0sDQ8AwOkQRnqBxWI5McWXrhoAAE6LMNJLvjCmbdwIg1gBADgtwkgvuXSUP4xsP+xWZW2TydUAABC6CCO9ZECCU+OHJEmSPuDuCAAAXSKM9KLL2mbVvLuz0uRKAAAIXYSRXnTF+YMk+Z9T0+r1mVwNAAChiTDSiyZlpSglPkbuplYVHWCKLwAAnSGM9CKb1XJSVw1TfAEA6AxhpJdd3tZV8+4Oxo0AANAZwkgvu2zMQFkt0s6KWh2qbjS7HAAAQg5hpJclxzuUOyxFkvQOd0cAADgFYaQP0FUDAEDXCCN9oH2K79o9VWpq4Sm+AACcjDDSB85LS1SGK1ZNLT4V7jlqdjkAAIQUwkgfsFgs+mLb3RHGjQAA0BFhpI9ccV7buJGdlTIMw+RqAAAIHYSRPnLJqAFy2K06eLxRn1XWmV0OAAAhgzDSR+IdduWNGCBJ+ud2umoAAGhHGOlD+ePSJEmrtpWbXAkAAKGDMNKHrhrrDyObS6tVWdtkcjUAAIQGwkgfSnfFKifTJcOQVtNVAwCAJMJIn7v6gnRJ0ttb6aoBAEAijPS5q9rGjfxrz1HVNbeaXA0AAOYjjPSx0YMSlD0gXp5Wn97fdcTscgAAMB1hpI9ZLJZAV82qbRUmVwMAgPkIIyZo76pZvb1CLV6fydUAAGAuwogJLhyaogH9HHI3tWrjvmNmlwMAgKkIIyawWS26cqz/WTVv01UDAIhyhBGTXD3uxBRfHpwHAIhmhBGTXDo6VXExNpXVNGlrmdvscgAAMA1hxCSxMTZ9YUyqJOkfLIAGAIhihBETXTN+sCTpzS2H6aoBAEStswojS5YsUXZ2tmJjYzVt2jRt2LDhtO2rq6s1f/58DR48WE6nU2PGjNGbb755VgVHkivGDpLDZtWeI/XaVVFndjkAAJgi6DCyYsUKFRQUaNGiRdq0aZNycnI0c+ZMVVZ2/uA3j8ejq666Svv379dLL72knTt3atmyZRoyZMg5Fx/ukmJjAl01b245bHI1AACYI+gw8thjj+m2227TvHnzNG7cOC1dulTx8fF69tlnO23/7LPP6tixY3rllVc0ffp0ZWdn67LLLlNOTs45Fx8JTu6qAQAgGgUVRjwej4qKipSfn3/iDaxW5efnq7CwsNNzXnvtNeXl5Wn+/PlKS0vT+PHj9eCDD8rr9Xb5Oc3NzXK73R22SJU/Lk0xNot2V9Zpd0Wt2eUAANDnggojVVVV8nq9SktL67A/LS1N5eWdzwjZu3evXnrpJXm9Xr355pt64IEH9Oijj+q///u/u/ycxYsXy+VyBbasrKxgygwrrrgYXTqqvauGWTUAgOjT67NpfD6fBg0apKeeekq5ubmaPXu2fvzjH2vp0qVdnrNgwQLV1NQEttLS0t4u01TXTvB31bz1KV01AIDoYw+mcWpqqmw2myoqOi5hXlFRofT09E7PGTx4sGJiYmSz2QL7xo4dq/Lycnk8HjkcjlPOcTqdcjqdwZQW1q4alya71aId5bXac6ROIwcmmF0SAAB9Jqg7Iw6HQ7m5uVq9enVgn8/n0+rVq5WXl9fpOdOnT9dnn30mn+/E02l37dqlwYMHdxpEolFyvEPT27pq3mIgKwAgygTdTVNQUKBly5bp+eef1/bt23XnnXeqvr5e8+bNkyTNnTtXCxYsCLS/8847dezYMd19993atWuX3njjDT344IOaP39+z32LCHBdW1fNG4wbAQBEmaC6aSRp9uzZOnLkiBYuXKjy8nJNmjRJK1euDAxqLSkpkdV6IuNkZWXpH//4h+69915NnDhRQ4YM0d13360f/vCHPfctIsBV49Jke9mi7Yfd2ldVr+Gp/cwuCQCAPmExwmAdcrfbLZfLpZqaGiUlJZldTq+5+Zn1+mB3lb5/9Rh994rRZpcDAMA56e7vb55NE0JmTcyQJL32cZnJlQAA0HcIIyFk5vh0OWxW7aqo047yyF3oDQCAkxFGQogrLkaXnz9QkvRqMXdHAADRgTASYr6c43+A4GvFZfL5Qn44DwAA54wwEmKuHDtI/Rw2Hapu1KaS42aXAwBAryOMhJjYGJtmjvevZktXDQAgGhBGQtANk/xdNW9sOawWr+8MrQEACG+EkRA0feQADejn0LF6j/71WZXZ5QAA0KsIIyHIbrPquon+5eFfo6sGABDhCCMh6oZJ/gXQ/rG1XI0er8nVAADQewgjIerCoSnKTIlTvcerf26vMLscAAB6DWEkRFksFn05x3935OXNh0yuBgCA3kMYCWFfy82UJK3ZdUSVtU0mVwMAQO8gjISwkQMTNHlosrw+Q69uZiArACAyEUZC3Ncu9N8deanooAyD5eEBAJGHMBLiZk3MkMNu1c6KWm0t40m+AIDIQxgJca74GF01Lk2S/+4IAACRhjASBr7e1lXz2sdl8rSyPDwAILIQRsLAjNGpGpjo1LF6j97dWWl2OQAA9CjCSBiw26z6ymT/w/P+SlcNACDCEEbCRPusmnd2VOpoXbPJ1QAA0HMII2HivPRETRjiUqvP0Ks8PA8AEEEII2HkG1P8d0dWbCxlzREAQMQgjISRGyYNUWyMf82RzaXVZpcDAECPIIyEEVdcjK6dMFiS9Of1JSZXAwBAzyCMhJlvTh0qSXr9k8NyN7WYXA0AAOeOMBJmcoelaNSgBDW2eBnICgCICISRMGOxWPRvF2VJkpZvoKsGABD+CCNh6GsXZsphs2prmVtbDtaYXQ4AAOeEMBKGUvo59KXx6ZKkF7g7AgAIc4SRMDWnbSDra8WHVN/canI1AACcPcJImLp4RH8NT+2neo9Xr33MQFYAQPgijISpkwey/rHwACuyAgDCFmEkjP1/U7LktFu17bBbm0qOm10OAABnhTASxlL6OfTlnAxJ0vNrD5hcDQAAZ4cwEuZuuSRbkvTWp4dVWdtkbjEAAJwFwkiYGz/EpclDk9XiNbR8Q6nZ5QAAEDTCSAS4JS9bkvR/6w+oxesztxgAAIJEGIkA10xIV2qCQxXuZq3aVmF2OQAABIUwEgGcdpv+7SL/ImjPr91vbjEAAASJMBIhvjltqGxWi9bvO6ad5bVmlwMAQLcRRiJERnKcrh6XJkl6bu0+k6sBAKD7CCMRZN704ZKkv206pKN1zSZXAwBA9xBGIshF2SmamOlSc6tPf1rH03wBAOGBMBJBLBaLbr3Uf3fkj+v2q6nFa3JFAACcGWEkwlw7YbAyXLGqqvPo1eJDZpcDAMAZEUYiTIzNqm9Nz5YkPf3BPp7mCwAIeYSRCPRvU4eqn8Om3ZV1WrPriNnlAABwWoSRCJQUG6PZbYugPfMh03wBAKGNMBKh5k3PltUifbC7SjvK3WaXAwBAlwgjESqrf7yuGT9YkvTUmr0mVwMAQNcIIxHs9i+MkCS9+nGZSo81mFwNAACdI4xEsJysZF06KlVen6FlH3B3BAAQmggjEe47XxwpSVqxsVRHalkiHgAQeggjES5v5ABNykpWc6tPz/6LmTUAgNBDGIlwFoslcHfkT4UHVNPYYnJFAAB0dFZhZMmSJcrOzlZsbKymTZumDRs2dOu85cuXy2Kx6MYbbzybj8VZyh+bpjFpCaptbtWf1h0wuxwAADoIOoysWLFCBQUFWrRokTZt2qScnBzNnDlTlZWVpz1v//79+v73v68ZM2acdbE4O1arRXe23R159sN9avTwAD0AQOgIOow89thjuu222zRv3jyNGzdOS5cuVXx8vJ599tkuz/F6vbrpppv005/+VCNGjDingnF2Zk3MUGZKnI7We7R8Y4nZ5QAAEBBUGPF4PCoqKlJ+fv6JN7BalZ+fr8LCwi7P+9nPfqZBgwbp1ltv7dbnNDc3y+12d9hwbuw2q+64zH93ZOmaPWpq4e4IACA0BBVGqqqq5PV6lZaW1mF/WlqaysvLOz3nww8/1DPPPKNly5Z1+3MWL14sl8sV2LKysoIpE134xpRMZbhiVeFu1gvruTsCAAgNvTqbpra2VjfffLOWLVum1NTUbp+3YMEC1dTUBLbS0tJerDJ6OO02zb9ilCTpSe6OAABChD2YxqmpqbLZbKqoqOiwv6KiQunp6ae037Nnj/bv369Zs2YF9vl8Pv8H2+3auXOnRo4cecp5TqdTTqczmNLQTd/IzdJv392jQ9WN+tO6A/r2DMbwAADMFdSdEYfDodzcXK1evTqwz+fzafXq1crLyzul/fnnn68tW7aouLg4sH35y1/W5ZdfruLiYrpfTOCwW3VX292RpWv2MrMGAGC6oO6MSFJBQYFuueUWTZkyRVOnTtXjjz+u+vp6zZs3T5I0d+5cDRkyRIsXL1ZsbKzGjx/f4fzk5GRJOmU/+s7XcjO15L3PVHrMf3fkti9wdwQAYJ6gw8js2bN15MgRLVy4UOXl5Zo0aZJWrlwZGNRaUlIiq5WFXUNZjM2quy4frf/66ydaumaPbrp4qOIdQf+jAABAj7AYhmGYXcSZuN1uuVwu1dTUKCkpyexyIkKL16crH12jkmMNuu+a8wPTfgEA6Cnd/f3NLYwoFWOz6u4rR0uSnnxvD8+sAQCYhjASxW6cPETnpSWqprFFS9fsMbscAECUIoxEMZvVoh/MPE+S9Pt/7VOFu8nkigAA0YgwEuWuHDtIU4alqKnFp8f/udvscgAAUYgwEuUsFovuu+Z8SdJfPirVniN1JlcEAIg2hBFoSnZ/5Y8dJK/P0CP/2Gl2OQCAKEMYgSTpBzPPl8UivfVpuYpLq80uBwAQRQgjkCSdl56or07OlCQ9+MZ2hcHyMwCACEEYQcB/Xj1GsTFWbdh/TCs/LTe7HABAlCCMICAjOU63f8G/EuuDb21XUwsP0QMA9D7CCDq447IRSktyqvRYo37/r/1mlwMAiAKEEXQQ77Drh1/yT/Vd8u5nOlLbbHJFAIBIRxjBKW6cNEQTM12qa27VY6uY6gsA6F2EEZzCarVo4fXjJEnLN5Zqa1mNyRUBACIZYQSdmpLdX9dPHCzDkH762jam+gIAeg1hBF1acO3YwFTflzcfMrscAECEIoygS0OS43TXFaMlSQ++uV01jS0mVwQAiESEEZzWbTNGaMTAfqqq8+h/V+0yuxwAQAQijOC0HHarfn7DeEnSHwr369NDDGYFAPQswgjOaPqoVF0/cbB8hvTAq5/K52MwKwCg5xBG0C33XzdO/Rw2bS6p1otFpWaXAwCIIIQRdEu6K1b35I+RJD345g5V1jaZXBEAIFIQRtBt86Zn64KMJNU0tuinf99mdjkAgAhBGEG32W1W/eJrE2WzWvTGJ4e1aluF2SUBACIAYQRBGT/EpdtmjJAk3f/KFrmbWHsEAHBuCCMI2j35ozVsQLwq3M16eOUOs8sBAIQ5wgiCFhtj0+KvTpAk/WldiTbsO2ZyRQCAcEYYwVm5ZGSqZk/JkiT910sfq8HTanJFAIBwRRjBWfvRdWM12BWr/Ucb9NBbdNcAAM4OYQRnzRUXo4e/PlGS9IfCA/pwd5XJFQEAwhFhBOdkxuiBuvniYZKkH7z0MbNrAABBI4zgnC249nwNGxCvwzVN+ulrLIYGAAgOYQTnLN5h16PfyJHFIv1100G9vbXc7JIAAGGEMIIeMSW7v27/gn8xtB/+9RNVuHl2DQCgewgj6DEFV43RBRlJOt7QontXFMvrM8wuCQAQBggj6DFOu02/njNZcTE2rd1zVL97f4/ZJQEAwgBhBD1q5MAE/fSGCyRJj729S5tLjptcEQAg1BFG0OO+kZup6ycOVqvP0PeWb1Yt030BAKdBGEGPs1gs+p+vTFBmSpxKjzVqwd+2yDAYPwIA6BxhBL3CFRejX/3bZNmtFr3+yWH9ofCA2SUBAEIUYQS9JndYihZcO1aS9N9vbNMmxo8AADpBGEGv+o/p2bpuwmC1eA3N/79NOlrXbHZJAIAQQxhBr7JYLHroaxM0IrWfDtc06R7WHwEAfA5hBL0uMTZGT/57ruJibPpgd5Ue/+cus0sCAIQQwgj6xHnpiVr81QmSpN+885ne+OSwyRUBAEIFYQR95sbJQ3TrpcMlSd9/8WNtLasxuSIAQCggjKBPLbjmfM0YnarGFq9u/0ORqhjQCgBRjzCCPmW3WfXEnAs1PLWfDlU36jt/2iRPq8/ssgAAJiKMoM+54mO0bG6uEpx2bdh/TAtf/ZQVWgEgihFGYIpRgxL16zmTZLFIyzeW6sk1POEXAKIVYQSmueL8NC26fpwk6eGVO/Xax2UmVwQAMANhBKb61vThJ2bY/OVjbdh3zOSKAAB9jTAC0/3o2rGaeUGaPF6fbvvDR9pzpM7skgAAfYgwAtPZrBY9PnuyJmUlq6axRXOf2aDymiazywIA9BHCCEJCnMOmp2+ZEpjy++/PrNexeo/ZZQEA+gBhBCEjNcGpP946VelJsfqssk7zfr9Bdc2tZpcFAOhlZxVGlixZouzsbMXGxmratGnasGFDl22XLVumGTNmKCUlRSkpKcrPzz9te0S3zJR4/enbU5USH6OPD9bo9j98pKYWr9llAQB6UdBhZMWKFSooKNCiRYu0adMm5eTkaObMmaqsrOy0/Xvvvac5c+bo3XffVWFhobKysnT11Vfr0KFD51w8ItOoQYl6/j+mqp/DprV7jup7f96sVi+rtAJApLIYQS59OW3aNF100UV64oknJEk+n09ZWVm66667dN99953xfK/Xq5SUFD3xxBOaO3dutz7T7XbL5XKppqZGSUlJwZSLMLZ2T5W+9fuN8rT69NXJQ/TLb+TIZrWYXRYAoJu6+/s7qDsjHo9HRUVFys/PP/EGVqvy8/NVWFjYrfdoaGhQS0uL+vfvH8xHIwpdMjJVT8yZLJvVor9tPqQfvPixvD6WjQeASBNUGKmqqpLX61VaWlqH/WlpaSovL+/We/zwhz9URkZGh0Dzec3NzXK73R02RKerL0jXbwgkABDR+nQ2zUMPPaTly5fr5ZdfVmxsbJftFi9eLJfLFdiysrL6sEqEmmsnDCaQAEAECyqMpKamymazqaKiosP+iooKpaenn/bcRx55RA899JDefvttTZw48bRtFyxYoJqamsBWWloaTJmIQNdOGKwn5kyWvS2QfJ9AAgARI6gw4nA4lJubq9WrVwf2+Xw+rV69Wnl5eV2e9/DDD+vnP/+5Vq5cqSlTppzxc5xOp5KSkjpswDUTBuuJb/oDycubD+meFcVqYZYNAIS9oLtpCgoKtGzZMj3//PPavn277rzzTtXX12vevHmSpLlz52rBggWB9r/4xS/0wAMP6Nlnn1V2drbKy8tVXl6uujqeP4LgfWn8YD3xzQtlt1r094/LdNsfPlKjh3VIACCcBR1GZs+erUceeUQLFy7UpEmTVFxcrJUrVwYGtZaUlOjw4cOB9k8++aQ8Ho++/vWva/DgwYHtkUce6blvgajypfHpevqWKYqNseq9nUd08zPrVdPYYnZZAICzFPQ6I2ZgnRF0pujAMc37/Ua5m1p1fnqi/nDrVA1K7HpgNACgb/XKOiNAKMkd1l8r/v88DUx0akd5rb6xtFAlRxvMLgsAECTCCMLa2MFJeumOPA3tH68DRxv01Sf/peLSarPLAgAEgTCCsDdsQD+9dEeexg1OUlWdR7N/V6i3thw+84kAgJBAGEFEGJQUq7/ckacrzh+k5laf7vy/TVq6Zo/CYEgUAEQ9wggiRoLTrqduztW3LsmWJD301g4t+NsW1iIBgBBHGEFEsdus+smXL9CiWeNktUjLN5Zq7jMbdLSu2ezSAABdIIwgIs2bPlxP3TxF8Q6bCvce1Zef+Jc+PVRjdlkAgE4QRhCx8sel6eXvTFf2gHgdqm7U155cq78WHTS7LADA5xBGENHOS0/Uq9+9VJefN1DNrT7954sf6yevbWUcCQCEEMIIIp4rLkbP3HKRvnflaEnSc2v3a85T63SoutHkygAAEmEEUcJqtajgqjF66uZcJTrt+ujAcV37qw/09tZys0sDgKhHGEFUufqCdL3+vUs1MdOlmsYW3f7HIv3kta1qbuXJvwBgFsIIoo5/xdZL9O1Lh0vyd9t89bdrta+q3uTKACA6EUYQlRx2q+6/fpye/dYUpcTHaGuZW9f+6gP9sXA/q7YCQB8jjCCqXXF+mt66+wu6ZOQANbZ49cCrWzX32Q0qY3ArAPQZwgiiXrorVn+6dZp+MmucnHarPthdpZmPv6+/bTrIXRIA6AOEEUD+2Tbfmj5cb949Q5OyklXb1KqCv3ys2/9YpMM13CUBgN5EGAFOMnJggl66I08/mHmeYmwWrdpWoasee19/KNwvr4+7JADQGwgjwOfYbVbNv3yUXr9rhiYPTVZdc6sWvrpVX1+6VjvK3WaXBwARhzACdOG89ES9dMcl+tkNFyjBadfmkmpd/+sP9fDKHWrwtJpdHgBEDMIIcBo2q0Vz87K1quALyh+bplafod++t0dXPLJGr31cxgBXAOgBhBGgGwa74rRsbq5+d3OuMlPiVO5u0vf+vFmzn1qnbWV03QDAubAYYfC/dm63Wy6XSzU1NUpKSjK7HES5phavnnp/r3773mdqavHJapHmTB2qu/NHa1BirNnlAUDI6O7vb8IIcJYOHm/Q4jd36I0thyVJ8Q6bvj1jhG7/wgglOO0mVwcA5iOMAH1k3d6jWvzWDn1cWi1JGtDPoe9dOVpzpg6Vw05PKIDoRRgB+pBhGFr5abke/sfOwAP3hg2IV8FVY3T9xAzZrBaTKwSAvkcYAUzQ4vVpxcZSPf7P3aqqa5YkjRzYT3ddMVrXTxwsu407JQCiB2EEMFF9c6t+/699WvbBPtU0tkiShqf20/zLR+nGSRmEEgBRgTAChIDaphb9ofCAnv5gr443+EPJ0P7xuvOLI/WVyUMUG2MzuUIA6D2EESCE1De36o/rDmjZ+3t1tN4jSUpNcGhuXrb+/eJh6t/PYXKFANDzCCNACGrwtOqF9SV69sN9KqtpkiTFxlj19dxM3XrpCA1P7WdyhQDQcwgjQAhr8fr05pbDWvbBXn16yL+Cq8UiXXl+mm7OG6YZo1JlZQYOgDBHGAHCgGEYKtx7VE9/sE/v7KgM7B82IF7/Pm2Yvp6bqRS6cACEKcIIEGY+q6zTn9Yd0F+LDqq22f9UYIfdqlkTM3TTxUM1OStZFgt3SwCED8IIEKYaPK16rbhMf1x3QFtPegjfyIH99PXcLH31wiFKS+IZOABCH2EECHOGYai4tFp/XHdAb245rKYWnyTJapFmjB6or+dm6qpxaUwPBhCyCCNABKltatFbW8r1YlGpNu4/Htif6LTr6gvSdX3OYF06KlUxLKYGIIQQRoAItb+qXn/ddFB/LToYmB4sScnxMbpmfLpmTczQtBEDeB4OANMRRoAI5/MZKio5rr9/XKY3txxWVZ0ncCw1walrxqfrqnFpunjEAJ4eDMAUhBEgirR6fVq/75he/6RMb31aruq2peclKcFp12XnDdRVY9N0+XmD5IqPMbFSANGEMAJEqRavTx9+VqW3t1bon9srdKS2OXDMZrVoanZ/5Y9L02VjUjVyYALThQH0GsIIAPl8hj45VKNV28q1aluFdlXUdTie4YrVjNEDNWNMqi4dlarkeBZYA9BzCCMATnHgaL1WbavQml1HtH7fMXlafYFjFos0MTNZXxidqmnDB+jCYcmKd9hNrBZAuCOMADitphav1u87pg92HdH7u4+cctfEbrVoYqZLF48YoGkjBmjKsBT1cxJOAHQfYQRAUA7XNOqDXVUq3HtU6/ce7TBtWPKPNxk/xKVpw/vrwqHJmpSVonQXK8EC6BphBMBZMwxDB483at3eo1q395jW7zuqg8cbT2k32BWryUOTNTkrRZOHJmv8EBcrwgIIIIwA6FGHqhu1fu9RFR04rs0l1dpR7pbvc//1sFstOn9woi4Y7NL4IUkal+HS2MGJjD0BohRhBECvqm9u1ZZDNdpcUq3NJce1qaRaVXXNp7SzWqThqf00fohLF2Qk6YIMl8YNTlJKP2buAJGOMAKgT7V37Xx6qEafltVoa5lbW8vcHdY5OdnARKfGpCVo9KBEjUlL1Oi0BI0ZlMiibEAEIYwACAmV7qa2YHIioJQca+iy/aBEp8akJWrUoASNGNhP2QP6aXhqP2Ukx/G8HSDMEEYAhKy65lZ9VlmnXRW12l1Rq10VddpdUXvKDJ6TOWxWDR0Q3xZO4pWd6g8p2QP6KS0plqAChKDu/v5mVBmAPpfgtGtSVrImZSV32F/b1KLdlf5gsudIvfZV+beSow3yeH36rLJOn1XWnfJ+MTaLBrvilJnSvsVrSHLb6/7xSkt0ym7jYYFAqCKMAAgZibExunBoii4cmtJhv9dnqKy6UfuP1mt/Vb32VTVoX1Wd9h9tUOmxBrV4DZUca+iy+8dmtWiwK1ZDkuOU7opVelKsBiX5f6a7nEpLitWgxFiebgyYhDACIOTZrBZl9Y9XVv94zRg9sMMxr89QhbtJh6obdfB4gw4ea9TB4406WN2gQ8cbdai6US1e/+DaztZKOdmAfg6lJcUqLcmpdJc/oKQmOpXaz6EBCU4NSHAotZ9TSXF2HjAI9CDCCICwZrNalJEcp4zkOF2U3f+U4z6focraZh083qBD1Y2qcDepvKZZFbVNqqhpUrm7SZXuZnm8Ph2t9+hovUfbDp/+M2NsFvXv51BqglMDEtrDij+w9O/nUEq8Q8nxMUqOi5ErPkbJcQ7uugCnQRgBENGsVou/a8YVqyldtDEMQ8cbWlRe09QhpFS4m3WsvllH6zyqqvP/rG1uVYvXUIW7WRXuzqctdybeYWsLJw4lx8X4w0p8jFxxJ4JLYmyMEmLtSoy1K9Fpb3sdo/gYm6wM0EUEI4wAiHoWi/9OR/9+Do3T6WfsNbV4daze4w8obUHlaF2zjtZ7VFXbrKp6j2oaW1TT4FF1Y4tqGltkGFKDx6sGj/e0M4a6rk9KcPhDSntASWgLK0mxdv9rZ4z6OW2Kc9gU77ApLsaueIfNv6/tdbyj/bid2UcIKWcVRpYsWaJf/vKXKi8vV05Ojn7zm99o6tSpXbZ/8cUX9cADD2j//v0aPXq0fvGLX+jaa68966IBwCyxMbZAt1B3+HyGaptaVd3oUXVDi6obW1Td4A8s1Q1tW6NHNQ0tqm1uVV1Tq2qbW/w/m1rV6jNkGFJtc6tqm1ulmp75Hg671R9WHPaTAoz/Z2yMTU67NfDTGWNTbNtP58k/7VY57TbFxvh/OmOsim372eF8u00xNgvjbNCloMPIihUrVFBQoKVLl2ratGl6/PHHNXPmTO3cuVODBg06pf3atWs1Z84cLV68WNdff71eeOEF3Xjjjdq0aZPGjx/fI18CAEKV1WqRK94/dmTYgODONQxDza0+1Ta1qrapRXVtYcXd1Nr2ukW1ba/dTa1q9LSqweNVY4v/Lkx9c2vgdaPHqwZPa+B5Qp5WnzytPlU3tPT8l+6Cw2aV3WZRjM2qGJtVDptFMXZr4M8xgWOWtuNt++1WxVjbjtk/d6ztPdvf2261yGa1ym61yGpt//NJP20njttO2k60s57a3nLSMduJYzaLhe6zHhL0omfTpk3TRRddpCeeeEKS5PP5lJWVpbvuukv33XffKe1nz56t+vp6vf7664F9F198sSZNmqSlS5d26zNZ9AwAzl17uGloCyaNbV1H/gDTqvpmf2hpbvWqudWnphb/z8DrFl/Xx1rbjrV0PBbprBb/IGqLxeJ/bbHIarHI0rbf/9oim1Wyth2znvzacvL+k/580mtb2/tZ20LRya+tFn83o63tfQOv29oo8P6SRf42Uvt5Jz5bkm69dLiy+sf36N9Pryx65vF4VFRUpAULFgT2Wa1W5efnq7CwsNNzCgsLVVBQ0GHfzJkz9corr3T5Oc3NzWpuPjEwzO12B1MmAKATFotFsTH+bpj+ffCgwvbw09ziU4vPpxavTy2thjxe/+tW74nX7Zun1fAf83Vs698MeVrbjrW9PvlYq8+Q1+d/X6+v/c8ntlafr8P+1pP2+3zqeNz7ueNd/G+7z5B8XkNSyC9mfkZfnpTR42Gku4IKI1VVVfJ6vUpLS+uwPy0tTTt27Oj0nPLy8k7bl5eXd/k5ixcv1k9/+tNgSgMAhJiTw0+4M4zPB5i2n15/UPG1HTfaXxuGDMOQz/CvheMz/MfaX7ef4/Od9Lp9/0lt/O/Z9jrwnoa8PrW95+dfd3w/fz2SobbXPkOGFHh/tf00ZCg9Kda0v9+QnE2zYMGCDndT3G63srKyTKwIABDNLBb/+BF7+OeqkBRUGElNTZXNZlNFRUWH/RUVFUpPT+/0nPT09KDaS5LT6ZTT6QymNAAAEKaCWhLQ4XAoNzdXq1evDuzz+XxavXq18vLyOj0nLy+vQ3tJWrVqVZftAQBAdAm6m6agoEC33HKLpkyZoqlTp+rxxx9XfX295s2bJ0maO3euhgwZosWLF0uS7r77bl122WV69NFHdd1112n58uX66KOP9NRTT/XsNwEAAGEp6DAye/ZsHTlyRAsXLlR5ebkmTZqklStXBgaplpSUyGo9ccPlkksu0QsvvKD7779fP/rRjzR69Gi98sorrDECAAAkncU6I2ZgnREAAMJPd39/8xhJAABgKsIIAAAwFWEEAACYijACAABMRRgBAACmIowAAABTEUYAAICpCCMAAMBUIfnU3s9rX5fN7XabXAkAAOiu9t/bZ1pfNSzCSG1trSQpKyvL5EoAAECwamtr5XK5ujweFsvB+3w+lZWVKTExURaLpcfe1+12KysrS6WlpSwzHyK4JqGF6xF6uCahhetxeoZhqLa2VhkZGR2eW/d5YXFnxGq1KjMzs9fePykpiX+IQgzXJLRwPUIP1yS0cD26dro7Iu0YwAoAAExFGAEAAKaK6jDidDq1aNEiOZ1Os0tBG65JaOF6hB6uSWjhevSMsBjACgAAIldU3xkBAADmI4wAAABTEUYAAICpCCMAAMBUUR1GlixZouzsbMXGxmratGnasGGD2SVFpPfff1+zZs1SRkaGLBaLXnnllQ7HDcPQwoULNXjwYMXFxSk/P1+7d+/u0ObYsWO66aablJSUpOTkZN16662qq6vrw28RORYvXqyLLrpIiYmJGjRokG688Ubt3LmzQ5umpibNnz9fAwYMUEJCgr72ta+poqKiQ5uSkhJdd911io+P16BBg/SDH/xAra2tfflVIsKTTz6piRMnBhbNysvL01tvvRU4zrUw30MPPSSLxaJ77rknsI/r0rOiNoysWLFCBQUFWrRokTZt2qScnBzNnDlTlZWVZpcWcerr65WTk6MlS5Z0evzhhx/Wr3/9ay1dulTr169Xv379NHPmTDU1NQXa3HTTTdq6datWrVql119/Xe+//75uv/32vvoKEWXNmjWaP3++1q1bp1WrVqmlpUVXX3216uvrA23uvfde/f3vf9eLL76oNWvWqKysTF/96lcDx71er6677jp5PB6tXbtWzz//vJ577jktXLjQjK8U1jIzM/XQQw+pqKhIH330ka644grdcMMN2rp1qySuhdk2btyo3/3ud5o4cWKH/VyXHmZEqalTpxrz588P/Nnr9RoZGRnG4sWLTawq8kkyXn755cCffT6fkZ6ebvzyl78M7KuurjacTqfx5z//2TAMw9i2bZshydi4cWOgzVtvvWVYLBbj0KFDfVZ7pKqsrDQkGWvWrDEMw//3HxMTY7z44ouBNtu3bzckGYWFhYZhGMabb75pWK1Wo7y8PNDmySefNJKSkozm5ua+/QIRKCUlxXj66ae5Fiarra01Ro8ebaxatcq47LLLjLvvvtswDP4d6Q1ReWfE4/GoqKhI+fn5gX1Wq1X5+fkqLCw0sbLos2/fPpWXl3e4Fi6XS9OmTQtci8LCQiUnJ2vKlCmBNvn5+bJarVq/fn2f1xxpampqJEn9+/eXJBUVFamlpaXDNTn//PM1dOjQDtdkwoQJSktLC7SZOXOm3G534P/oETyv16vly5ervr5eeXl5XAuTzZ8/X9ddd12Hv3+Jf0d6Q1g8KK+nVVVVyev1dviHRJLS0tK0Y8cOk6qKTuXl5ZLU6bVoP1ZeXq5BgwZ1OG6329W/f/9AG5wdn8+ne+65R9OnT9f48eMl+f++HQ6HkpOTO7T9/DXp7Jq1H0NwtmzZory8PDU1NSkhIUEvv/yyxo0bp+LiYq6FSZYvX65NmzZp48aNpxzj35GeF5VhBIDf/Pnz9emnn+rDDz80u5Sodt5556m4uFg1NTV66aWXdMstt2jNmjVmlxW1SktLdffdd2vVqlWKjY01u5yoEJXdNKmpqbLZbKeMfK6oqFB6erpJVUWn9r/v012L9PT0UwYWt7a26tixY1yvc/Dd735Xr7/+ut59911lZmYG9qenp8vj8ai6urpD+89fk86uWfsxBMfhcGjUqFHKzc3V4sWLlZOTo1/96ldcC5MUFRWpsrJSF154oex2u+x2u9asWaNf//rXstvtSktL47r0sKgMIw6HQ7m5uVq9enVgn8/n0+rVq5WXl2diZdFn+PDhSk9P73At3G631q9fH7gWeXl5qq6uVlFRUaDNO++8I5/Pp2nTpvV5zeHOMAx997vf1csvv6x33nlHw4cP73A8NzdXMTExHa7Jzp07VVJS0uGabNmypUNIXLVqlZKSkjRu3Li++SIRzOfzqbm5mWthkiuvvFJbtmxRcXFxYJsyZYpuuummwGuuSw8zewStWZYvX244nU7jueeeM7Zt22bcfvvtRnJycoeRz+gZtbW1xubNm43NmzcbkozHHnvM2Lx5s3HgwAHDMAzjoYceMpKTk41XX33V+OSTT4wbbrjBGD58uNHY2Bh4jy996UvG5MmTjfXr1xsffvihMXr0aGPOnDlmfaWwdueddxoul8t47733jMOHDwe2hoaGQJs77rjDGDp0qPHOO+8YH330kZGXl2fk5eUFjre2thrjx483rr76aqO4uNhYuXKlMXDgQGPBggVmfKWwdt999xlr1qwx9u3bZ3zyySfGfffdZ1gsFuPtt982DINrESpOnk1jGFyXnha1YcQwDOM3v/mNMXToUMPhcBhTp0411q1bZ3ZJEendd981JJ2y3XLLLYZh+Kf3PvDAA0ZaWprhdDqNK6+80ti5c2eH9zh69KgxZ84cIyEhwUhKSjLmzZtn1NbWmvBtwl9n10KS8fvf/z7QprGx0fjOd75jpKSkGPHx8cZXvvIV4/Dhwx3eZ//+/cY111xjxMXFGampqcZ//ud/Gi0tLX38bcLff/zHfxjDhg0zHA6HMXDgQOPKK68MBBHD4FqEis+HEa5Lz7IYhmGYc08GAAAgSseMAACA0EEYAQAApiKMAAAAUxFGAACAqQgjAADAVIQRAABgKsIIAAAwFWEEAACYijACAABMRRgBAACmIowAAABTEUYAAICp/h+tNwJCkeidIQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "a=1\n",
    "v=[]\n",
    "for i in range(450):\n",
    "    v.append(a)\n",
    "    a=0.985*a\n",
    "\n",
    "plt.plot(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "93d492af-4fc7-4661-9c87-e3181d7c481c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([4.4133e+11,        nan]),\n",
       " torch.return_types.max(\n",
       " values=tensor(nan),\n",
       " indices=tensor(1)))"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=torch.tensor([441325321423,torch.nan])\n",
    "\n",
    "a,a.max(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "561fe871-4c0c-4df8-ada2-e8cdd491efdf",
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[54], line 9\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnosaveddata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m      8\u001b[0m Q\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mtensor([[\u001b[38;5;241m0.8\u001b[39m, \u001b[38;5;241m1.2\u001b[39m, \u001b[38;5;241m0\u001b[39m],[\u001b[38;5;241m0.8\u001b[39m, \u001b[38;5;241m1.2\u001b[39m, \u001b[38;5;241m0\u001b[39m]])\n\u001b[1;32m----> 9\u001b[0m n\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mtensor([[\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m0\u001b[39m],[\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m0\u001b[39m]])\n\u001b[0;32m     10\u001b[0m values \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m])\n\u001b[0;32m     11\u001b[0m probs \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m3\u001b[39m),\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from nosaveddata import *\n",
    "\n",
    "\n",
    "Q=torch.tensor([[0.8, 1.2, 0],[0.8, 1.2, 0]])\n",
    "n=torch.tensor([[1,1,0],[1,1]])\n",
    "values = torch.tensor([1,2])\n",
    "probs = F.softmax(torch.randn(2,3),-1)\n",
    "\n",
    "n.shape, values.shape, (n*values[:,None]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "21e7934e-3800-4185-8dad-967d3b93e37c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1.), tensor(0.5000))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g=1\n",
    "Q=torch.tensor([[1, 1.2, 0],[1, 1.2, 0]])\n",
    "n=torch.tensor([[1,1,0],[1,1,0]])\n",
    "\n",
    "\n",
    "\n",
    "Q[0,0]= (n[0,0]*Q[0,0]+g) / (n[0,0]+1)\n",
    "n[0,0]+=1\n",
    "Q[0,0], n[0,0]/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414b4617-8aee-47cc-ae5d-50efadec5f11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8cd4ddab-2cd3-4395-8ced-d2fb0be25fec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack([n,n,n,n],0).float().mean(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e9b06424-c1f2-4649-bc5c-95d2dd7af627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(torch.Size([2]), torch.Size([2]), torch.Size([2]), torch.Size([2]))\n",
      "torch.Size([2, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.8000, 1.2000, 0.0000],\n",
       "        [0.8000, 1.2000, 0.0000]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "v_mix = (1/(1+n.sum(-1))) * ( values + (n.sum(-1)/(((n!=0)*probs).sum(-1)) ) * ((n!=0)*Q*probs).sum(-1) )\n",
    "print(f\"{v_mix.shape, (1/(1+n.sum(-1))).shape, (n.sum(-1)/(((n!=0)*probs).sum(-1)) ).shape, ((n!=0)*Q*probs).sum(-1).shape}\")\n",
    "v_mix = v_mix[:,None].repeat_interleave(Q.shape[-1], -1)\n",
    "\n",
    "print(f\"{v_mix.shape}\")\n",
    "completed_Q = (n==0)*(Q*probs) + (n>0)*Q\n",
    "\n",
    "\n",
    "completed_Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799a7b0f-66be-4239-9513-76698a0691d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc38641-3e7e-4ee0-b301-ef2d872e2b6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5bd4c710-8d4a-47fc-ab9c-da5b1ba7d897",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(pid=29644)\u001b[0m C:\\Users\\sneep\\anaconda3\\envs\\python_\\Lib\\site-packages\\diffusers\\utils\\outputs.py:63: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "\u001b[36m(pid=29644)\u001b[0m   torch.utils._pytree._register_pytree_node(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch tensor([[ 0.8652,  1.5215,  0.1581, -1.1357],\n",
      "        [-0.4001, -0.0973,  0.0908,  0.3312]])\n"
     ]
    }
   ],
   "source": [
    "from ray.util.queue import Queue\n",
    "import ray\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import time\n",
    "\n",
    "from nosaveddata import *\n",
    "\n",
    "\n",
    "class QueueStorage(object):\n",
    "    def __init__(self, threshold=15, size=20):\n",
    "        \"\"\"Queue storage\n",
    "        Parameters\n",
    "        ----------\n",
    "        threshold: int\n",
    "            if the current size if larger than threshold, the data won't be collected\n",
    "        size: int\n",
    "            the size of the queue\n",
    "        \"\"\"\n",
    "        self.threshold = threshold\n",
    "        self.queue = Queue(maxsize=size)\n",
    "\n",
    "    def push(self, batch):\n",
    "        if self.queue.qsize() <= self.threshold:\n",
    "            self.queue.put(batch)\n",
    "\n",
    "    def pop(self):\n",
    "        if self.queue.qsize() > 0:\n",
    "            return self.queue.get()\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.queue.qsize()\n",
    "\n",
    "\n",
    "mcts_storage = QueueStorage()\n",
    "\n",
    "\n",
    "\n",
    "@ray.remote\n",
    "class Buffer(Hypers):\n",
    "    def __init__(self, storage):\n",
    "        super().__init__()\n",
    "\n",
    "    def rand_data(self):\n",
    "        return torch.randn(2,4)\n",
    "\n",
    "    \n",
    "    def run(self):\n",
    "        if len(self.storage) < 20:\n",
    "            data = self.rand_data()\n",
    "            #print(f\"DATA on RUN {data}\")\n",
    "            self.storage.push(data)\n",
    "        pass\n",
    "\n",
    "\n",
    "workers = []\n",
    "\n",
    "cpu_workers = [Buffer.remote(mcts_storage) for i in range(2)]\n",
    "workers += [cpu_worker.run.remote() for cpu_worker in cpu_workers]\n",
    "\n",
    "time.sleep(10)\n",
    "\n",
    "batch = mcts_storage.pop()\n",
    "\n",
    "print(f\"batch {batch}\")\n",
    "#ray.wait(workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb34e19c-20e2-4c56-b295-60ab9a3662c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4074d1c-f628-4cea-bcc6-a85018868276",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "181922ec-2896-489c-b26b-a280c8082d47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sneep\\anaconda3\\envs\\python_\\Lib\\site-packages\\diffusers\\utils\\outputs.py:63: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.6931)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import time\n",
    "\n",
    "import math\n",
    "\n",
    "from nosaveddata import *\n",
    "\n",
    "symlog(torch.tensor(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caddd798-8430-4440-b297-8608cd8cbe2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "540beb1b-bfeb-43b8-9622-341fdcb7adc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Efficient Zero Network Parameters: 5.82M\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 1, 7]), torch.Size([3, 1, 7]), torch.Size([3, 1, 51]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import time\n",
    "\n",
    "import math\n",
    "\n",
    "from nosaveddata import *\n",
    "\n",
    "\n",
    "\n",
    "class Residual_Block(nn.Module):\n",
    "    def __init__(self, in_channels, channels, stride=1, act=nn.SiLU(), out_act=nn.SiLU(), norm=True, init=init_xavier, bias=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        \n",
    "\n",
    "        conv1 = nn.Sequential(nn.Conv2d(in_channels, channels, kernel_size=3, padding=1,\n",
    "                                            stride=stride, bias=bias),\n",
    "                              (nn.GroupNorm(32, channels, eps=1e-6) if channels%32==0 else nn.BatchNorm2d(channels, eps=1e-6)) if norm else nn.Identity(),\n",
    "                              act)\n",
    "        conv2 = nn.Sequential(nn.Conv2d(channels, channels, kernel_size=3, padding=1, bias=bias),\n",
    "                              (nn.GroupNorm(32, channels, eps=1e-6) if channels%32==0 else nn.BatchNorm2d(channels, eps=1e-6)) if norm else nn.Identity())\n",
    "\n",
    "        conv1.apply(init)\n",
    "        conv2.apply(init if out_act!=nn.Identity() else init_xavier)\n",
    "        \n",
    "        self.conv = nn.Sequential(conv1, conv2)\n",
    "        \n",
    "        self.proj=nn.Identity()\n",
    "        if stride>1 or in_channels!=channels:\n",
    "            self.proj = nn.Conv2d(in_channels, channels, kernel_size=3, padding=1,\n",
    "                        stride=stride)\n",
    "        \n",
    "        self.proj.apply(init_proj2d)\n",
    "        self.out_act = out_act\n",
    "        \n",
    "    def forward(self, X):\n",
    "        Y = self.conv(X)\n",
    "        Y = Y+self.proj(X)\n",
    "        return self.out_act(Y)\n",
    "    \n",
    "\n",
    "\n",
    "class EffZ_Perception(nsd_Module):\n",
    "    def __init__(self, n_actions, scale_width=1, act=nn.ReLU()):\n",
    "        super().__init__()\n",
    "        \n",
    "        \n",
    "        self.conv1 = nn.Sequential(nn.Conv2d(12, 32, 3, stride=2, padding=1, bias=False),\n",
    "                                   nn.BatchNorm2d(32),\n",
    "                                   act)\n",
    "        self.conv2 = Residual_Block(32, 32, act=act, out_act=act)\n",
    "        self.conv3 = Residual_Block(32, 64, act=act, out_act=act, stride=2)\n",
    "        self.conv4 = Residual_Block(64, 64, act=act, out_act=act)\n",
    "        self.pool1 = nn.AvgPool2d(3, stride=2, padding=1)\n",
    "        self.conv5 = Residual_Block(64, 64, act=act, out_act=act)\n",
    "        self.pool2 = nn.AvgPool2d(3, stride=2, padding=1)\n",
    "        self.conv6 = Residual_Block(64, 64, act=act, out_act=act)\n",
    "        \n",
    "        self.conv1.apply(init_xavier)\n",
    "        \n",
    "        self.conv = nn.Sequential(self.conv1, self.conv2, self.conv3, self.conv4, self.pool1,\n",
    "                                   self.conv5, self.pool2, self.conv6)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        X = self.conv(X)\n",
    "        return X\n",
    "\n",
    "class _1conv_residual(nn.Module):\n",
    "    def __init__(self, hiddens, act=nn.ReLU()):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.net = nn.Sequential(nn.Conv2d(hiddens+1, hiddens, 3, padding=1, bias=False),\n",
    "                                        nn.BatchNorm2d(hiddens))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        proj = x[:,:-1]\n",
    "        x = self.net(x)\n",
    "        \n",
    "        return x+proj\n",
    "        \n",
    "class RewardPred(nsd_Module):\n",
    "    def __init__(self, in_channels, out_channels, in_hiddens, hiddens, bottleneck=32, out_dim=51, act=nn.ReLU(), k=5):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv = nn.Sequential(nn.Conv2d(in_channels, out_channels, 1),\n",
    "                                 nn.BatchNorm2d(out_channels),\n",
    "                                 act)\n",
    "        \n",
    "        self.lstm = nn.LSTMCell(in_hiddens, hiddens)\n",
    "        self.norm_relu = nn.Sequential(nn.BatchNorm1d(hiddens))\n",
    "        \n",
    "        self.mlp = MLP_LayerNorm(hiddens, bottleneck, out_dim, layers=2, in_act=act, init=init_xavier, last_init=init_zeros)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        bs, seq = x.shape[:2]\n",
    "        \n",
    "        x = self.conv(x.view(bs*seq, *x.shape[-3:])).view(bs,seq,-1)\n",
    "        \n",
    "        ht = torch.zeros(x.shape[0], self.hiddens, device='cuda')\n",
    "        ct = torch.zeros_like(ht)\n",
    "        \n",
    "        hs = []\n",
    "        for i in range(self.k):\n",
    "            \n",
    "            ht, ct = self.lstm(x[:,i], (ht, ct))\n",
    "            hs.append(ht)\n",
    "        hs = torch.stack(hs,1)\n",
    "        \n",
    "        x = self.mlp(hs)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def transition_one_step(self, x, ht):\n",
    "        \n",
    "        x = self.conv(x).view(x.shape[0],-1)\n",
    "        #print('reward one step', x.shape)\n",
    "        \n",
    "        ht, ct = self.lstm(x, ht)\n",
    "        \n",
    "        x = self.mlp(ht)\n",
    "        \n",
    "        return x, (ht,ct)\n",
    "        \n",
    "\n",
    "class ActorCritic(nsd_Module):\n",
    "    def __init__(self, in_channels, out_channels, in_hiddens, bottleneck=32, out_value=51, out_policy=1, act=nn.ReLU()):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.residual = Residual_Block(in_channels, in_channels, act=self.act, out_act=self.act)\n",
    "        \n",
    "        conv_policy = nn.Sequential(nn.Conv2d(in_channels, out_channels, 1),\n",
    "                                 nn.BatchNorm2d(out_channels),\n",
    "                                 act)\n",
    "        conv_value  = nn.Sequential(nn.Conv2d(in_channels, out_channels, 1),\n",
    "                                 nn.BatchNorm2d(out_channels),\n",
    "                                 act)\n",
    "        \n",
    "        self.policy = nn.Sequential(conv_policy,\n",
    "                                    nn.Flatten(-3,-1),\n",
    "                                   MLP_LayerNorm(in_hiddens, bottleneck, out_policy, layers=2, in_act=act, init=init_xavier, last_init=init_zeros))\n",
    "        self.value = nn.Sequential(conv_policy,\n",
    "                                    nn.Flatten(-3,-1),\n",
    "                                   MLP_LayerNorm(in_hiddens, bottleneck, out_value,  layers=2, in_act=act,\n",
    "                                                 out_act=nn.Softmax(dim=-1), init=init_xavier, last_init=init_zeros))\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        bs, seq = x.shape[:2]\n",
    "        x = self.residual(x.view(-1, *x.shape[-3:]))\n",
    "        \n",
    "        logits = self.policy(x).view(bs, seq, -1)\n",
    "        probs = F.softmax(logits, -1)\n",
    "        \n",
    "        value_probs = self.value(x).view(bs, seq, -1)\n",
    "        \n",
    "        return logits, probs, value_probs\n",
    "    \n",
    "        \n",
    "    \n",
    "class EfficientZero(nsd_Module):\n",
    "    def __init__(self, n_actions, hiddens=512, mlp_layers=1, scale_width=4,\n",
    "                 n_atoms=51, Vmin=-10, Vmax=10):\n",
    "        super().__init__()\n",
    "        self.support = torch.linspace(Vmin, Vmax, n_atoms).cuda()\n",
    "        \n",
    "        self.hiddens=hiddens\n",
    "        self.scale_width=scale_width\n",
    "        self.act = nn.ReLU()\n",
    "        \n",
    "        \n",
    "        #self.encoder_cnn = IMPALA_Resnet(scale_width=scale_width, norm=False, init=init_xavier, act=self.act)\n",
    "        self.encoder_cnn = EffZ_Perception(n_actions, scale_width)\n",
    "        \n",
    "\n",
    "        self.projection = MLP_LayerNorm(2304*scale_width, hiddens, hiddens*2,\n",
    "                                        init=init_xavier, last_init=init_xavier, layers=3, in_act=self.act,\n",
    "                                        add_last_norm=False)\n",
    "        self.prediction = MLP_LayerNorm(hiddens*2, hiddens, hiddens*2, layers=2,\n",
    "                                        init=init_xavier, last_init=init_xavier,\n",
    "                                        in_act=self.act, add_last_norm=False)\n",
    "        \n",
    "                                       \n",
    "            \n",
    "        self.transition = nn.Sequential(_1conv_residual(64, self.act),\n",
    "                                        Residual_Block(64, 64, act=self.act, out_act=self.act))\n",
    "        \n",
    "        self.reward_pred = RewardPred(64, 16, 16*((96//16)**2), hiddens)\n",
    "        #self.reward_pred = RewardPred(64, 16, 576, hiddens)\n",
    "\n",
    "        \n",
    "        self.ac = ActorCritic(64, 16, 16*((96//16)**2), out_policy=n_actions)\n",
    "    \n",
    "        params_count(self, 'Efficient Zero Network')\n",
    "        \n",
    "    \n",
    "    def forward(self, X, y_action):\n",
    "        z_proj, z = self.encode(X)\n",
    "        \n",
    "        \n",
    "        #q, action = self.q_head(X)\n",
    "        logits, probs, value_probs = self.ac(z)\n",
    "        \n",
    "        z_proj_pred, reward_pred = self.get_transition(z[:,0][:,None], y_action)\n",
    "\n",
    "        #return q, action, X[:,1:].clone().detach(), z_pred\n",
    "        return z_proj, z_proj_pred, reward_pred, logits, probs, value_probs\n",
    "    \n",
    "\n",
    "    def encode(self, X):\n",
    "        batch, seq = X.shape[:2]\n",
    "        self.batch = batch\n",
    "        self.seq = seq\n",
    "        X = self.encoder_cnn(X.contiguous().view(self.batch*self.seq, *(X.shape[2:])))\n",
    "        X = X.contiguous().view(self.batch, self.seq, *X.shape[-3:])\n",
    "        z = X.clone()\n",
    "        X = X.flatten(-3,-1)\n",
    "        X = self.projection(X)\n",
    "        return X, z\n",
    "    \n",
    "    def encode_z(self, X):\n",
    "        batch, seq = X.shape[:2]\n",
    "        self.batch = batch\n",
    "        self.seq = seq\n",
    "        X = self.encoder_cnn(X.contiguous().view(self.batch*self.seq, *(X.shape[2:])))\n",
    "        X = X.contiguous().view(self.batch, self.seq, *X.shape[-3:])\n",
    "        \n",
    "        return X\n",
    "    \n",
    "    def get_zero_ht(self, batch_size):\n",
    "        ht = torch.zeros(batch_size, self.hiddens, device='cuda')\n",
    "        ct = torch.zeros_like(ht)\n",
    "        return (ht, ct)\n",
    "    \n",
    "    def transition_one_step(self, z, action, ht):\n",
    "        z = z.contiguous().view(-1, *z.shape[-3:])\n",
    "        \n",
    "        action_one_hot = (\n",
    "            torch.ones(\n",
    "                (\n",
    "                    z.shape[0],\n",
    "                    z.shape[2],\n",
    "                    z.shape[3],\n",
    "                )\n",
    "            )\n",
    "            .to(action.device)\n",
    "            .float()\n",
    "        )\n",
    "        \n",
    "        action = (action[:, None, None] * action_one_hot / self.n_actions)[:,None]\n",
    "        #print('one step', z.shape, action.shape)\n",
    "        z_pred = torch.cat( (z, action), 1)\n",
    "        z_pred = self.transition(z_pred)\n",
    "        \n",
    "        #print('one step z transition', z.shape)\n",
    "        \n",
    "        \n",
    "        reward_pred, ht = self.reward_pred.transition_one_step(z_pred, ht)\n",
    "        \n",
    "        #print('one step z_pred reward_pred', z_pred.shape, reward_pred.shape)\n",
    "\n",
    "        \n",
    "        return z_pred, reward_pred, ht\n",
    "    \n",
    "    def get_transition(self, z, action):\n",
    "        z = z.contiguous().view(-1, *z.shape[-3:])\n",
    "        \n",
    "        action_one_hot = (\n",
    "            torch.ones(\n",
    "                (\n",
    "                    z.shape[0],\n",
    "                    5,\n",
    "                    z.shape[2],\n",
    "                    z.shape[3],\n",
    "                )\n",
    "            )\n",
    "            .to(action.device)\n",
    "            .float()\n",
    "        )\n",
    "        \n",
    "        action = (action[:, :, None, None] * action_one_hot / self.n_actions)[:,:,None]\n",
    "\n",
    "        #print('transition full', z.shape, action.shape)\n",
    "        z_pred = torch.cat( (z, action[:,0]), 1)\n",
    "        z_pred = self.transition(z_pred)\n",
    "        \n",
    "        \n",
    "        z_preds=[z_pred.clone()]\n",
    "        \n",
    "\n",
    "        for k in range(4):\n",
    "            z_pred = torch.cat( (z_pred, action[:,k+1]), 1)\n",
    "            z_pred = self.transition(z_pred)\n",
    "            \n",
    "            \n",
    "            z_preds.append(z_pred)\n",
    "        \n",
    "        \n",
    "        z_pred = torch.stack(z_preds,1)\n",
    "        \n",
    "        reward_pred = self.reward_pred(z_pred)\n",
    "        \n",
    "        #print('transition full z_pred reward_pred', z_pred.shape, reward_pred.shape)\n",
    "\n",
    "        z_proj_pred = self.projection(z_pred.flatten(-3,-1)).view(self.batch,5,-1)\n",
    "        z_proj_pred = self.prediction(z_proj_pred)\n",
    "        \n",
    "        return z_proj_pred, reward_pred\n",
    "\n",
    "    \n",
    "    def q_head(self, X):\n",
    "        q = self.dueling_dqn(X)\n",
    "        action = (q*self.support).sum(-1).argmax(-1)\n",
    "        \n",
    "        return q, action\n",
    "\n",
    "\n",
    "    def dueling_dqn(self, X):\n",
    "        X = F.relu(X)\n",
    "        \n",
    "        a = self.a(X).view(self.batch, -1, n_actions, num_buckets)\n",
    "        v = self.v(X).view(self.batch, -1, 1, num_buckets)\n",
    "        \n",
    "        q = v + a - a.mean(-2,keepdim=True)\n",
    "        q = F.softmax(q,-1)\n",
    "        \n",
    "        return q\n",
    "\n",
    "\n",
    "n_actions=7\n",
    "num_buckets=51\n",
    "\n",
    "model = EfficientZero(n_actions, 512, scale_width=1).cuda()\n",
    "\n",
    "\n",
    "x = torch.randn(3,1,12,96,96).cuda()\n",
    "actions = torch.randint(0,n_actions,(x.shape[0],5)).cuda()\n",
    "\n",
    "z = model.encode_z(x)\n",
    "\n",
    "ht = model.get_zero_ht(3)\n",
    "z_pred, reward_pred, ht = model.transition_one_step(z, actions[:,0], ht)\n",
    "z_pred, reward_pred, ht = model.transition_one_step(z_pred, actions[:,0], ht)\n",
    "\n",
    "#print(z.shape, z_pred.shape)\n",
    "\n",
    "logits, probs, value_probs = model.ac(z_pred[:,None])\n",
    "\n",
    "logits.shape, probs.shape, value_probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31374083-d629-4bac-90ee-b953545c1734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(torch.Size([4, 51]), torch.Size([4, 51]), torch.Size([51]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sneep\\AppData\\Local\\Temp\\ipykernel_13696\\2895786251.py:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  c = F.softmax(torch.zeros(3,51))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import time\n",
    "\n",
    "import math\n",
    "\n",
    "from nosaveddata import *\n",
    "\n",
    "def project_distribution(supports, weights, target_support):\n",
    "    with torch.no_grad():\n",
    "        print(f\"{supports.shape, weights.shape, target_support.shape}\")\n",
    "        v_min, v_max = target_support[0], target_support[-1]\n",
    "        # `N` in Eq7.\n",
    "        num_dims = target_support.shape[-1]\n",
    "        # delta_z = `\\Delta z` in Eq7.\n",
    "        delta_z = (v_max - v_min) / (num_buckets-1)\n",
    "        # clipped_support = `[\\hat{T}_{z_j}]^{V_max}_{V_min}` in Eq7.\n",
    "        clipped_support = supports.clip(v_min, v_max)\n",
    "        # numerator = `|clipped_support - z_i|` in Eq7.\n",
    "        numerator = (clipped_support[:,None] - target_support[None,:,None].repeat_interleave(clipped_support.shape[0],0)).abs()\n",
    "        quotient = 1 - (numerator / delta_z)\n",
    "        # clipped_quotient = `[1 - numerator / (\\Delta z)]_0^1` in Eq7.\n",
    "        clipped_quotient = quotient.clip(0, 1)\n",
    "        # inner_prod = `\\sum_{j=0}^{N-1} clipped_quotient * p_j(x', \\pi(x'))` in Eq7.\n",
    "        inner_prod = (clipped_quotient * weights[:,None]).sum(-1)\n",
    "        #inner_prod = (clipped_quotient).sum(-1) * weights\n",
    "        return inner_prod.squeeze()\n",
    "    \n",
    "\n",
    "support = torch.linspace(-10,10,51).cuda()\n",
    "q = F.softmax(torch.randn(4,51),-1).cuda()\n",
    "\n",
    "rewards = torch.tensor([1,1,1,1])[:,None].clip(-1,1).cuda()\n",
    "\n",
    "G = rewards + 0.997*support\n",
    "\n",
    "dist = project_distribution(G, torch.ones_like(q), support).cuda()\n",
    "dist#-dist*torch.log(q)\n",
    "\n",
    "c = F.softmax(torch.zeros(3,51))\n",
    "c[0,-1]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c4f6e5a6-fccf-403d-b535-dbe601c114a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sneep\\AppData\\Local\\Temp\\ipykernel_13696\\2374444164.py:5: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  p = F.softmax(a)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "a=torch.randn(3)\n",
    "p = F.softmax(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5580782-d854-4ad6-ab4b-9008af3acd4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0]), tensor([0.5413, 0.2273, 0.2314]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.multinomial(p, 1), p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c92f714-fe68-414b-8865-2d2e57a54a90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec56835-663f-491a-aa12-68b3582d7fa8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "badd5af6-bbfb-4dee-8382-86b02462756d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sneep\\AppData\\Local\\Temp\\ipykernel_13696\\1726576003.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  c = F.softmax(torch.zeros(3,51))\n",
      "100%|| 40000/40000 [00:37<00:00, 1059.37it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.0001, 0.0001, 0.0083, 0.0197, 0.0197, 0.0197, 0.0197, 0.0197, 0.0197,\n",
       "         0.0197, 0.0197, 0.0197, 0.0197, 0.0197, 0.0197, 0.0197, 0.0197, 0.0197,\n",
       "         0.0197, 0.0197, 0.0197, 0.0197, 0.0197, 0.0197, 0.0197, 0.0197, 0.0197,\n",
       "         0.0197, 0.0197, 0.0197, 0.0197, 0.0197, 0.0197, 0.0197, 0.0197, 0.0197,\n",
       "         0.0197, 0.0197, 0.0197, 0.0197, 0.0197, 0.0197, 0.0197, 0.0197, 0.0197,\n",
       "         0.0197, 0.0197, 0.0197, 0.0197, 0.0197, 0.0673],\n",
       "        [0.0001, 0.0001, 0.0083, 0.0196, 0.0197, 0.0196, 0.0197, 0.0196, 0.0196,\n",
       "         0.0196, 0.0196, 0.0197, 0.0196, 0.0197, 0.0197, 0.0196, 0.0196, 0.0196,\n",
       "         0.0197, 0.0196, 0.0197, 0.0196, 0.0196, 0.0196, 0.0197, 0.0196, 0.0197,\n",
       "         0.0197, 0.0197, 0.0197, 0.0197, 0.0196, 0.0196, 0.0196, 0.0197, 0.0197,\n",
       "         0.0196, 0.0197, 0.0197, 0.0197, 0.0197, 0.0196, 0.0197, 0.0197, 0.0196,\n",
       "         0.0197, 0.0196, 0.0197, 0.0197, 0.0196, 0.0673],\n",
       "        [0.0001, 0.0001, 0.0083, 0.0197, 0.0196, 0.0197, 0.0196, 0.0197, 0.0197,\n",
       "         0.0197, 0.0197, 0.0197, 0.0197, 0.0196, 0.0196, 0.0197, 0.0197, 0.0197,\n",
       "         0.0196, 0.0197, 0.0196, 0.0197, 0.0197, 0.0197, 0.0196, 0.0197, 0.0196,\n",
       "         0.0196, 0.0196, 0.0196, 0.0196, 0.0197, 0.0197, 0.0197, 0.0196, 0.0196,\n",
       "         0.0197, 0.0196, 0.0196, 0.0197, 0.0196, 0.0197, 0.0196, 0.0196, 0.0197,\n",
       "         0.0196, 0.0197, 0.0196, 0.0196, 0.0197, 0.0673],\n",
       "        [0.0001, 0.0001, 0.0083, 0.0197, 0.0197, 0.0197, 0.0197, 0.0197, 0.0197,\n",
       "         0.0197, 0.0197, 0.0197, 0.0197, 0.0197, 0.0197, 0.0197, 0.0197, 0.0197,\n",
       "         0.0197, 0.0197, 0.0197, 0.0197, 0.0197, 0.0197, 0.0197, 0.0197, 0.0197,\n",
       "         0.0197, 0.0197, 0.0197, 0.0197, 0.0197, 0.0197, 0.0197, 0.0197, 0.0197,\n",
       "         0.0197, 0.0197, 0.0197, 0.0197, 0.0197, 0.0197, 0.0197, 0.0197, 0.0197,\n",
       "         0.0197, 0.0197, 0.0197, 0.0197, 0.0197, 0.0673]], device='cuda:0',\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tqdm\n",
    "\n",
    "def project_distribution(supports, weights, target_support):\n",
    "    with torch.no_grad():\n",
    "        v_min, v_max = target_support[0], target_support[-1]\n",
    "        # `N` in Eq7.\n",
    "        num_dims = target_support.shape[-1]\n",
    "        # delta_z = `\\Delta z` in Eq7.\n",
    "        delta_z = (v_max - v_min) / (num_buckets-1)\n",
    "        # clipped_support = `[\\hat{T}_{z_j}]^{V_max}_{V_min}` in Eq7.\n",
    "        clipped_support = supports.clip(v_min, v_max)\n",
    "        # numerator = `|clipped_support - z_i|` in Eq7.\n",
    "        numerator = (clipped_support[:,None] - target_support[None,:,None].repeat_interleave(clipped_support.shape[0],0)).abs()\n",
    "        quotient = 1 - (numerator / delta_z)\n",
    "        # clipped_quotient = `[1 - numerator / (\\Delta z)]_0^1` in Eq7.\n",
    "        clipped_quotient = quotient.clip(0, 1)\n",
    "        # inner_prod = `\\sum_{j=0}^{N-1} clipped_quotient * p_j(x', \\pi(x'))` in Eq7.\n",
    "        inner_prod = (clipped_quotient * weights[:,None]).sum(-1)\n",
    "        #inner_prod = (clipped_quotient).sum(-1) * weights\n",
    "        return inner_prod.squeeze()\n",
    "    \n",
    "\n",
    "support = torch.linspace(-10,10,51).cuda()\n",
    "q = F.softmax(torch.randn(4,51),-1).cuda()\n",
    "\n",
    "rewards = torch.tensor([1,1,1,1])[:,None].clip(-1,1).cuda()\n",
    "\n",
    "G = rewards + 0.997*support\n",
    "\n",
    "dist = project_distribution(G, torch.ones_like(q), support).cuda()\n",
    "dist#-dist*torch.log(q)\n",
    "\n",
    "c = F.softmax(torch.zeros(3,51))\n",
    "c[0,-1]=1\n",
    "\n",
    "\n",
    "model = nn.Sequential(nn.Linear(51,51),\n",
    "                      nn.Softmax(-1)).cuda()\n",
    "optim = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "\n",
    "for i in tqdm.tqdm(range(40000)):\n",
    "    x=torch.randn(4,51).cuda()\n",
    "    \n",
    "    y = model(x)\n",
    "    \n",
    "    loss = -(dist*torch.log(y))\n",
    "    \n",
    "    loss = loss.sum(-1).mean()\n",
    "    loss.backward()\n",
    "    \n",
    "    optim.step()\n",
    "    optim.zero_grad()\n",
    "\n",
    "y#(dist*support).mean(-1), (c*support).mean(-1), (F.softmax(dist,-1)*support).sum(-1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c9c491a6-b303-47c5-a083-ac628355b66f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9637, 0.9644, 0.9632, 0.9639], device='cuda:0',\n",
       "       grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(y*support).sum(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5b1fdad6-00af-4042-8b72-fdc14b2d29fd",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Sequential.forward() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m5\u001b[39m,\u001b[38;5;241m12\u001b[39m,\u001b[38;5;241m96\u001b[39m,\u001b[38;5;241m96\u001b[39m)\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m----> 3\u001b[0m z_proj, z_proj_pred, reward_pred, logits, probs, value_probs \u001b[38;5;241m=\u001b[39m model(x, actions)\n\u001b[0;32m      4\u001b[0m z_proj\u001b[38;5;241m.\u001b[39mshape, z_proj_pred\u001b[38;5;241m.\u001b[39mshape, probs\u001b[38;5;241m.\u001b[39mshape, value_probs\u001b[38;5;241m.\u001b[39mshape, reward_pred\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\python_\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\python_\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: Sequential.forward() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3,5,12,96,96).cuda()\n",
    "\n",
    "z_proj, z_proj_pred, reward_pred, logits, probs, value_probs = model(x, actions)\n",
    "z_proj.shape, z_proj_pred.shape, probs.shape, value_probs.shape, reward_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49663d55-2c55-4f90-8209-b7c666cde2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import time\n",
    "\n",
    "import math\n",
    "\n",
    "from nosaveddata import *\n",
    "\n",
    "n_actions=7\n",
    "num_buckets=51\n",
    "\n",
    "model = EfficientZero(n_actions, 512, scale_width=1).cuda()\n",
    "\n",
    "\n",
    "x = torch.randn(3,1,12,96,96).cuda()\n",
    "actions = torch.randint(0,n_actions,(x.shape[0],5)).cuda()\n",
    "\n",
    "z = model.encode_z(x)\n",
    "\n",
    "ht = model.get_zero_ht(3)\n",
    "z_pred, reward_pred, ht = model.transition_one_step(z, actions[:,0], ht)\n",
    "print(ht)\n",
    "z_pred, reward_pred, ht = model.transition_one_step(z_pred, actions[:,0], ht)\n",
    "\n",
    "print(z.shape, z_pred.shape)\n",
    "\n",
    "logits, probs, value_probs = model.ac(z_pred[:,None])\n",
    "\n",
    "logits.shape, probs.shape, value_probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d9b65b-b8a9-4019-b4a0-a6d7a9ad16ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for i in range(1000):\n",
    "    buckets = torch.linspace(-299,299,51)\n",
    "\n",
    "    values = torch.randn(3)*200\n",
    "    values_bucketized = torch.bucketize(values, buckets)\n",
    "    values, values_bucketized\n",
    "    buckets[values_bucketized-1]\n",
    "\n",
    "values, values_bucketized, buckets[values_bucketized-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d615da-82f0-424f-b147-566cb2bda18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "boundaries = torch.linspace(-20,20,51).cuda()\n",
    "#values = torch.tensor([[-1,0,1],[2,30,-15]]).cuda()\n",
    "values = torch.tensor([-1,0,1,2,30,-15]).cuda()\n",
    "#torch.bucketize(values, buckets)\n",
    "\n",
    "th = two_hot(values, 51, boundaries)#-1/51\n",
    "(th*symexp(boundaries)).sum(-1), th\n",
    "#symlog(values), buckets[26]\n",
    "#symexp(symlog(values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb0a18b-aad0-4c5b-9f25-5b0927ef2377",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=torch.tensor([1,0,-1])\n",
    "a!=-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be866db-c464-4266-ad03-8ce0524d6594",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import time\n",
    "\n",
    "import math\n",
    "\n",
    "from nosaveddata import *\n",
    "\n",
    "\n",
    "def renormalize(tensor, has_batch=False):\n",
    "    shape = tensor.shape\n",
    "    tensor = tensor.view(tensor.shape[0], -1)\n",
    "    max_value,_ = torch.max(tensor, -1, keepdim=True)\n",
    "    min_value,_ = torch.min(tensor, -1, keepdim=True)\n",
    "    return ((tensor - min_value) / (max_value - min_value + 1e-5)).view(shape)\n",
    "\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, n_actions, hiddens=2048, mlp_layers=1, scale_width=4,\n",
    "                 n_atoms=51, Vmin=-10, Vmax=10):\n",
    "        super().__init__()\n",
    "        self.support = torch.linspace(Vmin, Vmax, n_atoms).cuda()\n",
    "        \n",
    "        self.hiddens=hiddens\n",
    "        self.scale_width=scale_width\n",
    "        self.act = nn.ReLU()\n",
    "        \n",
    "        \n",
    "        self.encoder_cnn = IMPALA_Resnet(scale_width=scale_width, norm=False, init=init_xavier, act=self.act)\n",
    "        \n",
    "        \n",
    "\n",
    "        # Single layer dense that maps the flattened encoded representation into hiddens.\n",
    "        self.projection = MLP(13824, med_hiddens=hiddens, out_hiddens=hiddens,\n",
    "                              last_init=init_xavier, layers=1)\n",
    "        self.prediction = MLP(hiddens, out_hiddens=hiddens, layers=1, last_init=init_xavier)\n",
    "                                              \n",
    "        self.transition = nn.Sequential(DQN_Conv(32*scale_width+n_actions, 32*scale_width, 3, 1, 1, norm=False, init=init_xavier, act=self.act),\n",
    "                                        DQN_Conv(32*scale_width, 32*scale_width, 3, 1, 1, norm=False, init=init_xavier, act=self.act))\n",
    "\n",
    "        self.reward_mlp = MLP(hiddens, out_hiddens=1, layers=1, last_init=init_xavier)\n",
    "\n",
    "        # Single layer dense that maps hiddens into the output dim according to:\n",
    "        # 1. https://arxiv.org/pdf/1707.06887.pdf -- Distributional Reinforcement Learning\n",
    "        # 2. https://arxiv.org/pdf/1511.06581.pdf -- Dueling DQN\n",
    "        self.a = MLP(hiddens, out_hiddens=n_actions*num_buckets, layers=1, in_act=self.act, last_init=init_xavier)\n",
    "        self.v = MLP(hiddens, out_hiddens=num_buckets, layers=1, in_act=self.act, last_init=init_xavier)\n",
    "    \n",
    "        params_count(self, 'DQN')\n",
    "    \n",
    "    def forward(self, X, y_action):\n",
    "        X, z = self.encode(X)\n",
    "        \n",
    "        \n",
    "        q, action = self.q_head(X)\n",
    "        z_pred = self.get_transition(z, y_action)\n",
    "\n",
    "        reward_pred = self.reward_mlp(z_pred)\n",
    "\n",
    "        return q, action, X[:,1:].clone().detach(), z_pred, reward_pred\n",
    "    \n",
    "    def get_root(self, X):\n",
    "        X, z = self.encode(X)\n",
    "        q, action = self.q_head(X)\n",
    "\n",
    "        return z, q.squeeze(-3)\n",
    "\n",
    "    def get_Q_last_state(self, z):\n",
    "        z = self.projection(z.flatten(-3,-1)).view(self.batch,-1)\n",
    "        z = self.prediction(z)\n",
    "\n",
    "        \n",
    "        q, action = self.q_head(z)\n",
    "\n",
    "        action = action[:,:,None,None].expand_as(q)[:,:,0][:,:,None]\n",
    "        q = q.gather(-2,action)\n",
    "        \n",
    "        return (q.squeeze()*self.support).sum(-1)\n",
    "    \n",
    "    def env_step(self, X):\n",
    "        with torch.no_grad():\n",
    "            X, _ = self.encode(X)\n",
    "            _, action = self.q_head(X)\n",
    "            \n",
    "            return action.detach()\n",
    "    \n",
    "\n",
    "    def encode(self, X):\n",
    "        batch, seq = X.shape[:2]\n",
    "        self.batch = batch\n",
    "        self.seq = seq\n",
    "        X = self.encoder_cnn(X.contiguous().view(self.batch*self.seq, *(X.shape[2:])))\n",
    "        X = renormalize(X).contiguous().view(self.batch, self.seq, *X.shape[-3:])\n",
    "        X = X.contiguous().view(self.batch, self.seq, *X.shape[-3:])\n",
    "        z = X.clone()\n",
    "        X = X.flatten(-3,-1)\n",
    "        X = self.projection(X)\n",
    "        return X, z\n",
    "\n",
    "    def transition_one_step(self, z, action):\n",
    "        self.batch = z.shape[0]\n",
    "        z = z.contiguous().view(-1, *z.shape[-3:])\n",
    "        \n",
    "        action = F.one_hot(action.clone(), n_actions).view(-1, n_actions)\n",
    "        action = action.view(-1, n_actions, 1, 1).expand(-1, n_actions, *z.shape[-2:])\n",
    "\n",
    "        #print(f\"transition_one_step {z.shape, action.shape}\")\n",
    "        z_pred = torch.cat( (z, action), 1)\n",
    "        z_pred = self.transition(z_pred)\n",
    "        z_transition = renormalize(z_pred)\n",
    "        \n",
    "        z_pred = self.projection(z_transition.flatten(-3,-1)).view(self.batch,-1)\n",
    "        z_pred = self.prediction(z_pred)\n",
    "\n",
    "        \n",
    "        q = self.dueling_dqn(z_pred).squeeze(-3)\n",
    "        action = (q*self.support).sum(-1).argmax(-1)\n",
    "        \n",
    "        reward_pred = self.reward_mlp(z_pred)\n",
    "        \n",
    "        return z_transition, q, reward_pred.squeeze(-1)\n",
    "        \n",
    "    def get_transition(self, z, action):\n",
    "        z = z.contiguous().view(-1, *z.shape[-3:])\n",
    "        \n",
    "        action = F.one_hot(action.clone(), n_actions).view(-1, n_actions)\n",
    "        action = action.view(-1, 5, n_actions, 1, 1).expand(-1, 5, n_actions, *z.shape[-2:])\n",
    "\n",
    "        z_pred = torch.cat( (z, action[:,0]), 1)\n",
    "        z_pred = self.transition(z_pred)\n",
    "        z_pred = renormalize(z_pred)\n",
    "        \n",
    "        z_preds=[z_pred.clone()]\n",
    "        \n",
    "\n",
    "        for k in range(4):\n",
    "            z_pred = torch.cat( (z_pred, action[:,k+1]), 1)\n",
    "            z_pred = self.transition(z_pred)\n",
    "            z_pred = renormalize(z_pred)\n",
    "            \n",
    "            z_preds.append(z_pred)\n",
    "        \n",
    "        \n",
    "        z_pred = torch.stack(z_preds,1)\n",
    "\n",
    "        z_pred = self.projection(z_pred.flatten(-3,-1)).view(self.batch,5,-1)\n",
    "        z_pred = self.prediction(z_pred)\n",
    "        \n",
    "        return z_pred\n",
    "\n",
    "    \n",
    "    def q_head(self, X):\n",
    "        q = self.dueling_dqn(X)\n",
    "        action = (q*self.support).sum(-1).argmax(-1)\n",
    "        \n",
    "        return q, action\n",
    "\n",
    "    def get_max_action(self, X):\n",
    "        with torch.no_grad():\n",
    "            X, _ = self.encode(X)\n",
    "            q = self.dueling_dqn(X)\n",
    "            \n",
    "            action = (q*self.support).sum(-1).argmax(-1)\n",
    "            return action\n",
    "\n",
    "    def evaluate(self, X, action):\n",
    "        with torch.no_grad():\n",
    "            X, _ = self.encode(X)\n",
    "            \n",
    "            q = self.dueling_dqn(X)\n",
    "            \n",
    "            action = action[:,:,None,None].expand_as(q)[:,:,0][:,:,None]\n",
    "            q = q.gather(-2,action)\n",
    "            \n",
    "            return q\n",
    "\n",
    "    def dueling_dqn(self, X):\n",
    "        X = F.relu(X)\n",
    "        \n",
    "        a = self.a(X).view(self.batch, -1, n_actions, num_buckets)\n",
    "        v = self.v(X).view(self.batch, -1, 1, num_buckets)\n",
    "        \n",
    "        q = v + a - a.mean(-2,keepdim=True)\n",
    "        q = F.softmax(q,-1)\n",
    "        \n",
    "        return q\n",
    "    \n",
    "    def network_ema(self, rand_network, target_network, alpha=0.5):\n",
    "        for param, param_target in zip(rand_network.parameters(), target_network.parameters()):\n",
    "            param_target.data = alpha * param_target.data + (1 - alpha) * param.data.clone()\n",
    "\n",
    "    def hard_reset(self, random_model, alpha=0.5):\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            self.network_ema(random_model.encoder_cnn, self.encoder_cnn, alpha)\n",
    "            self.network_ema(random_model.transition, self.transition, alpha)\n",
    "\n",
    "            self.network_ema(random_model.projection, self.projection, 0)\n",
    "            self.network_ema(random_model.prediction, self.prediction, 0)\n",
    "            self.network_ema(random_model.reward_mlp, self.reward_mlp, 0)\n",
    "\n",
    "            self.network_ema(random_model.a, self.a, 0)\n",
    "            self.network_ema(random_model.v, self.v, 0)\n",
    "\n",
    "\n",
    "num_buckets = 51\n",
    "n_actions = 7\n",
    "\n",
    "model = DQN(n_actions).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255d604f-55b2-457f-84ce-d365d43b44a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class MCTS_Node(nsd_Module):\n",
    "    def __init__(self, z, Q, reward, prev_state, n_actions, hiddens=2048, batch_size=3):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.transitions = [None]*self.n_actions\n",
    "        \n",
    "        self.n = torch.zeros(n_actions, device='cuda')\n",
    "        \n",
    "        self.p = F.softmax(Q,-1)\n",
    "        self.Q_sa = Q\n",
    "        self.Q = torch.zeros_like(Q)\n",
    "\n",
    "        self.choosen_action = torch.tensor(-1, device='cuda', dtype=torch.long)\n",
    "        \n",
    "        \n",
    "    def reset_n(self):\n",
    "        self.n = torch.zeros(self.n_actions, device='cuda')\n",
    "    \n",
    "    def get_stats(self):\n",
    "\n",
    "        return self.Q, self.Q_sa, self.z, self.p, self.n, self.reward, self.choosen_action\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class MCTS(nsd_Module):\n",
    "    def __init__(self, n_actions, k=5, batch_size=3, n_sim=16, topk_actions=8, c_visit=50, c_scale=0.1):\n",
    "        # Good c_scale values are 0.1 and 1\n",
    "        super().__init__()\n",
    "        self.topk_actions = torch.tensor(topk_actions)\n",
    "    \n",
    "\n",
    "    def get_root(self, model, x):\n",
    "        z, Q = model.get_root(x)\n",
    "        Q = (Q*model.support).sum(-1)\n",
    "        #print(f\"{z.shape, Q.shape}\")\n",
    "        nodes = []\n",
    "        for i in range(Q.shape[0]):\n",
    "            root = MCTS_Node(z[i], Q[i], torch.tensor([0]*self.batch_size).cuda()[i], prev_state=None, n_actions=self.n_actions, batch_size=self.batch_size)\n",
    "            nodes.append(root)\n",
    "        self.root = nodes\n",
    "        return self.root\n",
    "\n",
    "    def collate_nodes(self):\n",
    "        Q, Q_sa, Z, P, N, R, A = [], [], [], [], [], [], []\n",
    "        for node in self.cur_state:\n",
    "            q, q_sa, z, p, n, r, a = node.get_stats()\n",
    "            Q.append(q)\n",
    "            Q_sa.append(q_sa)\n",
    "            Z.append(z)\n",
    "            P.append(p)\n",
    "            N.append(n)\n",
    "            R.append(r)\n",
    "            A.append(a)\n",
    "        return torch.stack(Q,0), torch.stack(Q_sa,0), torch.stack(Z,0), torch.stack(P,0), torch.stack(N,0), torch.stack(R,0), torch.stack(A,0)\n",
    "    \n",
    "    def transition(self, model, x, action):\n",
    "        \n",
    "        z, Q, reward_pred = model.transition_one_step(x, action)\n",
    "        Q = (Q*model.support).sum(-1)\n",
    "        #print(f\"{Q.shape, reward_pred.shape}\")\n",
    "    \n",
    "        nodes = []\n",
    "        \n",
    "        for i in range(Q.shape[0]):\n",
    "            if self.cur_state[i].transitions[action[i]] == None:\n",
    "                node = MCTS_Node(z[i], Q[i], reward_pred[i], prev_state=self.cur_state[i], n_actions=self.n_actions, batch_size=self.batch_size)\n",
    "                nodes.append(node)\n",
    "                self.cur_state[i].transitions[action[i]] = node\n",
    "            else:\n",
    "                nodes.append(self.cur_state[i].transitions[action[i]])\n",
    "                \n",
    "\n",
    "        return nodes\n",
    "\n",
    "    \n",
    "    def backup(self, model):\n",
    "        \n",
    "        Q, Q_sa, z, p, n, r_t, choosen_action = self.collate_nodes()\n",
    "        \n",
    "        next_values = model.get_Q_last_state(z)#[:,None]\n",
    "\n",
    "        rewards = [r_t]\n",
    "        gammas = torch.ones(self.batch_size, self.k, device='cuda')*0.997\n",
    "        \n",
    "\n",
    "        for i in range(len(self.cur_state)):\n",
    "            self.cur_state[i] = self.cur_state[i].prev_state\n",
    "            \n",
    "\n",
    "        for l in range(self.k):\n",
    "            Q, Q_sa, z, p, n, r_t, choosen_action = self.collate_nodes()\n",
    "            \n",
    "            r = torch.stack(list(reversed(rewards)), -1)\n",
    "\n",
    "            returns = (r*gammas[:,:l+1].cumprod(-1)).sum(-1) + next_values*(gammas[:,:l+1].prod(-1))\n",
    "            #print(f\"backup returns {returns.shape}\")\n",
    "            #print(f\"n choosen action {n}\\n{choosen_action}\\n\")\n",
    "            \n",
    "            n_action = n[torch.arange(self.batch_size), choosen_action]\n",
    "            \n",
    "\n",
    "            Q[torch.arange(self.batch_size), choosen_action] = (n_action*Q[torch.arange(self.batch_size),choosen_action] + returns) / (n_action+1)\n",
    "            \n",
    "            n[torch.arange(self.batch_size), choosen_action] += 1\n",
    "\n",
    "            \n",
    "            \n",
    "            rewards.append(r_t)\n",
    "            \n",
    "            for i in range(len(self.cur_state)):\n",
    "                self.cur_state[i].Q = Q[i]\n",
    "                self.cur_state[i].n = n[i]\n",
    "                \n",
    "                self.cur_state[i] = self.cur_state[i].prev_state\n",
    "                \n",
    "        \n",
    "    def forward(self, model, x):\n",
    "\n",
    "        self.cur_state = self.get_root(model, x)\n",
    "\n",
    "        q_sa = self.collate_nodes()[1]\n",
    "        gumbel = F.gumbel_softmax(torch.zeros_like(q_sa))\n",
    "        gumbel_sa = gumbel+q_sa\n",
    "        print(f\"GUMBEL & Q_sa {gumbel_sa}\\n{q_sa}\\n\\n\")\n",
    "\n",
    "        k = min(self.topk_actions, self.n_actions)\n",
    "        \n",
    "        Q_mask = F.one_hot(gumbel_sa.topk(k)[1], self.n_actions).sum(-2)\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "        halve_sims = self.n_sim//2\n",
    "        halves = 2\n",
    "\n",
    "        next_halve = math.floor(self.n_sim/(torch.log2(self.topk_actions)*(self.topk_actions)))\n",
    "        \n",
    "        for sim in range(self.n_sim):\n",
    "            actions_to_step = []\n",
    "            for l in range(self.k):\n",
    "                q, q_sa, z, p, n, _, _ = self.collate_nodes()\n",
    "                \n",
    "                if l==0 and sim==next_halve:\n",
    "                    print(f\"gumbel q {gumbel_sa.shape, q.shape, n.shape}\")\n",
    "                    gumbel_sa = gumbel + q_sa\n",
    "                    k = min(self.topk_actions//halves*2, self.n_actions)\n",
    "                    print(f\"ACTION AMMOUNT IS HEREBYRAUBIESUYRBAESYUI {k}\")\n",
    "                    \n",
    "                    Q_mask = F.one_hot(gumbel_sa.topk(k)[1], self.n_actions).sum(-2)\n",
    "                    print(f\"TOPK K ACTIONS {gumbel_sa.topk(k)[1]}\")\n",
    "                    \n",
    "                    #Q_mask = gumbel_sa + (self.c_visit + n.max(-1)[0])*self.c_scale*q\n",
    "                    #Q_mask = F.one_hot(gumbel_sa.topk(self.topk_actions//halves)[1], self.n_actions).sum(-2)\n",
    "                    \n",
    "                    print(f\"HALVE SIM {Q_mask.shape, n.shape}\")\n",
    "\n",
    "                    next_halve += math.floor(self.n_sim/(torch.log2(self.topk_actions)*(self.topk_actions/(halves*2))))\n",
    "                    print(f\"next_halve {next_halve}\")\n",
    "                    \n",
    "                    halves*=2\n",
    "\n",
    "                if l==0:\n",
    "                    \n",
    "                    #print(f\"gumel n q {gumbel_sa.shape, n.max(-1)[0].shape, q.shape}\")\n",
    "                    Q = gumbel_sa + (self.c_visit + n.max(-1)[0][:,None])*self.c_scale*q\n",
    "                    print(f\"Q Mask {Q, Q_mask}\")\n",
    "                    #print(f\"{n}\")\n",
    "\n",
    "                    action = (Q*Q_mask).argmax(-1) # Gumbel top-k\n",
    "                    \n",
    "                else:\n",
    "                    completed_Q = (n==0)*q_sa + (n>0)*q\n",
    "                    \n",
    "                    completed_Q = (self.c_visit + n.max(-1)[0][:,None])*self.c_scale*completed_Q\n",
    "                    \n",
    "                    \n",
    "                    improved_policy = F.softmax((q_sa+completed_Q),-1)\n",
    "\n",
    "                    \n",
    "                    #print(f\"{improved_policy.shape, (n/(1+n.sum(-1)[:,None])).shape}\")\n",
    "                    Q = improved_policy - n/(1+n.sum(-1)[:,None])\n",
    "                    \n",
    "                    action = Q.argmax(-1)\n",
    "                    #print(f\"l and action: {l} {action}\")\n",
    "\n",
    "                \n",
    "                \n",
    "                \n",
    "                actions_to_step.append(Q.argmax(-1))\n",
    "                \n",
    "                \n",
    "                for i in range(len(self.cur_state)):\n",
    "                    self.cur_state[i].choosen_action = action[i]\n",
    "                    \n",
    "                self.cur_state = self.transition(model, z, action)\n",
    "                \n",
    "            self.backup(model)\n",
    "            self.cur_state = self.root\n",
    "            print(f\"\\n\\n\")\n",
    "        \n",
    "        Q = self.collate_nodes()[0]\n",
    "        return Q/self.n_sim, Q.argmax(-1), actions_to_step\n",
    "\n",
    "\n",
    "mcts = MCTS(n_actions=7, n_sim=16, topk_actions=8, batch_size=3)\n",
    "\n",
    "\n",
    "\n",
    "#x = torch.randn(3,128,12,9).cuda()\n",
    "#actions = torch.randint(0, n_actions, (x.shape[0],)).cuda()\n",
    "\n",
    "#q, next_state = model.transition_one_step(x, actions)\n",
    "\n",
    "x = torch.randn(3,1,12,96,72).cuda()\n",
    "\n",
    "#print(f\"{q.shape, next_state.shape}\")\n",
    "Q, act, _ = mcts(model, x)\n",
    "Q#.shape, act.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989e6d59-c449-4852-8886-3de5133113f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor([-1,0,1,2,3,4,5]).max(-1)[0], torch.tensor([-1,0,1,2,3,4,5]).topk(3)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea3f2e3-d421-4d91-b56e-101a62ac25dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "math.ceil(7/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151f6e40-fb4b-4871-9a5c-6f565e250893",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor([-1,2]).max(-1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6559b1e5-39c4-4336-baaf-4b69b2f240b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "0.995**(4/1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bd6adc-aa12-4b33-8d4b-999bf31cfea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(3)\n",
    "a, F.gumbel_softmax(torch.zeros_like(a))+a, F.softmax(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e919cefd-c3a2-46e3-b352-22dc28641d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def a(p):\n",
    "    return p.pop(0)\n",
    "\n",
    "c = [1,2,3]\n",
    "\n",
    "a(c), c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265cb70a-c286-4b88-87f3-ac14d386603e",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_envs=1\n",
    "model.env_step(x).view(num_envs).squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b11ed3-3333-43c6-b618-163cc3b5a516",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ec618b-d86b-4b22-a100-71a8cde87338",
   "metadata": {},
   "outputs": [],
   "source": [
    "def project_distribution(supports, weights, target_support):\n",
    "    with torch.no_grad():\n",
    "        v_min, v_max = target_support[0], target_support[-1]\n",
    "        # `N` in Eq7.\n",
    "        num_dims = target_support.shape[-1]\n",
    "        # delta_z = `\\Delta z` in Eq7.\n",
    "        delta_z = (v_max - v_min) / (num_buckets-1)\n",
    "        # clipped_support = `[\\hat{T}_{z_j}]^{V_max}_{V_min}` in Eq7.\n",
    "        clipped_support = supports.clip(v_min, v_max)\n",
    "        # numerator = `|clipped_support - z_i|` in Eq7.\n",
    "        numerator = (clipped_support[:,None] - target_support[None,:,None].repeat_interleave(clipped_support.shape[0],0)).abs()\n",
    "        quotient = 1 - (numerator / delta_z)\n",
    "        # clipped_quotient = `[1 - numerator / (\\Delta z)]_0^1` in Eq7.\n",
    "        clipped_quotient = quotient.clip(0, 1)\n",
    "        # inner_prod = `\\sum_{j=0}^{N-1} clipped_quotient * p_j(x', \\pi(x'))` in Eq7.\n",
    "        inner_prod = (clipped_quotient * weights[:,None]).sum(-1)\n",
    "        #inner_prod = (clipped_quotient).sum(-1) * weights\n",
    "        return inner_prod.squeeze()\n",
    "\n",
    "\n",
    "support = torch.linspace(-10, 10, 8).cuda()\n",
    "w = torch.ones(1,8).cuda()\n",
    "#w[0,6]=1\n",
    "weights = F.softmax(w,-1)\n",
    "\n",
    "tgt_sup = 0.6+(0.997**10)*support\n",
    "\n",
    "tgt_sup, project_distribution(tgt_sup, weights, support)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333f0dc7-59f4-428a-91ac-d62ed9e60df5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e888185e-e629-40b3-936e-b47cf99455f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150e3f2c-372a-40e5-8803-e88d11306508",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60080665-e9e7-4b05-8948-27911c47795b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "initial_n = 4\n",
    "final_n = 1\n",
    "schedule_max_step = 10000\n",
    "\n",
    "v=[]\n",
    "for grad_step in range(40000):\n",
    "    n = initial_n * (final_n/initial_n)**(min(torch.tensor(grad_step-10000).clip(0),schedule_max_step) / schedule_max_step)\n",
    "    v.append(n)\n",
    "\n",
    "plt.plot(v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828e5304-fe03-4de4-bda4-bb803c582ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from nosaveddata import *\n",
    "\n",
    "class Quantizer1d(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_embeddings=256,\n",
    "                 dim=512\n",
    "                 ):\n",
    "        super(Quantizer1d, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings, dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        \n",
    "        dist = torch.cdist(x, self.embedding.weight[None, :].repeat((x.size(0), 1, 1)))\n",
    "        min_encoding_indices = torch.argmin(dist, dim=-1)\n",
    "        \n",
    "        quant_out = torch.index_select(self.embedding.weight, 0, min_encoding_indices.view(-1)).view(B,T,C)\n",
    "        print(f\"{min_encoding_indices.shape, min_encoding_indices}\")\n",
    "        \n",
    "        commmitment_loss = ((quant_out.detach() - x) ** 2).mean((1,2))\n",
    "        codebook_loss = ((quant_out - x.detach()) ** 2).mean((1,2))\n",
    "        quantize_losses = {\n",
    "            'codebook_loss' : codebook_loss,\n",
    "            'commitment_loss' : commmitment_loss\n",
    "        }\n",
    "        quant_out = x + (quant_out - x).detach()\n",
    "        min_encoding_indices = min_encoding_indices.contiguous().view((B,-1))\n",
    "        return quant_out, quantize_losses, min_encoding_indices\n",
    "\n",
    "    def forward_idx(self, x, idx):\n",
    "        B, T, C = x.shape\n",
    "        \n",
    "        \n",
    "        quant_out = torch.index_select(self.embedding.weight, 0, idx.view(-1)).view(B,T,C)\n",
    "        \n",
    "        print(f\"{x.shape}\")\n",
    "        commmitment_loss = ((quant_out.detach() - x) ** 2).mean((1,2))\n",
    "        codebook_loss = ((quant_out - x.detach()) ** 2).mean((1,2))\n",
    "        quantize_losses = {\n",
    "            'codebook_loss' : codebook_loss,\n",
    "            'commitment_loss' : commmitment_loss\n",
    "        }\n",
    "        quant_out = x + (quant_out - x).detach()\n",
    "        return quant_out, quantize_losses\n",
    "\n",
    "quant = Quantizer1d(256,512)\n",
    "\n",
    "x = torch.randn(32,1,512)\n",
    "idx = torch.randint(0,256,(32,))[:,None]\n",
    "\n",
    "a, b, c  = quant(x)\n",
    "\n",
    "x, loss = quant.forward_idx(x, idx)\n",
    "\n",
    "loss['codebook_loss'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720d21a2-abe3-4f2d-a17d-6614bff47a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from nosaveddata import *\n",
    "\n",
    "seed_np_torch(42)\n",
    "\n",
    "a = torch.randn(38,2048)\n",
    "\n",
    "b = nn.Embedding(38,2048)\n",
    "\n",
    "state_dict = {'weight': a}\n",
    "\n",
    "b.load_state_dict(state_dict)\n",
    "\n",
    "c = torch.randint(0,38,(4,))\n",
    "\n",
    "b(c), c, a[26]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38d8d4b-dcd4-4a47-8cbe-2757621a804e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor([[2]]).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb33f37-c373-48e0-8871-ebb1cbfb5ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "1000 in range(999,1001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac55d1b8-7a6f-4d01-a4bb-1a66ee33b4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "a=torch.arange(10)\n",
    "\n",
    "a.topk(3,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6767425-f06c-4aed-8ddf-f744b36658fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9c5bf5-a80a-4229-a7a5-5f5e70b65fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Assuming you have 5 vectors stored in a tensor called 'vectors'\n",
    "#vectors = torch.randn(5, 10)  # Assuming each vector has size 10\n",
    "\n",
    "\n",
    "# Compute pairwise distances between all vectors\n",
    "\n",
    "def get_highest_l1_vectors(vectors, ammount=300):\n",
    "    distances = torch.cdist(vectors, vectors)\n",
    "    \n",
    "    # Initialize a list to store the sampled indices\n",
    "    sampled_indices = []\n",
    "    \n",
    "    # Start by randomly selecting the first index\n",
    "    sampled_indices.append(torch.randint(0, vectors.size(0), (1,)).item())\n",
    "    \n",
    "    # Repeat until you have sampled 3 vectors\n",
    "    while len(sampled_indices) < ammount:\n",
    "        # Compute the distances from the already sampled vectors to all others\n",
    "        sampled_distances = distances[sampled_indices, :].min(dim=0).values\n",
    "        \n",
    "        # Select the index with the maximum distance as the next sampled index\n",
    "        next_index = sampled_distances.argmax().item()\n",
    "        # Add the index to the list of sampled indices\n",
    "        sampled_indices.append(next_index)\n",
    "    \n",
    "    # Extract the sampled vectors\n",
    "    return vectors[sampled_indices], sampled_indices\n",
    "\n",
    "vectors = torch.randn(300, 2048)\n",
    "print(f\"{vectors.shape}\")\n",
    "\n",
    "sampled_vectors, sampled_indices = get_highest_l1_vectors(vectors)\n",
    "\n",
    "print(\"Sampled Vectors:\")\n",
    "print(f\"{sampled_indices}\")\n",
    "print(sampled_vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ce085b-8569-4b18-9385-e2318888306f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d80acf-1a3e-4b64-ab3b-549f0147967e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import DPMSolverMultistepScheduler \n",
    "noise_scheduler = DPMSolverMultistepScheduler(num_train_timesteps=1000, beta_start=0.00085, beta_end=0.012, use_karras_sigmas=False, solver_order=2)\n",
    "\n",
    "idx=10\n",
    "noise_scheduler.alpha_t[idx], (1-noise_scheduler.alpha_t[idx]).sqrt(), (1-noise_scheduler.alphas_cumprod[idx]).sqrt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b056f0f-3a26-4b68-a041-9ef4abef9bf1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abd5d6d-7413-4d28-a5dd-ca79729fc05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from nosaveddata import *\n",
    "\n",
    "\n",
    "def get_patches(x, patch_shape):\n",
    "    c, (h, w) = x.shape[1], patch_shape\n",
    "    \n",
    "    return x.unfold(2,h,1).unfold(3,w,1).transpose(1,3).reshape(-1,c,h,w).float()\n",
    "\n",
    "def get_whitening_parameters(patches):\n",
    "    n,c,h,w = patches.shape\n",
    "    patches_flat = patches.view(n, -1)\n",
    "    est_patch_covariance = (patches_flat.T @ patches_flat) / n\n",
    "    \n",
    "    eigenvalues, eigenvectors = torch.linalg.eigh(est_patch_covariance, UPLO='U')\n",
    "    \n",
    "    return eigenvalues.flip(0).view(-1, 1, 1, 1), eigenvectors.T.reshape(c*h*w,c,h,w).flip(0)\n",
    "\n",
    "def init_whitening_conv(layer, train_set, eps=5e-4):\n",
    "    patches = get_patches(train_set, patch_shape=layer.weight.data.shape[2:])\n",
    "    \n",
    "    eigenvalues, eigenvectors = get_whitening_parameters(patches)\n",
    "    \n",
    "    eigenvectors_scaled = eigenvectors / torch.sqrt(eigenvalues + eps)\n",
    "    layer.weight.data[:] = torch.cat((eigenvectors_scaled, -eigenvectors_scaled))\n",
    "    layer.weight.requires_grad=False\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class IMPALA_Resnet_Whitened(nsd_Module):\n",
    "    def __init__(self, first_channels=12, scale_width=1, norm=True, init=init_partial_dirac, act=nn.SiLU()):\n",
    "        super().__init__()\n",
    "        # lhs 2 is because we use concatenate positive and negative eigenvectors, 3 is the kernel size\n",
    "        self.whitened_channels = 2 * first_channels * 3**2\n",
    "        \n",
    "        self.cnn = nn.Sequential(self.whitened_block(first_channels, 16*scale_width),\n",
    "                                 self.get_block(16*scale_width, 32*scale_width),\n",
    "                                 self.get_block(32*scale_width, 32*scale_width, last_relu=True))\n",
    "        \n",
    "        self.cnn[0][1].apply(init)\n",
    "        params_count(self, 'IMPALA ResNet')\n",
    "\n",
    "    def whitened_block(self, in_hiddens, out_hiddens, last_relu=False):\n",
    "        \n",
    "        blocks = nn.Sequential(DQN_Conv(in_hiddens, self.whitened_channels, 3, 1, 1, max_pool=True, act=self.act, norm=self.norm, init=self.init),\n",
    "                               nn.Conv2d(self.whitened_channels,out_hiddens, 1, padding=0, stride=1),\n",
    "                               Residual_Block(out_hiddens, out_hiddens, norm=self.norm, act=self.act, init=self.init),\n",
    "                               Residual_Block(out_hiddens, out_hiddens, norm=self.norm, act=self.act, init=self.init, out_act=self.act if last_relu else nn.Identity())\n",
    "                              )\n",
    "        \n",
    "        return blocks\n",
    "    \n",
    "    def get_block(self, in_hiddens, out_hiddens, last_relu=False):\n",
    "        \n",
    "        blocks = nn.Sequential(DQN_Conv(in_hiddens, out_hiddens, 3, 1, 1, max_pool=True, act=self.act, norm=self.norm, init=self.init),\n",
    "                               Residual_Block(out_hiddens, out_hiddens, norm=self.norm, act=self.act, init=self.init),\n",
    "                               Residual_Block(out_hiddens, out_hiddens, norm=self.norm, act=self.act, init=self.init, out_act=self.act if last_relu else nn.Identity())\n",
    "                              )\n",
    "        \n",
    "        return blocks\n",
    "        \n",
    "    def forward(self, X):\n",
    "        return self.cnn(X)\n",
    "\n",
    "\n",
    "x=torch.randn(500,12,96,72)\n",
    "IMPALA_Resnet(12,scale_width=4)\n",
    "network = IMPALA_Resnet_Whitened(12,scale_width=4)\n",
    "\n",
    "init_whitening_conv(network.cnn[0][0].conv[0], x)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c01737-7852-40ff-881c-48d83c11a73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps=40000\n",
    "k=5\n",
    "sched = 0.95**k * (torch.arange(steps+1) / steps)**3\n",
    "\n",
    "sched[::500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babf7648-ba1b-4193-9968-bd7227f706c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LookaheadState:\n",
    "    def __init__(self, net, steps, k=5):\n",
    "        self.k=k\n",
    "        self.net_ema = {k: v.clone() for k, v in net.state_dict().items()}\n",
    "        self.sched = 0.95**k * (torch.arange(steps+1) / steps)**3\n",
    "\n",
    "    def update(self, net, step):\n",
    "        decay = self.sched[step].item()\n",
    "        if step%self.k==0:\n",
    "            for ema_param, net_param in zip(self.net_ema.values(), net.state_dict().values()):\n",
    "                ema_param.lerp_(net_param, 1-decay)\n",
    "                net_param.copy_(ema_param)\n",
    "                \n",
    "    def update_fixed_decay(self, net, decay, step):\n",
    "        if step%self.k==0:\n",
    "            for ema_param, net_param in zip(self.net_ema.values(), net.state_dict().values()):\n",
    "                ema_param.lerp_(net_param, 1-decay)\n",
    "                net_param.copy_(ema_param)\n",
    "\n",
    "lookahead_state = LookaheadState(network, 40000)\n",
    "\n",
    "lookahead_state.update(network, 8)\n",
    "\n",
    "def Triangle_Scheduler(optimizer, steps, start=0.2, end=0.07, peak=0.23):\n",
    "    def triangle(steps, start, end, peak):\n",
    "        xp = torch.tensor([0, int(peak * steps), steps])\n",
    "        fp = torch.tensor([start, 1, end])\n",
    "        x = torch.arange(1+steps)\n",
    "        m = (fp[1:] - fp[:-1]) / (xp[1:] - xp[:-1])\n",
    "        b = fp[:-1] - (m * xp[:-1])\n",
    "        indices = torch.sum(torch.ge(x[:, None], xp[None, :]), 1) - 1\n",
    "        indices = torch.clamp(indices, 0, len(m) - 1)\n",
    "        return m[indices] * x + b[indices]\n",
    "    lr_schedule = triangle(steps, start, end, peak)\n",
    "    return torch.optim.lr_scheduler.LambdaLR(optimizer, lambda i: lr_schedule[i])\n",
    "\n",
    "momentum=0.9\n",
    "lr = 0.1 / (1+1/(1-momentum))\n",
    "print(f\"{lr}\")\n",
    "\n",
    "optim = torch.optim.SGD(network.parameters(), lr=lr, weight_decay=0.1, momentum=momentum, nesterov=True)\n",
    "\n",
    "sched = Triangle_Scheduler(optim, 40000)\n",
    "\n",
    "print(f\"{optim.param_groups[0]['lr']}\")\n",
    "for i in range(int(40000*0.23)):\n",
    "    sched.step()\n",
    "print(f\"{optim.param_groups[0]['lr']}\")\n",
    "\n",
    "total_train_steps=40000\n",
    "\n",
    "\n",
    "lookahead_state.update(network, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e6cdee-1248-4eb9-9181-36f58ec87c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 11.5/1024# * (1 + 1 / (1 - 0.85))\n",
    "\n",
    "lr, 0.0153/(1 + 1 / (1 - 0.85))*lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ed1821-d35e-48f5-b41f-a0d4c6eb7e8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9d0b46-19f3-4a8c-a404-bcedaf8e30b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv(nn.Conv2d):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, padding='same', bias=False):\n",
    "        super().__init__(in_channels, out_channels, kernel_size=kernel_size, padding=padding, bias=bias)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        super().reset_parameters()\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.zero_()\n",
    "        w = self.weight.data\n",
    "        torch.nn.init.dirac_(w[:w.size(1)])\n",
    "\n",
    "def init_partial_dirac(module):\n",
    "    if type(module) in (nn.Linear, nn.Conv2d, nn.Conv1d, nn.Conv3d):\n",
    "        w = module.weight.data\n",
    "        \n",
    "        nn.init.dirac_(module.weight[:w.shape[1]])\n",
    "        nn.init.xavier_uniform_(module.weight[w.shape[1]:], gain=1)\n",
    "\n",
    "        if module.bias is not None:\n",
    "            nn.init.zeros_(module.bias)\n",
    "            \n",
    "\n",
    "model = Conv(16,32)\n",
    "model.apply(init_partial_dirac)\n",
    "last_relu=False\n",
    "act=nn.ReLU()\n",
    "\n",
    "in_hiddens=16\n",
    "out_hiddens=32\n",
    "\n",
    "norm = False\n",
    "model = nn.Sequential(DQN_Conv(in_hiddens, out_hiddens, 3, 1, 1, max_pool=True, act=act, norm=norm, init=init_partial_dirac),\n",
    "                               Residual_Block(out_hiddens, out_hiddens, norm=norm, act=act, init=init_partial_dirac),\n",
    "                               Residual_Block(out_hiddens, out_hiddens, norm=norm, act=act, init=init_partial_dirac, out_act=act if last_relu else nn.Identity()),\n",
    "                               MLP(512,512)\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793d1ae1-ad48-46d0-9240-477a3b3a9634",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from unidecode import unidecode\n",
    "from phonemizer import phonemize\n",
    "from phonemizer.backend import EspeakBackend\n",
    "backend = EspeakBackend('en-us', preserve_punctuation=True, with_stress=True)\n",
    "\n",
    "def english_cleaners2(text):\n",
    "  '''Pipeline for English text, including abbreviation expansion. + punctuation + stress'''\n",
    "  text = unidecode(text)\n",
    "  print(f\"{text}\")\n",
    "  text = text.lower()\n",
    "    \n",
    "  print(f\"{text}\")\n",
    "  phonemes = backend.phonemize([text], strip=True)[0]\n",
    "  phonemes = collapse_whitespace(phonemes)\n",
    "  return phonemes\n",
    "\n",
    "english_cleaners2(\"Ol doutor\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2a41eb-227b-43f1-a3fe-4938beb7667b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(10,30)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "model = Model()\n",
    "model = torch.compile(model)\n",
    "\n",
    "x=torch.randn(1,10)\n",
    "model(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb20ac00-c5b6-4a8c-81a4-7bfaff184ec2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d80d672-4bfe-42fb-88be-5e3fd3c0d36f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc37ab2-106c-40fa-92ba-eb7fbc318e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import epitran\n",
    "\n",
    "# Carregar o modelo em portugus\n",
    "nlp = spacy.load('pt_core_news_sm')\n",
    "\n",
    "# Texto de exemplo\n",
    "texto = \"O cachorro correu pelo parque.\"\n",
    "\n",
    "#texto = \"Exemplo de texto para fonemizao.\"\n",
    "\n",
    "doc = nlp(texto)\n",
    "for token in doc:\n",
    "    try:\n",
    "        fonemas = fonemizar_texto(token.text)\n",
    "        print(f'{token.text} -> {fonemas}')\n",
    "    except Exception as e:\n",
    "        pass\n",
    "\n",
    "\n",
    "fonemizar_texto(tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293c8a2d-db7d-48e3-ae9d-19cfc3391aa2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608559a3-677c-431e-8a9a-8fb8461a71a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2825037a-8e22-4865-a828-bd33bd2f6c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nosaveddata import *\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self, len_state, num_quant, num_actions):\n",
    "        nn.Module.__init__(self)\n",
    "       \n",
    "        self.num_quant = num_quant\n",
    "        self.num_actions = num_actions\n",
    "       \n",
    "        self.layer1 = nn.Linear(len_state, 256)\n",
    "        self.layer2 = nn.Linear(256, num_actions*num_quant)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = torch.tanh(x)\n",
    "        x = self.layer2(x)\n",
    "        return x.view(-1, self.num_actions, self.num_quant)\n",
    "   \n",
    "    def select_action(self, state, eps):\n",
    "        if not isinstance(state, torch.Tensor):\n",
    "            state = torch.Tensor([state])    \n",
    "        action = torch.randint(0, 2, (1,))\n",
    "        if random.random() > eps:\n",
    "            action = self.forward(state).mean(2).max(1)[1]\n",
    "        return int(action)\n",
    "   \n",
    "\n",
    "eps_start, eps_end, eps_dec = 0.9, 0.1, 500\n",
    "eps = lambda steps: eps_end + (eps_start - eps_end) * np.exp(-1. * steps / eps_dec)\n",
    "\n",
    "Z = Network(len_state=8, num_quant=2, num_actions=7)\n",
    "Ztgt = Network(len_state=8, num_quant=2, num_actions=7)\n",
    "Ztgt.load_state_dict(Z.state_dict())\n",
    "tau = torch.Tensor((2 * np.arange(Z.num_quant) + 1) / (2.0 * Z.num_quant)).view(1, -1)\n",
    "\n",
    "batch_size=3\n",
    "\n",
    "def huber(x, k=1.0):\n",
    "    return torch.where(x.abs() < k, 0.5 * x.pow(2), k * (x.abs() - 0.5 * k))\n",
    "\n",
    "next_states = torch.randn(batch_size,8)\n",
    "states = next_states + torch.randn(batch_size,8)*0.01\n",
    "\n",
    "gamma=0.997\n",
    "rewards=torch.ones(batch_size,1)\n",
    "\n",
    "\n",
    "theta = Z(states)\n",
    "print(f\"{theta.shape, theta.mean(-1).argmax(-1)}\")\n",
    "theta = theta[np.arange(batch_size), theta.mean(2).max(1)[1]]\n",
    "\n",
    "\n",
    "Znext = Ztgt(next_states).detach()\n",
    "Znext_max = Znext[np.arange(batch_size), Znext.mean(2).max(1)[1]]\n",
    "\n",
    "print(f\"{Znext.mean(2).max(1)[1]}\")\n",
    "\n",
    "Ttheta = rewards + gamma  * Znext_max\n",
    "\n",
    "print(f\"{Ttheta.t()[..., None].shape, theta.shape}\")\n",
    "\n",
    "diff = Ttheta.t()[..., None] - theta\n",
    "\n",
    "loss = huber(diff) * (tau - (diff.detach() < 0).float()).abs()\n",
    "\n",
    "\n",
    "loss, diff, Ttheta, theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f11d706-7d17-40e9-80f3-97f58142ea74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf15e4c3-6015-41d0-b314-1dca5148c5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nosaveddata import *\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "a=torch.arange(2,device='cuda').long()[:,None].repeat_interleave(15,0)\n",
    "\n",
    "a,torch.zeros(6,1,device='cuda').long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac57f6d-e45c-43c9-a29f-e94d81fc69b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "\n",
    "p1 = 0.6697\n",
    "p2 = 0.6649\n",
    "n = 10000\n",
    "\n",
    "def statistical_difference(p1, p2, n):\n",
    "    \n",
    "    d=torch.tensor(p1-p2).abs()\n",
    "\n",
    "    std = 1.65 * math.sqrt((p1*(1-p1) + p2*(1-p2))/n)\n",
    "    \n",
    "    difference = torch.tensor([d-std, d+std])\n",
    "    \n",
    "    return difference.sort()[0]\n",
    "\n",
    "print(statistical_difference(0.834, 0.831, 100000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8e20d0-e2d2-4ce7-9c42-12aea4125e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nosaveddata import *\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class IMPALA_Resnet(nn.Module):\n",
    "    def __init__(self, first_channels=12, scale_width=1, norm=True, init=init_relu, act=nn.SiLU()):\n",
    "        super().__init__()\n",
    "        self.norm=norm\n",
    "        self.init=init\n",
    "        self.act =act\n",
    "        \n",
    "        self.cnn = nn.Sequential(self.get_block(first_channels, 16*scale_width),\n",
    "                                 self.get_block(16*scale_width, 32*scale_width),\n",
    "                                 self.get_block(32*scale_width, 32*scale_width, last_relu=True))\n",
    "        params_count(self, 'IMPALA ResNet')\n",
    "    def get_block(self, in_hiddens, out_hiddens, last_relu=False):\n",
    "        \n",
    "        blocks = nn.Sequential(DQN_Conv(in_hiddens, out_hiddens, 3, 1, 1, max_pool=True, act=self.act, norm=self.norm, init=self.init),\n",
    "                               Residual_Block(out_hiddens, out_hiddens, norm=self.norm, act=self.act, init=self.init),\n",
    "                               Residual_Block(out_hiddens, out_hiddens, norm=self.norm, act=self.act, init=self.init, out_act=self.act if last_relu else nn.Identity())\n",
    "                              )\n",
    "        \n",
    "        return blocks\n",
    "        \n",
    "    def forward(self, X):\n",
    "        return self.cnn(X)\n",
    "\n",
    "\n",
    "class IMPALA_YY(nn.Module):\n",
    "    def __init__(self, first_channels=12, scale_width=1, norm=True, init=init_relu, act=nn.SiLU()):\n",
    "        super().__init__()\n",
    "        self.norm=norm\n",
    "        self.init=init\n",
    "        self.act =act\n",
    "\n",
    "        self.yin = self.get_yin(first_channels, 16*scale_width, 32*scale_width)\n",
    "        \n",
    "        self.yang = self.get_yang(first_channels, 16*scale_width)\n",
    "                                 \n",
    "        self.head = nn.Sequential(self.get_yang(16*scale_width, 32*scale_width),\n",
    "                                  self.get_yang(32*scale_width, 32*scale_width, last_relu=True))\n",
    "        \n",
    "        params_count(self, 'IMPALA ResNet')\n",
    "\n",
    "    def get_yin(self, in_hiddens, hiddens, out_hiddens):\n",
    "        blocks = nn.Sequential(DQN_Conv(1, hiddens, 3, 1, 1, max_pool=True, act=self.act, norm=self.norm, init=self.init),\n",
    "                               Residual_Block(hiddens, hiddens, norm=self.norm, act=self.act, init=self.init),\n",
    "                               #DQN_Conv(hiddens, out_hiddens, 3, 1, 1, max_pool=True, act=self.act, norm=self.norm, init=self.init),\n",
    "                               #Residual_Block(out_hiddens, out_hiddens, norm=self.norm, act=self.act, init=self.init),\n",
    "                               #Residual_Block(out_hiddens, out_hiddens, norm=self.norm, act=self.act, init=self.init)\n",
    "                              )\n",
    "        return blocks          \n",
    "        \n",
    "    def get_yang(self, in_hiddens, out_hiddens, last_relu=False):\n",
    "        \n",
    "        blocks = nn.Sequential(DQN_Conv(in_hiddens, out_hiddens, 3, 1, 1, max_pool=True, act=self.act, norm=self.norm, init=self.init),\n",
    "                               Residual_Block(out_hiddens, out_hiddens, norm=self.norm, act=self.act, init=self.init),\n",
    "                               Residual_Block(out_hiddens, out_hiddens, norm=self.norm, act=self.act, init=self.init, out_act=self.act if last_relu else nn.Identity())\n",
    "                              )\n",
    "        \n",
    "        return blocks\n",
    "    \n",
    "    def forward(self, X):\n",
    "\n",
    "        y = self.yin(X[:,-3:].mean(-3)[:,None])\n",
    "        x = self.yang(X)\n",
    "        \n",
    "        X = x*(1-y) + x + y\n",
    "        \n",
    "        return self.head(X)\n",
    "\n",
    "model = IMPALA_Resnet(scale_width=4)\n",
    "x=torch.randn(32,12,96,72)\n",
    "model2 = IMPALA_YY(scale_width=4)\n",
    "\n",
    "model(x).shape, model2(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dcc521d-7bac-44e8-9584-ce0bab652aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from nosaveddata import *\n",
    "\n",
    "seed_np_torch(42)\n",
    "\n",
    "def network_ema(target_network, new_network, alpha=0.5):\n",
    "    for (param_name, param_target), param_new  in zip(target_network.cuda().named_parameters(), new_network.parameters()):\n",
    "        if 'ln' in param_name: #layer norm\n",
    "            param_target.data = param_new.data.clone()\n",
    "        else:\n",
    "            param_target.data = alpha * param_target.data + (1 - alpha) * param_new.data.clone()\n",
    "\n",
    "\n",
    "class Modeld(nsd_Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear = nn.Linear(10,32)\n",
    "        self.ln = nn.LayerNorm(32)\n",
    "\n",
    "    def forward(self,X):\n",
    "        return self.ln(self.linear(X))\n",
    "\n",
    "m = Modeld().cuda()\n",
    "m_rand= Modeld().cuda()\n",
    "\n",
    "\n",
    "optim=torch.optim.AdamW(m.parameters(), lr=1e-4)\n",
    "\n",
    "for i in range(4000):\n",
    "    x=torch.randn(1,10).cuda()\n",
    "    \n",
    "    loss = m(x).sum()\n",
    "    loss.backward()\n",
    "    \n",
    "    optim.step()\n",
    "    optim.zero_grad()\n",
    "\n",
    "network_ema(m,m_rand)\n",
    "\n",
    "m.ln.weight, m.linear.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3912798-5ac5-43f3-b591-98c21b5f71e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nosaveddata import *\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "model = nn.Linear(10,2).cuda()\n",
    "model.apply(init_xavier)\n",
    "model2 = nn.Linear(10,2).cuda()\n",
    "network_ema(model, model2, 0)\n",
    "model.apply(init_xavier)\n",
    "\n",
    "model.weight.data==model2.weight.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe94662c-07ea-4abb-b390-a28cf08d5a81",
   "metadata": {},
   "source": [
    "<h1>Preprocessing</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3a89a6-7f64-4075-8138-50b0e8229dd8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import os, glob\n",
    "from nosaveddata import *\n",
    "\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "paths = glob.glob('C:/Users/Augusto/Python/PyTorch/RL/mc_data/4/2023_01_09_14_48_09_100636/*.jpg')\n",
    "path = 'C:/Users/Augusto/Python/PyTorch/RL/mc_data/4/2023_01_09_14_48_09_100636/7,0,0,0,0,0,0,0,0,0,0,0,0,3,0,.jpg'\n",
    "\n",
    "\n",
    "\n",
    "tfms = transforms.Compose([\n",
    "                           transforms.Resize((96, 72)),\n",
    "                           transforms.ToTensor()\n",
    "                        ])\n",
    "\n",
    "img = Image.open(path)\n",
    "imgs=[]\n",
    "for p in paths:\n",
    "    imgs.append(tfms(Image.open(p)))\n",
    "imgs=torch.stack(imgs)\n",
    "\n",
    "print(imgs.shape)\n",
    "\n",
    "\n",
    "\n",
    "imgs, augments_applied = preprocess_iwm_no_solarize(imgs)\n",
    "    \n",
    "\n",
    "\n",
    "#plt.imshow(img_tfms)\n",
    "plot_imgs(imgs.permute(0,2,3,1))\n",
    "augments_applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a6f04e-3fe2-4c94-8984-2e74348e007b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from nosaveddata import *\n",
    "\n",
    "\n",
    "\n",
    "def gray_scale_stacked(X, p=0.2, stacks=4):\n",
    "    # Input: Tensor T e (B,C,T,D)\n",
    "    \n",
    "    probs = get_img_preprocessing_prob(X.shape[0], p, X.device)\n",
    "    stacked_probs = probs.repeat_interleave(stacks,0)\n",
    "    X = X.view(-1,X.shape[1]//stacks,*X.shape[-2:])\n",
    "    \n",
    "    gray_img = X.mean(1,keepdim=True).expand(-1,3,-1,-1)\n",
    "    \n",
    "    X = (1-stacked_probs)*X + stacked_probs*gray_img\n",
    "    \n",
    "    return X.view(X.shape[0]//stacks, -1, *X.shape[-2:]), probs.squeeze()\n",
    "\n",
    "def gaussian_blur(X, p=0.2, stacks=4, sigma_min=0.1, sigma_max=2):\n",
    "    # Input: Tensor T e (B,C,T,D)\n",
    "    \n",
    "    probs = get_img_preprocessing_prob(X.shape[0], p, X.device)\n",
    "    tfms = transforms.GaussianBlur(3, (sigma_min, sigma_max))\n",
    "    \n",
    "    blurred = tfms(X)\n",
    "    X = (1-probs)*X + probs*blurred\n",
    "    \n",
    "    return X, probs.squeeze()\n",
    "\n",
    "def solarization_stacked(X, p=0.2, stacks=4):\n",
    "    # Input: Tensor T e (B,C,T,D)\n",
    "\n",
    "    probs = get_img_preprocessing_prob(X.shape[0], p, X.device)\n",
    "    stacked_probs = probs.repeat_interleave(stacks,0)\n",
    "    \n",
    "    X = X.view(-1,X.shape[1]//stacks,*X.shape[-2:])\n",
    "    \n",
    "    tfms = transforms.RandomSolarize(0,p=1) # This prob is applied over all the batch or no image at all\n",
    "    \n",
    "    solarized = tfms(X)\n",
    "    X = (1-stacked_probs)*X + stacked_probs*solarized\n",
    "    \n",
    "    return X.view(X.shape[0]//stacks, -1, *X.shape[-2:]), probs.squeeze()\n",
    "\n",
    "\n",
    "def preprocess_iwm_stacked(imgs, p=0.2, stacks=4):\n",
    "    # Applies the same preprocessing for all images in the sequence, but separated by each beach\n",
    "    augments_applied=[]\n",
    "    \n",
    "    imgs, augmented = gray_scale_stacked(imgs, p, stacks)\n",
    "    augments_applied.append(augmented)\n",
    "    \n",
    "    imgs, augmented = gaussian_blur_stacked(imgs, p, stacks)\n",
    "    augments_applied.append(augmented)\n",
    "    \n",
    "    imgs, augmented = solarization_stacked(imgs, p, stacks)\n",
    "    augments_applied.append(augmented)\n",
    "    \n",
    "    augments_applied = torch.stack(augments_applied,1)\n",
    "    return imgs, augments_applied\n",
    "\n",
    "preprocess_iwm_stacked(torch.randn(32,12,96,72, device='cuda'))[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880deeee-5d84-47ef-9775-583dffaba410",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_img(imgs[-1].permute(1,2,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c2cb75-c68e-4775-aa5f-0a5afcec9a9a",
   "metadata": {},
   "source": [
    "<h1>DiT</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892bf794-e949-4766-a710-0b7002597581",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from nosaveddata import *\n",
    "\n",
    "unet = UNet_DiT_S_4(in_channels=4).cuda()\n",
    "x=torch.randn(32,4,32,32).cuda()\n",
    "c=torch.randn(32,384).cuda()\n",
    "t=torch.randint(0,1000,(32,)).cuda()\n",
    "unet(x,t).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea6bc6f-523d-4dfa-8ae1-8412cd2d0b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from nosaveddata import *\n",
    "\n",
    "\n",
    "model = DiT_Transformer(128, 8, 8, 108).cuda()\n",
    "\n",
    "X = torch.randn(16,108,128).cuda()\n",
    "c = torch.randn(16,128).cuda()\n",
    "\n",
    "model(X, c).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36203835-4741-4e59-b1af-fdf05b13c46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "list(torch.tensor([1,2,3,4]).split(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86dcafb6-0a86-4b23-a858-ffd84d0d0aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from nosaveddata import *\n",
    "\n",
    "model = UNet_DiT_1D(2048, 256, 2, 2048//64, seq_len=5).cuda()\n",
    "\n",
    "X = torch.randn(16,5,2048).cuda()\n",
    "t=torch.randint(0,1000,(16,)).cuda()\n",
    "c = torch.randn(16,256).cuda()\n",
    "\n",
    "model(X, t, c).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6a7bc2-2f1d-4bd2-a116-59fe67d4e0f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c34211d-6121-49b9-8e8e-ac989892c917",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
